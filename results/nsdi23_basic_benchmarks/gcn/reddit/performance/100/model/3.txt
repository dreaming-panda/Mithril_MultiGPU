Initialized node 0 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 6 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 5 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.907 seconds.
Building the CSC structure...
        It takes 2.049 seconds.
Building the CSC structure...
        It takes 2.375 seconds.
Building the CSC structure...
        It takes 2.394 seconds.
Building the CSC structure...
        It takes 2.399 seconds.
Building the CSC structure...
        It takes 2.424 seconds.
Building the CSC structure...
        It takes 2.653 seconds.
Building the CSC structure...
        It takes 2.666 seconds.
Building the CSC structure...
        It takes 1.841 seconds.
        It takes 1.866 seconds.
        It takes 2.186 seconds.
        It takes 2.301 seconds.
        It takes 2.307 seconds.
Building the Feature Vector...
        It takes 2.384 seconds.
        It takes 0.286 seconds.
Building the Label Vector...
        It takes 2.385 seconds.
        It takes 0.041 seconds.
        It takes 2.401 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.278 seconds.
Building the Label Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
        It takes 0.300 seconds.
        It takes 0.269 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.036 seconds.
        It takes 0.030 seconds.
        It takes 0.034 seconds.
        It takes 0.291 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
Building the Feature Vector...
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.274 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 3
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.257 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
Building the Feature Vector...
        It takes 0.271 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
Building the Label Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
        It takes 0.032 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.063 Gbps (per GPU), 440.501 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.800 Gbps (per GPU), 438.404 Gbps (aggregated)
The layer-level communication performance: 54.803 Gbps (per GPU), 438.422 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.583 Gbps (per GPU), 436.665 Gbps (aggregated)
The layer-level communication performance: 54.566 Gbps (per GPU), 436.524 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.396 Gbps (per GPU), 435.172 Gbps (aggregated)
The layer-level communication performance: 54.345 Gbps (per GPU), 434.761 Gbps (aggregated)
The layer-level communication performance: 54.319 Gbps (per GPU), 434.548 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.702 Gbps (per GPU), 1245.617 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.812 Gbps (per GPU), 1246.496 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.737 Gbps (per GPU), 1245.895 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.650 Gbps (per GPU), 1245.201 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.705 Gbps (per GPU), 1245.640 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.705 Gbps (per GPU), 1245.640 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.705 Gbps (per GPU), 1245.640 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.705 Gbps (per GPU), 1245.642 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.813 Gbps (per GPU), 838.506 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.815 Gbps (per GPU), 838.518 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.814 Gbps (per GPU), 838.511 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.815 Gbps (per GPU), 838.518 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.814 Gbps (per GPU), 838.511 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.814 Gbps (per GPU), 838.511 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.814 Gbps (per GPU), 838.511 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.815 Gbps (per GPU), 838.518 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.124 Gbps (per GPU), 280.995 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.123 Gbps (per GPU), 280.987 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.124 Gbps (per GPU), 280.995 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.124 Gbps (per GPU), 280.988 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.123 Gbps (per GPU), 280.985 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.123 Gbps (per GPU), 280.986 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.124 Gbps (per GPU), 280.992 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.123 Gbps (per GPU), 280.987 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.42ms  2.56ms  2.33ms  1.10  8.38K  3.53M
 chk_1  2.90ms  2.79ms  2.64ms  1.10  6.74K  3.60M
 chk_2  2.76ms  2.67ms  2.53ms  1.09  7.27K  3.53M
 chk_3  2.81ms  2.72ms  2.56ms  1.10  7.92K  3.61M
 chk_4  2.74ms  2.68ms  2.57ms  1.07  5.33K  3.68M
 chk_5  2.70ms  2.62ms  2.38ms  1.13 10.07K  3.45M
 chk_6  2.88ms  2.76ms  2.54ms  1.13  9.41K  3.48M
 chk_7  2.73ms  2.65ms  2.47ms  1.10  8.12K  3.60M
 chk_8  2.83ms  2.75ms  2.62ms  1.08  6.09K  3.64M
 chk_9  2.65ms  2.52ms  2.26ms  1.17 11.10K  3.38M
chk_10  2.87ms  2.79ms  2.67ms  1.08  5.67K  3.63M
chk_11  2.74ms  2.64ms  2.47ms  1.11  8.16K  3.54M
chk_12  2.93ms  2.83ms  2.68ms  1.10  7.24K  3.55M
chk_13  2.75ms  2.71ms  2.58ms  1.07  5.41K  3.68M
chk_14  2.99ms  2.88ms  2.73ms  1.09  7.14K  3.53M
chk_15  2.85ms  2.77ms  2.55ms  1.12  9.25K  3.49M
chk_16  2.70ms  2.67ms  2.55ms  1.06  4.78K  3.77M
chk_17  2.82ms  2.73ms  2.58ms  1.09  6.85K  3.60M
chk_18  2.64ms  2.55ms  2.40ms  1.10  7.47K  3.57M
chk_19  2.70ms  2.65ms  2.54ms  1.07  4.88K  3.75M
chk_20  2.71ms  2.61ms  2.48ms  1.09  7.00K  3.63M
chk_21  2.67ms  2.61ms  2.49ms  1.07  5.41K  3.68M
chk_22  2.87ms  2.75ms  2.49ms  1.15 11.07K  3.39M
chk_23  2.82ms  2.71ms  2.56ms  1.10  7.23K  3.64M
chk_24  2.83ms  2.72ms  2.50ms  1.13 10.13K  3.43M
chk_25  2.64ms  2.56ms  2.44ms  1.08  6.40K  3.57M
chk_26  2.85ms  2.77ms  2.64ms  1.08  5.78K  3.55M
chk_27  2.81ms  2.63ms  2.44ms  1.15  9.34K  3.48M
chk_28  3.03ms  2.93ms  2.78ms  1.09  6.37K  3.57M
chk_29  2.87ms  2.81ms  2.67ms  1.08  5.16K  3.78M
chk_30  2.72ms  2.69ms  2.56ms  1.06  5.44K  3.67M
chk_31  2.87ms  2.80ms  2.68ms  1.07  6.33K  3.63M
   Avg  2.78  2.70  2.54
   Max  3.03  2.93  2.78
   Min  2.42  2.52  2.26
 Ratio  1.25  1.16  1.23
   Var  0.01  0.01  0.01
Profiling takes 2.979 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 348.669 ms
Partition 0 [0, 4) has cost: 348.669 ms
Partition 1 [4, 8) has cost: 346.102 ms
Partition 2 [8, 12) has cost: 346.102 ms
Partition 3 [12, 16) has cost: 346.102 ms
Partition 4 [16, 20) has cost: 346.102 ms
Partition 5 [20, 24) has cost: 346.102 ms
Partition 6 [24, 28) has cost: 346.102 ms
Partition 7 [28, 32) has cost: 340.972 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 164.316 ms
GPU 0, Compute+Comm Time: 130.289 ms, Bubble Time: 29.222 ms, Imbalance Overhead: 4.805 ms
GPU 1, Compute+Comm Time: 128.819 ms, Bubble Time: 28.924 ms, Imbalance Overhead: 6.572 ms
GPU 2, Compute+Comm Time: 128.819 ms, Bubble Time: 28.837 ms, Imbalance Overhead: 6.659 ms
GPU 3, Compute+Comm Time: 128.819 ms, Bubble Time: 28.687 ms, Imbalance Overhead: 6.809 ms
GPU 4, Compute+Comm Time: 128.819 ms, Bubble Time: 28.594 ms, Imbalance Overhead: 6.902 ms
GPU 5, Compute+Comm Time: 128.819 ms, Bubble Time: 28.616 ms, Imbalance Overhead: 6.881 ms
GPU 6, Compute+Comm Time: 128.819 ms, Bubble Time: 28.637 ms, Imbalance Overhead: 6.859 ms
GPU 7, Compute+Comm Time: 126.932 ms, Bubble Time: 28.847 ms, Imbalance Overhead: 8.537 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 312.160 ms
GPU 0, Compute+Comm Time: 241.118 ms, Bubble Time: 54.394 ms, Imbalance Overhead: 16.649 ms
GPU 1, Compute+Comm Time: 244.361 ms, Bubble Time: 54.176 ms, Imbalance Overhead: 13.624 ms
GPU 2, Compute+Comm Time: 244.361 ms, Bubble Time: 54.189 ms, Imbalance Overhead: 13.610 ms
GPU 3, Compute+Comm Time: 244.361 ms, Bubble Time: 54.203 ms, Imbalance Overhead: 13.596 ms
GPU 4, Compute+Comm Time: 244.361 ms, Bubble Time: 54.474 ms, Imbalance Overhead: 13.325 ms
GPU 5, Compute+Comm Time: 244.361 ms, Bubble Time: 54.844 ms, Imbalance Overhead: 12.955 ms
GPU 6, Compute+Comm Time: 244.361 ms, Bubble Time: 55.166 ms, Imbalance Overhead: 12.633 ms
GPU 7, Compute+Comm Time: 245.458 ms, Bubble Time: 56.256 ms, Imbalance Overhead: 10.446 ms
The estimated cost of the whole pipeline: 500.300 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 694.772 ms
Partition 0 [0, 8) has cost: 694.772 ms
Partition 1 [8, 16) has cost: 692.205 ms
Partition 2 [16, 24) has cost: 692.205 ms
Partition 3 [24, 32) has cost: 687.074 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 172.369 ms
GPU 0, Compute+Comm Time: 141.779 ms, Bubble Time: 27.285 ms, Imbalance Overhead: 3.305 ms
GPU 1, Compute+Comm Time: 140.952 ms, Bubble Time: 26.782 ms, Imbalance Overhead: 4.634 ms
GPU 2, Compute+Comm Time: 140.952 ms, Bubble Time: 26.529 ms, Imbalance Overhead: 4.887 ms
GPU 3, Compute+Comm Time: 140.126 ms, Bubble Time: 26.255 ms, Imbalance Overhead: 5.987 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 315.062 ms
GPU 0, Compute+Comm Time: 256.706 ms, Bubble Time: 48.226 ms, Imbalance Overhead: 10.131 ms
GPU 1, Compute+Comm Time: 258.143 ms, Bubble Time: 48.538 ms, Imbalance Overhead: 8.381 ms
GPU 2, Compute+Comm Time: 258.143 ms, Bubble Time: 48.877 ms, Imbalance Overhead: 8.042 ms
GPU 3, Compute+Comm Time: 258.775 ms, Bubble Time: 49.926 ms, Imbalance Overhead: 6.361 ms
    The estimated cost with 2 DP ways is 511.802 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1386.976 ms
Partition 0 [0, 16) has cost: 1386.976 ms
Partition 1 [16, 32) has cost: 1379.279 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 217.672 ms
GPU 0, Compute+Comm Time: 189.195 ms, Bubble Time: 23.417 ms, Imbalance Overhead: 5.060 ms
GPU 1, Compute+Comm Time: 188.370 ms, Bubble Time: 23.883 ms, Imbalance Overhead: 5.419 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 352.889 ms
GPU 0, Compute+Comm Time: 308.203 ms, Bubble Time: 38.537 ms, Imbalance Overhead: 6.149 ms
GPU 1, Compute+Comm Time: 309.205 ms, Bubble Time: 38.457 ms, Imbalance Overhead: 5.227 ms
    The estimated cost with 4 DP ways is 599.089 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2766.255 ms
Partition 0 [0, 32) has cost: 2766.255 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 546.542 ms
GPU 0, Compute+Comm Time: 546.542 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 669.065 ms
GPU 0, Compute+Comm Time: 669.065 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1276.388 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 24)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [24, 48)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [48, 72)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [72, 96)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [96, 120)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [120, 144)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [144, 168)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [168, 190)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 24)...
+++++++++ Node 1 initializing the weights for op[24, 48)...
+++++++++ Node 5 initializing the weights for op[120, 144)...
+++++++++ Node 2 initializing the weights for op[48, 72)...
+++++++++ Node 6 initializing the weights for op[144, 168)...
+++++++++ Node 3 initializing the weights for op[72, 96)...
+++++++++ Node 7 initializing the weights for op[168, 190)...
+++++++++ Node 4 initializing the weights for op[96, 120)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 1, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.3185
	Epoch 50:	Loss 3.1048
	Epoch 75:	Loss 2.7347
	Epoch 100:	Loss 2.4159
	Epoch 125:	Loss 2.0433
	Epoch 150:	Loss 1.7214
	Epoch 175:	Loss 1.4855
	Epoch 200:	Loss 1.2782
	Epoch 225:	Loss 1.0775
	Epoch 250:	Loss 1.0028
	Epoch 275:	Loss 0.9322
	Epoch 300:	Loss 0.8838
	Epoch 325:	Loss 0.8481
	Epoch 350:	Loss 0.8156
	Epoch 375:	Loss 0.7994
	Epoch 400:	Loss 0.7817
	Epoch 425:	Loss 0.7703
	Epoch 450:	Loss 0.7457
	Epoch 475:	Loss 0.7250
	Epoch 500:	Loss 0.7059
	Epoch 525:	Loss 0.7276
	Epoch 550:	Loss 0.7039
	Epoch 575:	Loss 0.6722
	Epoch 600:	Loss 0.6710
	Epoch 625:	Loss 0.6668
	Epoch 650:	Loss 0.6515
	Epoch 675:	Loss 0.6348
	Epoch 700:	Loss 0.6291
	Epoch 725:	Loss 0.6545
	Epoch 750:	Loss 0.6291
	Epoch 775:	Loss 0.6098
	Epoch 800:	Loss 0.6033
	Epoch 825:	Loss 0.6061
	Epoch 850:	Loss 0.5974
	Epoch 875:	Loss 0.5852
	Epoch 900:	Loss 0.5827
	Epoch 925:	Loss 0.6180
	Epoch 950:	Loss 0.5862
	Epoch 975:	Loss 0.5787
	Epoch 1000:	Loss 0.5703
	Epoch 1025:	Loss 0.5687
	Epoch 1050:	Loss 0.5593
	Epoch 1075:	Loss 0.5565
	Epoch 1100:	Loss 0.5459
	Epoch 1125:	Loss 0.5463
	Epoch 1150:	Loss 0.5466
	Epoch 1175:	Loss 0.5374
	Epoch 1200:	Loss 0.5370
	Epoch 1225:	Loss 0.5327
	Epoch 1250:	Loss 0.5273
	Epoch 1275:	Loss 0.5289
	Epoch 1300:	Loss 0.5291
	Epoch 1325:	Loss 0.5257
	Epoch 1350:	Loss 0.5192
	Epoch 1375:	Loss 0.5199
	Epoch 1400:	Loss 0.5165
	Epoch 1425:	Loss 0.5269
	Epoch 1450:	Loss 0.5157
	Epoch 1475:	Loss 0.5079
	Epoch 1500:	Loss 0.5073
	Epoch 1525:	Loss 0.5052
	Epoch 1550:	Loss 0.5030
	Epoch 1575:	Loss 0.5002
	Epoch 1600:	Loss 0.5005
	Epoch 1625:	Loss 0.4947
	Epoch 1650:	Loss 0.4980
	Epoch 1675:	Loss 0.4916
	Epoch 1700:	Loss 0.4904
	Epoch 1725:	Loss 0.4882
	Epoch 1750:	Loss 0.4861
	Epoch 1775:	Loss 0.4853
	Epoch 1800:	Loss 0.4822
	Epoch 1825:	Loss 0.4859
	Epoch 1850:	Loss 0.4770
	Epoch 1875:	Loss 0.4762
	Epoch 1900:	Loss 0.4738
	Epoch 1925:	Loss 0.4730
	Epoch 1950:	Loss 0.4765
	Epoch 1975:	Loss 0.4735
	Epoch 2000:	Loss 0.4697
	Epoch 2025:	Loss 0.4741
	Epoch 2050:	Loss 0.4704
	Epoch 2075:	Loss 0.4684
	Epoch 2100:	Loss 0.4681
	Epoch 2125:	Loss 0.5128
	Epoch 2150:	Loss 0.4726
	Epoch 2175:	Loss 0.4700
	Epoch 2200:	Loss 0.4692
	Epoch 2225:	Loss 0.4718
	Epoch 2250:	Loss 0.4654
	Epoch 2275:	Loss 0.4641
	Epoch 2300:	Loss 0.4643
	Epoch 2325:	Loss 0.5032
	Epoch 2350:	Loss 0.4654
	Epoch 2375:	Loss 0.4632
	Epoch 2400:	Loss 0.4643
	Epoch 2425:	Loss 0.5080
	Epoch 2450:	Loss 0.4990
	Epoch 2475:	Loss 0.5408
	Epoch 2500:	Loss 0.5269
	Epoch 2525:	Loss 0.5433
	Epoch 2550:	Loss 0.5261
	Epoch 2575:	Loss 0.5040
	Epoch 2600:	Loss 0.4778
	Epoch 2625:	Loss 0.4715
	Epoch 2650:	Loss 0.4667
	Epoch 2675:	Loss 0.4702
	Epoch 2700:	Loss 0.4583
	Epoch 2725:	Loss 0.4608
	Epoch 2750:	Loss 0.4550
	Epoch 2775:	Loss 0.4538
	Epoch 2800:	Loss 0.4508
	Epoch 2825:	Loss 0.4479
	Epoch 2850:	Loss 0.4489
	Epoch 2875:	Loss 0.4450
	Epoch 2900:	Loss 0.4455
	Epoch 2925:	Loss 0.4455
	Epoch 2950:	Loss 0.4445
	Epoch 2975:	Loss 0.4405
	Epoch 3000:	Loss 0.4409
	Epoch 3025:	Loss 0.4449
	Epoch 3050:	Loss 0.4380
	Epoch 3075:	Loss 0.4389
	Epoch 3100:	Loss 0.4427
	Epoch 3125:	Loss 0.4501
	Epoch 3150:	Loss 0.4569
	Epoch 3175:	Loss 0.4565
	Epoch 3200:	Loss 0.4592
	Epoch 3225:	Loss 0.4964
	Epoch 3250:	Loss 0.4701
	Epoch 3275:	Loss 0.4653
	Epoch 3300:	Loss 0.4573
	Epoch 3325:	Loss 0.4518
	Epoch 3350:	Loss 0.4457
	Epoch 3375:	Loss 0.4450
	Epoch 3400:	Loss 0.4403
	Epoch 3425:	Loss 0.4368
	Epoch 3450:	Loss 0.4326
	Epoch 3475:	Loss 0.4344
	Epoch 3500:	Loss 0.4344
	Epoch 3525:	Loss 0.4460
	Epoch 3550:	Loss 0.4360
	Epoch 3575:	Loss 0.4483
	Epoch 3600:	Loss 0.4495
	Epoch 3625:	Loss 0.4800
	Epoch 3650:	Loss 0.4778
	Epoch 3675:	Loss 0.4727
	Epoch 3700:	Loss 0.4594
	Epoch 3725:	Loss 0.4491
	Epoch 3750:	Loss 0.4452
	Epoch 3775:	Loss 0.4447
	Epoch 3800:	Loss 0.4393
	Epoch 3825:	Loss 0.4392
	Epoch 3850:	Loss 0.4350
	Epoch 3875:	Loss 0.4338
	Epoch 3900:	Loss 0.4317
	Epoch 3925:	Loss 0.4306
	Epoch 3950:	Loss 0.4343
	Epoch 3975:	Loss 0.4281
	Epoch 4000:	Loss 0.4284
	Epoch 4025:	Loss 0.4274
	Epoch 4050:	Loss 0.4282
	Epoch 4075:	Loss 0.4263
	Epoch 4100:	Loss 0.4258
	Epoch 4125:	Loss 0.4284
	Epoch 4150:	Loss 0.4237
	Epoch 4175:	Loss 0.4243
	Epoch 4200:	Loss 0.4231
	Epoch 4225:	Loss 0.4242
	Epoch 4250:	Loss 0.4331
	Epoch 4275:	Loss 0.4302
	Epoch 4300:	Loss 0.4285
	Epoch 4325:	Loss 0.4337
	Epoch 4350:	Loss 0.4356
	Epoch 4375:	Loss 0.4355
	Epoch 4400:	Loss 0.4298
	Epoch 4425:	Loss 0.4300
	Epoch 4450:	Loss 0.4253
	Epoch 4475:	Loss 0.4230
	Epoch 4500:	Loss 0.4224
	Epoch 4525:	Loss 0.4224
	Epoch 4550:	Loss 0.4196
	Epoch 4575:	Loss 0.4211
	Epoch 4600:	Loss 0.4213
	Epoch 4625:	Loss 0.4227
	Epoch 4650:	Loss 0.4230
	Epoch 4675:	Loss 0.4201
	Epoch 4700:	Loss 0.4229
	Epoch 4725:	Loss 0.4566
	Epoch 4750:	Loss 0.4605
	Epoch 4775:	Loss 0.4627
	Epoch 4800:	Loss 0.4523
	Epoch 4825:	Loss 0.4475
	Epoch 4850:	Loss 0.4344
	Epoch 4875:	Loss 0.4310
	Epoch 4900:	Loss 0.4233
	Epoch 4925:	Loss 0.4218
	Epoch 4950:	Loss 0.4190
	Epoch 4975:	Loss 0.4155
	Epoch 5000:	Loss 0.4124
Node 1, Pre/Post-Pipelining: 1.085 / 0.860 ms, Bubble: 75.815 ms, Compute: 259.501 ms, Comm: 27.701 ms, Imbalance: 13.147 ms
Node 2, Pre/Post-Pipelining: 1.082 / 0.884 ms, Bubble: 76.107 ms, Compute: 248.728 ms, Comm: 29.629 ms, Imbalance: 22.066 ms
Node 3, Pre/Post-Pipelining: 1.084 / 0.848 ms, Bubble: 75.536 ms, Compute: 250.247 ms, Comm: 26.324 ms, Imbalance: 24.334 ms
Node 0, Pre/Post-Pipelining: 1.085 / 0.878 ms, Bubble: 76.604 ms, Compute: 261.024 ms, Comm: 16.910 ms, Imbalance: 21.542 ms
Node 5, Pre/Post-Pipelining: 1.085 / 0.850 ms, Bubble: 75.348 ms, Compute: 251.800 ms, Comm: 30.672 ms, Imbalance: 18.762 ms
Node 7, Pre/Post-Pipelining: 1.084 / 16.945 ms, Bubble: 59.125 ms, Compute: 267.911 ms, Comm: 16.925 ms, Imbalance: 16.337 ms
Node 4, Pre/Post-Pipelining: 1.080 / 0.882 ms, Bubble: 75.617 ms, Compute: 245.884 ms, Comm: 26.282 ms, Imbalance: 28.945 ms
Node 6, Pre/Post-Pipelining: 1.085 / 0.846 ms, Bubble: 75.233 ms, Compute: 249.801 ms, Comm: 29.128 ms, Imbalance: 22.480 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.085 ms
Cluster-Wide Average, Post-Pipelining Overhead: 0.878 ms
Cluster-Wide Average, Bubble: 76.604 ms
Cluster-Wide Average, Compute: 261.024 ms
Cluster-Wide Average, Communication: 16.910 ms
Cluster-Wide Average, Imbalance: 21.542 ms
Node 0, GPU memory consumption: 6.739 GB
Node 3, GPU memory consumption: 5.796 GB
Node 1, GPU memory consumption: 5.819 GB
Node 2, GPU memory consumption: 5.819 GB
Node 4, GPU memory consumption: 5.796 GB
Node 7, GPU memory consumption: 5.569 GB
Node 5, GPU memory consumption: 5.819 GB
Node 6, GPU memory consumption: 5.819 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.378711 s---------------
------------------------node id 1,  per-epoch time: 0.378711 s---------------
------------------------node id 2,  per-epoch time: 0.378711 s---------------
------------------------node id 3,  per-epoch time: 0.378711 s---------------
------------------------node id 4,  per-epoch time: 0.378711 s---------------
------------------------node id 5,  per-epoch time: 0.378711 s---------------
------------------------node id 6,  per-epoch time: 0.378711 s---------------
------------------------node id 7,  per-epoch time: 0.378711 s---------------
************ Profiling Results ************
	Bubble: 125.149856 (ms) (33.04 percentage)
	Compute: 250.091708 (ms) (66.02 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 3.569963 (ms) (0.94 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.215 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
	Total communication (cluster-wide, per-epoch): 1.216 GB
	Aggregated layer-level communication throughput: 410.151 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
