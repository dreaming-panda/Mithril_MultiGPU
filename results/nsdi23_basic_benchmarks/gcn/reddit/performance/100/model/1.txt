Initialized node 1 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 0 on machine gnerv1
Initialized node 5 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 6 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.911 seconds.
Building the CSC structure...
        It takes 1.927 seconds.
Building the CSC structure...
        It takes 2.342 seconds.
Building the CSC structure...
        It takes 2.355 seconds.
Building the CSC structure...
        It takes 2.388 seconds.
Building the CSC structure...
        It takes 2.416 seconds.
Building the CSC structure...
        It takes 2.424 seconds.
Building the CSC structure...
        It takes 2.653 seconds.
Building the CSC structure...
        It takes 1.850 seconds.
        It takes 1.835 seconds.
        It takes 2.145 seconds.
        It takes 2.307 seconds.
        It takes 2.248 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.361 seconds.
        It takes 2.357 seconds.
        It takes 2.345 seconds.
        It takes 0.279 seconds.
Building the Label Vector...
        It takes 0.309 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.040 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.303 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
        It takes 0.312 seconds.
Building the Label Vector...
        It takes 0.305 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.033 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.247 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
        It takes 0.300 seconds.
Building the Label Vector...
        It takes 0.269 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.038 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.585 Gbps (per GPU), 444.683 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.322 Gbps (per GPU), 442.580 Gbps (aggregated)
The layer-level communication performance: 55.307 Gbps (per GPU), 442.454 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.116 Gbps (per GPU), 440.932 Gbps (aggregated)
The layer-level communication performance: 55.078 Gbps (per GPU), 440.627 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.902 Gbps (per GPU), 439.213 Gbps (aggregated)
The layer-level communication performance: 54.862 Gbps (per GPU), 438.899 Gbps (aggregated)
The layer-level communication performance: 54.831 Gbps (per GPU), 438.649 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 160.720 Gbps (per GPU), 1285.758 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.640 Gbps (per GPU), 1285.118 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.720 Gbps (per GPU), 1285.758 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.649 Gbps (per GPU), 1285.192 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.699 Gbps (per GPU), 1285.588 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.701 Gbps (per GPU), 1285.610 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.695 Gbps (per GPU), 1285.561 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 160.698 Gbps (per GPU), 1285.586 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 105.070 Gbps (per GPU), 840.563 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.069 Gbps (per GPU), 840.554 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.071 Gbps (per GPU), 840.570 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.070 Gbps (per GPU), 840.563 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.070 Gbps (per GPU), 840.563 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.069 Gbps (per GPU), 840.556 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.069 Gbps (per GPU), 840.556 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.069 Gbps (per GPU), 840.556 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.983 Gbps (per GPU), 263.865 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.983 Gbps (per GPU), 263.866 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.982 Gbps (per GPU), 263.854 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.982 Gbps (per GPU), 263.853 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.982 Gbps (per GPU), 263.856 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.982 Gbps (per GPU), 263.857 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.983 Gbps (per GPU), 263.860 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.982 Gbps (per GPU), 263.854 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.43ms  2.55ms  2.34ms  1.09  8.38K  3.53M
 chk_1  2.83ms  2.76ms  2.57ms  1.10  6.74K  3.60M
 chk_2  2.76ms  2.63ms  2.49ms  1.10  7.27K  3.53M
 chk_3  2.76ms  2.68ms  2.51ms  1.10  7.92K  3.61M
 chk_4  2.68ms  2.63ms  2.53ms  1.06  5.33K  3.68M
 chk_5  2.65ms  2.57ms  2.34ms  1.14 10.07K  3.45M
 chk_6  2.84ms  2.73ms  2.51ms  1.13  9.41K  3.48M
 chk_7  2.70ms  2.61ms  2.44ms  1.11  8.12K  3.60M
 chk_8  2.78ms  2.71ms  2.60ms  1.07  6.09K  3.64M
 chk_9  2.61ms  2.48ms  2.24ms  1.17 11.10K  3.38M
chk_10  2.81ms  2.76ms  2.63ms  1.07  5.67K  3.63M
chk_11  2.68ms  2.60ms  2.43ms  1.11  8.16K  3.54M
chk_12  2.89ms  2.79ms  2.65ms  1.09  7.24K  3.55M
chk_13  2.71ms  2.66ms  2.55ms  1.06  5.41K  3.68M
chk_14  2.96ms  2.86ms  2.70ms  1.09  7.14K  3.53M
chk_15  2.81ms  2.71ms  2.52ms  1.12  9.25K  3.49M
chk_16  2.66ms  2.59ms  2.50ms  1.06  4.78K  3.77M
chk_17  2.76ms  2.68ms  2.54ms  1.09  6.85K  3.60M
chk_18  2.59ms  2.48ms  2.34ms  1.11  7.47K  3.57M
chk_19  2.64ms  2.58ms  2.48ms  1.06  4.88K  3.75M
chk_20  2.66ms  2.56ms  2.64ms  1.04  7.00K  3.63M
chk_21  2.63ms  2.57ms  2.45ms  1.07  5.41K  3.68M
chk_22  2.82ms  2.70ms  2.45ms  1.15 11.07K  3.39M
chk_23  2.76ms  2.65ms  2.51ms  1.10  7.23K  3.64M
chk_24  2.77ms  2.70ms  2.47ms  1.12 10.13K  3.43M
chk_25  2.61ms  2.58ms  2.42ms  1.08  6.40K  3.57M
chk_26  2.79ms  2.74ms  2.63ms  1.06  5.78K  3.55M
chk_27  2.71ms  2.60ms  2.41ms  1.13  9.34K  3.48M
chk_28  2.90ms  2.89ms  2.75ms  1.05  6.37K  3.57M
chk_29  2.80ms  2.81ms  2.63ms  1.07  5.16K  3.78M
chk_30  2.67ms  2.66ms  2.51ms  1.06  5.44K  3.67M
chk_31  2.82ms  2.79ms  2.65ms  1.07  6.33K  3.63M
   Avg  2.73  2.67  2.51
   Max  2.96  2.89  2.75
   Min  2.43  2.48  2.24
 Ratio  1.22  1.16  1.23
   Var  0.01  0.01  0.01
Profiling takes 2.946 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 343.491 ms
Partition 0 [0, 4) has cost: 343.491 ms
Partition 1 [4, 8) has cost: 341.301 ms
Partition 2 [8, 12) has cost: 341.301 ms
Partition 3 [12, 16) has cost: 341.301 ms
Partition 4 [16, 20) has cost: 341.301 ms
Partition 5 [20, 24) has cost: 341.301 ms
Partition 6 [24, 28) has cost: 341.301 ms
Partition 7 [28, 32) has cost: 336.411 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 162.500 ms
GPU 0, Compute+Comm Time: 128.410 ms, Bubble Time: 28.982 ms, Imbalance Overhead: 5.108 ms
GPU 1, Compute+Comm Time: 127.082 ms, Bubble Time: 28.725 ms, Imbalance Overhead: 6.693 ms
GPU 2, Compute+Comm Time: 127.082 ms, Bubble Time: 28.611 ms, Imbalance Overhead: 6.807 ms
GPU 3, Compute+Comm Time: 127.082 ms, Bubble Time: 28.452 ms, Imbalance Overhead: 6.966 ms
GPU 4, Compute+Comm Time: 127.082 ms, Bubble Time: 28.344 ms, Imbalance Overhead: 7.074 ms
GPU 5, Compute+Comm Time: 127.082 ms, Bubble Time: 28.326 ms, Imbalance Overhead: 7.093 ms
GPU 6, Compute+Comm Time: 127.082 ms, Bubble Time: 28.308 ms, Imbalance Overhead: 7.111 ms
GPU 7, Compute+Comm Time: 125.193 ms, Bubble Time: 28.499 ms, Imbalance Overhead: 8.809 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 308.213 ms
GPU 0, Compute+Comm Time: 238.041 ms, Bubble Time: 53.859 ms, Imbalance Overhead: 16.312 ms
GPU 1, Compute+Comm Time: 241.042 ms, Bubble Time: 53.684 ms, Imbalance Overhead: 13.487 ms
GPU 2, Compute+Comm Time: 241.042 ms, Bubble Time: 53.723 ms, Imbalance Overhead: 13.447 ms
GPU 3, Compute+Comm Time: 241.042 ms, Bubble Time: 53.763 ms, Imbalance Overhead: 13.408 ms
GPU 4, Compute+Comm Time: 241.042 ms, Bubble Time: 53.982 ms, Imbalance Overhead: 13.189 ms
GPU 5, Compute+Comm Time: 241.042 ms, Bubble Time: 54.284 ms, Imbalance Overhead: 12.887 ms
GPU 6, Compute+Comm Time: 241.042 ms, Bubble Time: 54.562 ms, Imbalance Overhead: 12.609 ms
GPU 7, Compute+Comm Time: 241.903 ms, Bubble Time: 55.544 ms, Imbalance Overhead: 10.765 ms
The estimated cost of the whole pipeline: 494.249 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 684.792 ms
Partition 0 [0, 8) has cost: 684.792 ms
Partition 1 [8, 16) has cost: 682.602 ms
Partition 2 [16, 24) has cost: 682.602 ms
Partition 3 [24, 32) has cost: 677.712 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 170.218 ms
GPU 0, Compute+Comm Time: 139.962 ms, Bubble Time: 27.106 ms, Imbalance Overhead: 3.149 ms
GPU 1, Compute+Comm Time: 139.288 ms, Bubble Time: 26.582 ms, Imbalance Overhead: 4.349 ms
GPU 2, Compute+Comm Time: 139.288 ms, Bubble Time: 26.238 ms, Imbalance Overhead: 4.692 ms
GPU 3, Compute+Comm Time: 138.387 ms, Bubble Time: 25.911 ms, Imbalance Overhead: 5.919 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 310.866 ms
GPU 0, Compute+Comm Time: 253.496 ms, Bubble Time: 47.471 ms, Imbalance Overhead: 9.899 ms
GPU 1, Compute+Comm Time: 254.819 ms, Bubble Time: 47.822 ms, Imbalance Overhead: 8.225 ms
GPU 2, Compute+Comm Time: 254.819 ms, Bubble Time: 48.229 ms, Imbalance Overhead: 7.818 ms
GPU 3, Compute+Comm Time: 255.225 ms, Bubble Time: 49.495 ms, Imbalance Overhead: 6.146 ms
    The estimated cost with 2 DP ways is 505.138 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1367.394 ms
Partition 0 [0, 16) has cost: 1367.394 ms
Partition 1 [16, 32) has cost: 1360.314 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 215.905 ms
GPU 0, Compute+Comm Time: 187.618 ms, Bubble Time: 23.323 ms, Imbalance Overhead: 4.965 ms
GPU 1, Compute+Comm Time: 186.866 ms, Bubble Time: 23.622 ms, Imbalance Overhead: 5.417 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 349.750 ms
GPU 0, Compute+Comm Time: 305.640 ms, Bubble Time: 38.269 ms, Imbalance Overhead: 5.841 ms
GPU 1, Compute+Comm Time: 306.490 ms, Bubble Time: 38.234 ms, Imbalance Overhead: 5.027 ms
    The estimated cost with 4 DP ways is 593.939 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2727.708 ms
Partition 0 [0, 32) has cost: 2727.708 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 573.431 ms
GPU 0, Compute+Comm Time: 573.431 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 694.201 ms
GPU 0, Compute+Comm Time: 694.201 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1331.013 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 24)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [24, 48)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [48, 72)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [72, 96)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [96, 120)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [120, 144)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [144, 168)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [168, 190)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[24, 48)...
+++++++++ Node 3 initializing the weights for op[72, 96)...
+++++++++ Node 4 initializing the weights for op[96, 120)...
+++++++++ Node 0 initializing the weights for op[0, 24)...
+++++++++ Node 5 initializing the weights for op[120, 144)...
+++++++++ Node 2 initializing the weights for op[48, 72)...
+++++++++ Node 6 initializing the weights for op[144, 168)...
+++++++++ Node 7 initializing the weights for op[168, 190)...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.3135
	Epoch 50:	Loss 3.1847
	Epoch 75:	Loss 2.9773
	Epoch 100:	Loss 2.7427
	Epoch 125:	Loss 2.4163
	Epoch 150:	Loss 2.1138
	Epoch 175:	Loss 1.8415
	Epoch 200:	Loss 1.6405
	Epoch 225:	Loss 1.4923
	Epoch 250:	Loss 1.3204
	Epoch 275:	Loss 1.1814
	Epoch 300:	Loss 1.0681
	Epoch 325:	Loss 1.0090
	Epoch 350:	Loss 0.9402
	Epoch 375:	Loss 0.8711
	Epoch 400:	Loss 0.8335
	Epoch 425:	Loss 0.7834
	Epoch 450:	Loss 0.7697
	Epoch 475:	Loss 0.7539
	Epoch 500:	Loss 0.7379
	Epoch 525:	Loss 0.7070
	Epoch 550:	Loss 0.7052
	Epoch 575:	Loss 0.6944
	Epoch 600:	Loss 0.6808
	Epoch 625:	Loss 0.6994
	Epoch 650:	Loss 0.6736
	Epoch 675:	Loss 0.6509
	Epoch 700:	Loss 0.6473
	Epoch 725:	Loss 0.6436
	Epoch 750:	Loss 0.6349
	Epoch 775:	Loss 0.6248
	Epoch 800:	Loss 0.6096
	Epoch 825:	Loss 0.6043
	Epoch 850:	Loss 0.6000
	Epoch 875:	Loss 0.5946
	Epoch 900:	Loss 0.5878
	Epoch 925:	Loss 0.5802
	Epoch 950:	Loss 0.5649
	Epoch 975:	Loss 0.5568
	Epoch 1000:	Loss 0.5587
	Epoch 1025:	Loss 0.5546
	Epoch 1050:	Loss 0.5453
	Epoch 1075:	Loss 0.5420
	Epoch 1100:	Loss 0.5284
	Epoch 1125:	Loss 0.5854
	Epoch 1150:	Loss 0.5512
	Epoch 1175:	Loss 0.5273
	Epoch 1200:	Loss 0.5238
	Epoch 1225:	Loss 0.5216
	Epoch 1250:	Loss 0.5108
	Epoch 1275:	Loss 0.5075
	Epoch 1300:	Loss 0.5004
	Epoch 1325:	Loss 0.5028
	Epoch 1350:	Loss 0.5009
	Epoch 1375:	Loss 0.4971
	Epoch 1400:	Loss 0.5005
	Epoch 1425:	Loss 0.5130
	Epoch 1450:	Loss 0.5031
	Epoch 1475:	Loss 0.4978
	Epoch 1500:	Loss 0.4973
	Epoch 1525:	Loss 0.5018
	Epoch 1550:	Loss 0.5042
	Epoch 1575:	Loss 0.4979
	Epoch 1600:	Loss 0.4865
	Epoch 1625:	Loss 0.4866
	Epoch 1650:	Loss 0.4769
	Epoch 1675:	Loss 0.4678
	Epoch 1700:	Loss 0.4675
	Epoch 1725:	Loss 0.4644
	Epoch 1750:	Loss 0.4624
	Epoch 1775:	Loss 0.4597
	Epoch 1800:	Loss 0.4609
	Epoch 1825:	Loss 0.4638
	Epoch 1850:	Loss 0.4582
	Epoch 1875:	Loss 0.4576
	Epoch 1900:	Loss 0.4513
	Epoch 1925:	Loss 0.4596
	Epoch 1950:	Loss 0.4545
	Epoch 1975:	Loss 0.4551
	Epoch 2000:	Loss 0.4606
	Epoch 2025:	Loss 0.4740
	Epoch 2050:	Loss 0.4635
	Epoch 2075:	Loss 0.4622
	Epoch 2100:	Loss 0.4544
	Epoch 2125:	Loss 0.4524
	Epoch 2150:	Loss 0.4477
	Epoch 2175:	Loss 0.4511
	Epoch 2200:	Loss 0.4471
	Epoch 2225:	Loss 0.4394
	Epoch 2250:	Loss 0.4400
	Epoch 2275:	Loss 0.4398
	Epoch 2300:	Loss 0.4387
	Epoch 2325:	Loss 0.4408
	Epoch 2350:	Loss 0.4368
	Epoch 2375:	Loss 0.4345
	Epoch 2400:	Loss 0.4309
	Epoch 2425:	Loss 0.4295
	Epoch 2450:	Loss 0.4338
	Epoch 2475:	Loss 0.4328
	Epoch 2500:	Loss 0.4337
	Epoch 2525:	Loss 0.4350
	Epoch 2550:	Loss 0.4315
	Epoch 2575:	Loss 0.4356
	Epoch 2600:	Loss 0.4366
	Epoch 2625:	Loss 0.4935
	Epoch 2650:	Loss 0.4584
	Epoch 2675:	Loss 0.4615
	Epoch 2700:	Loss 0.4497
	Epoch 2725:	Loss 0.4401
	Epoch 2750:	Loss 0.4360
	Epoch 2775:	Loss 0.4361
	Epoch 2800:	Loss 0.4304
	Epoch 2825:	Loss 0.4257
	Epoch 2850:	Loss 0.4242
	Epoch 2875:	Loss 0.4206
	Epoch 2900:	Loss 0.4214
	Epoch 2925:	Loss 0.4195
	Epoch 2950:	Loss 0.4221
	Epoch 2975:	Loss 0.4200
	Epoch 3000:	Loss 0.4193
	Epoch 3025:	Loss 0.4256
	Epoch 3050:	Loss 0.4206
	Epoch 3075:	Loss 0.4204
	Epoch 3100:	Loss 0.4220
	Epoch 3125:	Loss 0.4230
	Epoch 3150:	Loss 0.4164
	Epoch 3175:	Loss 0.4137
	Epoch 3200:	Loss 0.4150
	Epoch 3225:	Loss 0.4166
	Epoch 3250:	Loss 0.4109
	Epoch 3275:	Loss 0.4133
	Epoch 3300:	Loss 0.4113
	Epoch 3325:	Loss 0.4092
	Epoch 3350:	Loss 0.4121
	Epoch 3375:	Loss 0.4149
	Epoch 3400:	Loss 0.4116
	Epoch 3425:	Loss 0.4229
	Epoch 3450:	Loss 0.4158
	Epoch 3475:	Loss 0.4234
	Epoch 3500:	Loss 0.4210
	Epoch 3525:	Loss 0.4378
	Epoch 3550:	Loss 0.4260
	Epoch 3575:	Loss 0.4351
	Epoch 3600:	Loss 0.4363
	Epoch 3625:	Loss 0.4567
	Epoch 3650:	Loss 0.4496
	Epoch 3675:	Loss 0.4370
	Epoch 3700:	Loss 0.4199
	Epoch 3725:	Loss 0.4119
	Epoch 3750:	Loss 0.4100
	Epoch 3775:	Loss 0.4140
	Epoch 3800:	Loss 0.4127
	Epoch 3825:	Loss 0.4089
	Epoch 3850:	Loss 0.4059
	Epoch 3875:	Loss 0.4033
	Epoch 3900:	Loss 0.4010
	Epoch 3925:	Loss 0.3996
	Epoch 3950:	Loss 0.3967
	Epoch 3975:	Loss 0.4024
	Epoch 4000:	Loss 0.4003
	Epoch 4025:	Loss 0.4017
	Epoch 4050:	Loss 0.3990
	Epoch 4075:	Loss 0.3966
	Epoch 4100:	Loss 0.3953
	Epoch 4125:	Loss 0.3959
	Epoch 4150:	Loss 0.3951
	Epoch 4175:	Loss 0.3944
	Epoch 4200:	Loss 0.3930
	Epoch 4225:	Loss 0.3918
	Epoch 4250:	Loss 0.3930
	Epoch 4275:	Loss 0.3909
	Epoch 4300:	Loss 0.3915
	Epoch 4325:	Loss 0.3910
	Epoch 4350:	Loss 0.3908
	Epoch 4375:	Loss 0.3889
	Epoch 4400:	Loss 0.3889
	Epoch 4425:	Loss 0.3892
	Epoch 4450:	Loss 0.3903
	Epoch 4475:	Loss 0.3873
	Epoch 4500:	Loss 0.3840
	Epoch 4525:	Loss 0.3858
	Epoch 4550:	Loss 0.3843
	Epoch 4575:	Loss 0.3860
	Epoch 4600:	Loss 0.3854
	Epoch 4625:	Loss 0.3876
	Epoch 4650:	Loss 0.3853
	Epoch 4675:	Loss 0.3831
	Epoch 4700:	Loss 0.3812
	Epoch 4725:	Loss 0.3834
	Epoch 4750:	Loss 0.3831
	Epoch 4775:	Loss 0.3828
	Epoch 4800:	Loss 0.3806
	Epoch 4825:	Loss 0.3848
	Epoch 4850:	Loss 0.3964
	Epoch 4875:	Loss 0.3964
	Epoch 4900:	Loss 0.4110
	Epoch 4925:	Loss 0.4629
	Epoch 4950:	Loss 0.4530
	Epoch 4975:	Loss 0.4385
	Epoch 5000:	Loss 0.4188
Node 1, Pre/Post-Pipelining: 1.087 / 0.854 ms, Bubble: 75.886 ms, Compute: 259.393 ms, Comm: 27.520 ms, Imbalance: 13.436 ms
Node 2, Pre/Post-Pipelining: 1.085 / 0.849 ms, Bubble: 76.263 ms, Compute: 246.816 ms, Comm: 29.557 ms, Imbalance: 24.019 ms
Node 3, Pre/Post-Pipelining: 1.084 / 0.855 ms, Bubble: 75.541 ms, Compute: 250.970 ms, Comm: 26.239 ms, Imbalance: 23.726 ms
Node 0, Pre/Post-Pipelining: 1.085 / 0.878 ms, Bubble: 76.634 ms, Compute: 261.115 ms, Comm: 16.881 ms, Imbalance: 21.475 ms
Node 4, Pre/Post-Pipelining: 1.083 / 0.849 ms, Bubble: 75.720 ms, Compute: 244.165 ms, Comm: 26.049 ms, Imbalance: 30.923 ms
Node 5, Pre/Post-Pipelining: 1.083 / 0.880 ms, Bubble: 75.240 ms, Compute: 252.473 ms, Comm: 30.691 ms, Imbalance: 18.153 ms
Node 6, Pre/Post-Pipelining: 1.084 / 0.882 ms, Bubble: 75.095 ms, Compute: 251.607 ms, Comm: 29.270 ms, Imbalance: 20.657 ms
Node 7, Pre/Post-Pipelining: 1.085 / 16.943 ms, Bubble: 59.067 ms, Compute: 267.656 ms, Comm: 17.045 ms, Imbalance: 16.577 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.085 ms
Cluster-Wide Average, Post-Pipelining Overhead: 0.878 ms
Cluster-Wide Average, Bubble: 76.634 ms
Cluster-Wide Average, Compute: 261.115 ms
Cluster-Wide Average, Communication: 16.881 ms
Cluster-Wide Average, Imbalance: 21.475 ms
Node 0, GPU memory consumption: 6.739 GB
Node 3, GPU memory consumption: 5.796 GB
Node 1, GPU memory consumption: 5.819 GB
Node 2, GPU memory consumption: 5.819 GB
Node 7, GPU memory consumption: 5.569 GB
Node 5, GPU memory consumption: 5.819 GB
Node 6, GPU memory consumption: 5.819 GB
Node 4, GPU memory consumption: 5.796 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.378761 s---------------
------------------------node id 1,  per-epoch time: 0.378761 s---------------
------------------------node id 2,  per-epoch time: 0.378761 s---------------
------------------------node id 3,  per-epoch time: 0.378761 s---------------
------------------------node id 4,  per-epoch time: 0.378761 s---------------
------------------------node id 5,  per-epoch time: 0.378761 s---------------
------------------------node id 6,  per-epoch time: 0.378761 s---------------
------------------------node id 7,  per-epoch time: 0.378761 s---------------
************ Profiling Results ************
	Bubble: 125.287922 (ms) (33.07 percentage)
	Compute: 250.001895 (ms) (65.99 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 3.567348 (ms) (0.94 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.215 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
	Total communication (cluster-wide, per-epoch): 1.216 GB
	Aggregated layer-level communication throughput: 410.792 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
