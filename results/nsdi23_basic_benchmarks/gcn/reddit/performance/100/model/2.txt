Initialized node 6 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 4 on machine gnerv2
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 2 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.907 seconds.
Building the CSC structure...
        It takes 2.345 seconds.
Building the CSC structure...
        It takes 2.396 seconds.
Building the CSC structure...
        It takes 2.437 seconds.
Building the CSC structure...
        It takes 2.458 seconds.
Building the CSC structure...
        It takes 2.610 seconds.
Building the CSC structure...
        It takes 2.626 seconds.
Building the CSC structure...
        It takes 2.671 seconds.
Building the CSC structure...
        It takes 1.838 seconds.
        It takes 2.299 seconds.
Building the Feature Vector...
        It takes 2.331 seconds.
        It takes 2.320 seconds.
        It takes 2.412 seconds.
        It takes 2.337 seconds.
        It takes 0.268 seconds.
        It takes 2.381 seconds.
Building the Label Vector...
        It takes 2.356 seconds.
        It takes 0.041 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.288 seconds.
Building the Label Vector...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
        It takes 0.043 seconds.
        It takes 0.281 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
        It takes 0.285 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.250 seconds.
Building the Label Vector...
Building the Feature Vector...
        It takes 0.031 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 2
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.261 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
        It takes 0.272 seconds.
Building the Label Vector...
        It takes 0.286 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
        It takes 0.037 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
WARNING: the current version only applies to linear GNN models!
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 57.119 Gbps (per GPU), 456.949 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 57.063 Gbps (per GPU), 456.508 Gbps (aggregated)
The layer-level communication performance: 56.848 Gbps (per GPU), 454.783 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.616 Gbps (per GPU), 452.924 Gbps (aggregated)
The layer-level communication performance: 56.581 Gbps (per GPU), 452.651 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.397 Gbps (per GPU), 451.176 Gbps (aggregated)
The layer-level communication performance: 56.356 Gbps (per GPU), 450.844 Gbps (aggregated)
The layer-level communication performance: 56.322 Gbps (per GPU), 450.576 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.020 Gbps (per GPU), 1272.158 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.017 Gbps (per GPU), 1272.140 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.008 Gbps (per GPU), 1272.061 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.020 Gbps (per GPU), 1272.158 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.011 Gbps (per GPU), 1272.085 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.023 Gbps (per GPU), 1272.182 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.008 Gbps (per GPU), 1272.061 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.002 Gbps (per GPU), 1272.013 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 103.162 Gbps (per GPU), 825.298 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.163 Gbps (per GPU), 825.305 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.164 Gbps (per GPU), 825.312 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.163 Gbps (per GPU), 825.305 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.164 Gbps (per GPU), 825.312 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.163 Gbps (per GPU), 825.305 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.162 Gbps (per GPU), 825.299 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 103.163 Gbps (per GPU), 825.305 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 30.378 Gbps (per GPU), 243.025 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.378 Gbps (per GPU), 243.027 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.379 Gbps (per GPU), 243.031 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.378 Gbps (per GPU), 243.025 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.378 Gbps (per GPU), 243.023 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.378 Gbps (per GPU), 243.027 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.379 Gbps (per GPU), 243.031 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.377 Gbps (per GPU), 243.016 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.43ms  2.57ms  2.34ms  1.10  8.38K  3.53M
 chk_1  2.89ms  2.80ms  2.63ms  1.10  6.74K  3.60M
 chk_2  2.79ms  2.68ms  2.53ms  1.10  7.27K  3.53M
 chk_3  2.82ms  2.71ms  2.54ms  1.11  7.92K  3.61M
 chk_4  2.74ms  2.68ms  2.56ms  1.07  5.33K  3.68M
 chk_5  2.71ms  2.62ms  2.39ms  1.14 10.07K  3.45M
 chk_6  2.87ms  2.76ms  2.54ms  1.13  9.41K  3.48M
 chk_7  2.73ms  2.66ms  2.47ms  1.11  8.12K  3.60M
 chk_8  2.84ms  2.76ms  2.63ms  1.08  6.09K  3.64M
 chk_9  2.65ms  2.53ms  2.26ms  1.17 11.10K  3.38M
chk_10  2.87ms  2.79ms  2.68ms  1.07  5.67K  3.63M
chk_11  2.73ms  2.65ms  2.47ms  1.11  8.16K  3.54M
chk_12  2.94ms  2.84ms  2.68ms  1.10  7.24K  3.55M
chk_13  2.77ms  2.72ms  2.59ms  1.07  5.41K  3.68M
chk_14  2.98ms  2.90ms  2.75ms  1.09  7.14K  3.53M
chk_15  2.86ms  2.78ms  2.58ms  1.11  9.25K  3.49M
chk_16  2.71ms  2.67ms  2.57ms  1.06  4.78K  3.77M
chk_17  2.83ms  2.74ms  2.59ms  1.09  6.85K  3.60M
chk_18  2.65ms  2.55ms  2.40ms  1.10  7.47K  3.57M
chk_19  2.69ms  2.66ms  2.54ms  1.06  4.88K  3.75M
chk_20  2.70ms  2.62ms  2.48ms  1.09  7.00K  3.63M
chk_21  2.68ms  2.61ms  2.49ms  1.08  5.41K  3.68M
chk_22  2.87ms  2.74ms  2.48ms  1.16 11.07K  3.39M
chk_23  2.82ms  2.71ms  2.68ms  1.05  7.23K  3.64M
chk_24  2.83ms  2.73ms  2.48ms  1.14 10.13K  3.43M
chk_25  2.66ms  2.57ms  2.43ms  1.10  6.40K  3.57M
chk_26  2.86ms  2.78ms  2.65ms  1.08  5.78K  3.55M
chk_27  2.83ms  2.64ms  2.45ms  1.16  9.34K  3.48M
chk_28  3.03ms  2.94ms  2.79ms  1.09  6.37K  3.57M
chk_29  2.86ms  2.83ms  2.68ms  1.07  5.16K  3.78M
chk_30  2.73ms  2.70ms  2.57ms  1.06  5.44K  3.67M
chk_31  2.88ms  3.18ms  2.71ms  1.17  6.33K  3.63M
   Avg  2.79  2.72  2.55
   Max  3.03  3.18  2.79
   Min  2.43  2.53  2.26
 Ratio  1.25  1.26  1.23
   Var  0.01  0.02  0.01
Profiling takes 2.987 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 350.627 ms
Partition 0 [0, 4) has cost: 350.627 ms
Partition 1 [4, 8) has cost: 348.515 ms
Partition 2 [8, 12) has cost: 348.515 ms
Partition 3 [12, 16) has cost: 348.515 ms
Partition 4 [16, 20) has cost: 348.515 ms
Partition 5 [20, 24) has cost: 348.515 ms
Partition 6 [24, 28) has cost: 348.515 ms
Partition 7 [28, 32) has cost: 343.006 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 164.050 ms
GPU 0, Compute+Comm Time: 130.089 ms, Bubble Time: 29.206 ms, Imbalance Overhead: 4.755 ms
GPU 1, Compute+Comm Time: 128.664 ms, Bubble Time: 28.895 ms, Imbalance Overhead: 6.491 ms
GPU 2, Compute+Comm Time: 128.664 ms, Bubble Time: 28.788 ms, Imbalance Overhead: 6.599 ms
GPU 3, Compute+Comm Time: 128.664 ms, Bubble Time: 28.631 ms, Imbalance Overhead: 6.756 ms
GPU 4, Compute+Comm Time: 128.664 ms, Bubble Time: 28.525 ms, Imbalance Overhead: 6.862 ms
GPU 5, Compute+Comm Time: 128.664 ms, Bubble Time: 28.541 ms, Imbalance Overhead: 6.845 ms
GPU 6, Compute+Comm Time: 128.664 ms, Bubble Time: 28.558 ms, Imbalance Overhead: 6.829 ms
GPU 7, Compute+Comm Time: 126.711 ms, Bubble Time: 28.751 ms, Imbalance Overhead: 8.588 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 321.722 ms
GPU 0, Compute+Comm Time: 242.399 ms, Bubble Time: 54.434 ms, Imbalance Overhead: 24.889 ms
GPU 1, Compute+Comm Time: 245.955 ms, Bubble Time: 55.371 ms, Imbalance Overhead: 20.397 ms
GPU 2, Compute+Comm Time: 245.955 ms, Bubble Time: 56.855 ms, Imbalance Overhead: 18.913 ms
GPU 3, Compute+Comm Time: 245.955 ms, Bubble Time: 58.339 ms, Imbalance Overhead: 17.429 ms
GPU 4, Compute+Comm Time: 245.955 ms, Bubble Time: 59.823 ms, Imbalance Overhead: 15.945 ms
GPU 5, Compute+Comm Time: 245.955 ms, Bubble Time: 61.307 ms, Imbalance Overhead: 14.461 ms
GPU 6, Compute+Comm Time: 245.955 ms, Bubble Time: 62.757 ms, Imbalance Overhead: 13.011 ms
GPU 7, Compute+Comm Time: 246.641 ms, Bubble Time: 64.984 ms, Imbalance Overhead: 10.098 ms
The estimated cost of the whole pipeline: 510.061 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 699.142 ms
Partition 0 [0, 8) has cost: 699.142 ms
Partition 1 [8, 16) has cost: 697.030 ms
Partition 2 [16, 24) has cost: 697.030 ms
Partition 3 [24, 32) has cost: 691.521 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 171.752 ms
GPU 0, Compute+Comm Time: 141.365 ms, Bubble Time: 27.202 ms, Imbalance Overhead: 3.185 ms
GPU 1, Compute+Comm Time: 140.561 ms, Bubble Time: 26.706 ms, Imbalance Overhead: 4.485 ms
GPU 2, Compute+Comm Time: 140.561 ms, Bubble Time: 26.438 ms, Imbalance Overhead: 4.753 ms
GPU 3, Compute+Comm Time: 139.695 ms, Bubble Time: 26.160 ms, Imbalance Overhead: 5.896 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 325.547 ms
GPU 0, Compute+Comm Time: 259.306 ms, Bubble Time: 48.206 ms, Imbalance Overhead: 18.035 ms
GPU 1, Compute+Comm Time: 261.034 ms, Bubble Time: 51.094 ms, Imbalance Overhead: 13.418 ms
GPU 2, Compute+Comm Time: 261.034 ms, Bubble Time: 54.378 ms, Imbalance Overhead: 10.134 ms
GPU 3, Compute+Comm Time: 261.263 ms, Bubble Time: 58.362 ms, Imbalance Overhead: 5.921 ms
    The estimated cost with 2 DP ways is 522.164 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1396.172 ms
Partition 0 [0, 16) has cost: 1396.172 ms
Partition 1 [16, 32) has cost: 1388.551 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 218.473 ms
GPU 0, Compute+Comm Time: 189.866 ms, Bubble Time: 23.504 ms, Imbalance Overhead: 5.103 ms
GPU 1, Compute+Comm Time: 189.054 ms, Bubble Time: 23.985 ms, Imbalance Overhead: 5.434 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 363.195 ms
GPU 0, Compute+Comm Time: 315.002 ms, Bubble Time: 38.669 ms, Imbalance Overhead: 9.524 ms
GPU 1, Compute+Comm Time: 315.936 ms, Bubble Time: 44.160 ms, Imbalance Overhead: 3.099 ms
    The estimated cost with 4 DP ways is 610.752 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2784.724 ms
Partition 0 [0, 32) has cost: 2784.724 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 613.540 ms
GPU 0, Compute+Comm Time: 613.540 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 747.633 ms
GPU 0, Compute+Comm Time: 747.633 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1429.231 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 24)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [24, 48)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [48, 72)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [72, 96)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [96, 120)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [120, 144)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [144, 168)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [168, 190)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[24, 48)...
+++++++++ Node 2 initializing the weights for op[48, 72)...
+++++++++ Node 3 initializing the weights for op[72, 96)...
+++++++++ Node 0 initializing the weights for op[0, 24)...
+++++++++ Node 4 initializing the weights for op[96, 120)...
+++++++++ Node 6 initializing the weights for op[144, 168)...
+++++++++ Node 7 initializing the weights for op[168, 190)...
+++++++++ Node 5 initializing the weights for op[120, 144)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
*** Node 4, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.3245
	Epoch 50:	Loss 3.0917
	Epoch 75:	Loss 2.9066
	Epoch 100:	Loss 2.6850
	Epoch 125:	Loss 2.3417
	Epoch 150:	Loss 2.0448
	Epoch 175:	Loss 1.8108
	Epoch 200:	Loss 1.6195
	Epoch 225:	Loss 1.4290
	Epoch 250:	Loss 1.3226
	Epoch 275:	Loss 1.2198
	Epoch 300:	Loss 1.1446
	Epoch 325:	Loss 1.0801
	Epoch 350:	Loss 1.0314
	Epoch 375:	Loss 0.9968
	Epoch 400:	Loss 0.9728
	Epoch 425:	Loss 0.9540
	Epoch 450:	Loss 0.9578
	Epoch 475:	Loss 0.9078
	Epoch 500:	Loss 0.8625
	Epoch 525:	Loss 0.8446
	Epoch 550:	Loss 0.8224
	Epoch 575:	Loss 0.7868
	Epoch 600:	Loss 0.7671
	Epoch 625:	Loss 0.7684
	Epoch 650:	Loss 0.7317
	Epoch 675:	Loss 0.7218
	Epoch 700:	Loss 0.7197
	Epoch 725:	Loss 0.7096
	Epoch 750:	Loss 0.6897
	Epoch 775:	Loss 0.6783
	Epoch 800:	Loss 0.6734
	Epoch 825:	Loss 0.6586
	Epoch 850:	Loss 0.6529
	Epoch 875:	Loss 0.6604
	Epoch 900:	Loss 0.6441
	Epoch 925:	Loss 0.6520
	Epoch 950:	Loss 0.6263
	Epoch 975:	Loss 0.6118
	Epoch 1000:	Loss 0.6202
	Epoch 1025:	Loss 0.6731
	Epoch 1050:	Loss 0.6258
	Epoch 1075:	Loss 0.6110
	Epoch 1100:	Loss 0.6031
	Epoch 1125:	Loss 0.5912
	Epoch 1150:	Loss 0.5966
	Epoch 1175:	Loss 0.5662
	Epoch 1200:	Loss 0.5631
	Epoch 1225:	Loss 0.5760
	Epoch 1250:	Loss 0.5533
	Epoch 1275:	Loss 0.5499
	Epoch 1300:	Loss 0.5377
	Epoch 1325:	Loss 0.5572
	Epoch 1350:	Loss 0.5415
	Epoch 1375:	Loss 0.5390
	Epoch 1400:	Loss 0.5324
	Epoch 1425:	Loss 0.5259
	Epoch 1450:	Loss 0.5253
	Epoch 1475:	Loss 0.5214
	Epoch 1500:	Loss 0.5232
	Epoch 1525:	Loss 0.5408
	Epoch 1550:	Loss 0.5271
	Epoch 1575:	Loss 0.5189
	Epoch 1600:	Loss 0.5196
	Epoch 1625:	Loss 0.5214
	Epoch 1650:	Loss 0.5096
	Epoch 1675:	Loss 0.5049
	Epoch 1700:	Loss 0.4950
	Epoch 1725:	Loss 0.5005
	Epoch 1750:	Loss 0.4933
	Epoch 1775:	Loss 0.4860
	Epoch 1800:	Loss 0.4874
	Epoch 1825:	Loss 0.4864
	Epoch 1850:	Loss 0.4939
	Epoch 1875:	Loss 0.4950
	Epoch 1900:	Loss 0.4985
	Epoch 1925:	Loss 0.5163
	Epoch 1950:	Loss 0.4909
	Epoch 1975:	Loss 0.4851
	Epoch 2000:	Loss 0.4833
	Epoch 2025:	Loss 0.5002
	Epoch 2050:	Loss 0.4755
	Epoch 2075:	Loss 0.4778
	Epoch 2100:	Loss 0.4726
	Epoch 2125:	Loss 0.4690
	Epoch 2150:	Loss 0.4706
	Epoch 2175:	Loss 0.4678
	Epoch 2200:	Loss 0.4659
	Epoch 2225:	Loss 0.4681
	Epoch 2250:	Loss 0.4582
	Epoch 2275:	Loss 0.4616
	Epoch 2300:	Loss 0.4593
	Epoch 2325:	Loss 0.4746
	Epoch 2350:	Loss 0.4533
	Epoch 2375:	Loss 0.4564
	Epoch 2400:	Loss 0.4560
	Epoch 2425:	Loss 0.4604
	Epoch 2450:	Loss 0.4650
	Epoch 2475:	Loss 0.4627
	Epoch 2500:	Loss 0.4580
	Epoch 2525:	Loss 0.4609
	Epoch 2550:	Loss 0.4629
	Epoch 2575:	Loss 0.4570
	Epoch 2600:	Loss 0.4651
	Epoch 2625:	Loss 0.4948
	Epoch 2650:	Loss 0.4834
	Epoch 2675:	Loss 0.4893
	Epoch 2700:	Loss 0.4917
	Epoch 2725:	Loss 0.4850
	Epoch 2750:	Loss 0.4733
	Epoch 2775:	Loss 0.4688
	Epoch 2800:	Loss 0.4615
	Epoch 2825:	Loss 0.4543
	Epoch 2850:	Loss 0.4436
	Epoch 2875:	Loss 0.4453
	Epoch 2900:	Loss 0.4415
	Epoch 2925:	Loss 0.4442
	Epoch 2950:	Loss 0.4445
	Epoch 2975:	Loss 0.4473
	Epoch 3000:	Loss 0.4412
	Epoch 3025:	Loss 0.4636
	Epoch 3050:	Loss 0.4894
	Epoch 3075:	Loss 0.4869
	Epoch 3100:	Loss 0.4722
	Epoch 3125:	Loss 0.4667
	Epoch 3150:	Loss 0.4628
	Epoch 3175:	Loss 0.4527
	Epoch 3200:	Loss 0.4396
	Epoch 3225:	Loss 0.4460
	Epoch 3250:	Loss 0.4530
	Epoch 3275:	Loss 0.4440
	Epoch 3300:	Loss 0.4413
	Epoch 3325:	Loss 0.4424
	Epoch 3350:	Loss 0.4349
	Epoch 3375:	Loss 0.4299
	Epoch 3400:	Loss 0.4275
	Epoch 3425:	Loss 0.4258
	Epoch 3450:	Loss 0.4199
	Epoch 3475:	Loss 0.4205
	Epoch 3500:	Loss 0.4243
	Epoch 3525:	Loss 0.4206
	Epoch 3550:	Loss 0.4220
	Epoch 3575:	Loss 0.4222
	Epoch 3600:	Loss 0.4180
	Epoch 3625:	Loss 0.4202
	Epoch 3650:	Loss 0.4211
	Epoch 3675:	Loss 0.4178
	Epoch 3700:	Loss 0.4149
	Epoch 3725:	Loss 0.4180
	Epoch 3750:	Loss 0.4141
	Epoch 3775:	Loss 0.4125
	Epoch 3800:	Loss 0.4098
	Epoch 3825:	Loss 0.4139
	Epoch 3850:	Loss 0.4127
	Epoch 3875:	Loss 0.4111
	Epoch 3900:	Loss 0.4126
	Epoch 3925:	Loss 0.4119
	Epoch 3950:	Loss 0.4126
	Epoch 3975:	Loss 0.4201
	Epoch 4000:	Loss 0.4268
	Epoch 4025:	Loss 0.4374
	Epoch 4050:	Loss 0.4197
	Epoch 4075:	Loss 0.4279
	Epoch 4100:	Loss 0.4212
	Epoch 4125:	Loss 0.4234
	Epoch 4150:	Loss 0.4208
	Epoch 4175:	Loss 0.4158
	Epoch 4200:	Loss 0.4095
	Epoch 4225:	Loss 0.4113
	Epoch 4250:	Loss 0.4074
	Epoch 4275:	Loss 0.4053
	Epoch 4300:	Loss 0.4018
	Epoch 4325:	Loss 0.4200
	Epoch 4350:	Loss 0.4378
	Epoch 4375:	Loss 0.4157
	Epoch 4400:	Loss 0.4348
	Epoch 4425:	Loss 0.4452
	Epoch 4450:	Loss 0.4418
	Epoch 4475:	Loss 0.4390
	Epoch 4500:	Loss 0.4265
	Epoch 4525:	Loss 0.4453
	Epoch 4550:	Loss 0.4385
	Epoch 4575:	Loss 0.4296
	Epoch 4600:	Loss 0.4244
	Epoch 4625:	Loss 0.4137
	Epoch 4650:	Loss 0.4085
	Epoch 4675:	Loss 0.4069
	Epoch 4700:	Loss 0.3994
	Epoch 4725:	Loss 0.3990
	Epoch 4750:	Loss 0.3976
	Epoch 4775:	Loss 0.3965
	Epoch 4800:	Loss 0.3992
	Epoch 4825:	Loss 0.3982
	Epoch 4850:	Loss 0.3947
	Epoch 4875:	Loss 0.3953
	Epoch 4900:	Loss 0.3952
	Epoch 4925:	Loss 0.3934
	Epoch 4950:	Loss 0.3912
	Epoch 4975:	Loss 0.3926
	Epoch 5000:	Loss 0.3906
Node 1, Pre/Post-Pipelining: 1.089 / 0.862 ms, Bubble: 76.075 ms, Compute: 259.478 ms, Comm: 27.661 ms, Imbalance: 13.774 ms
Node 2, Pre/Post-Pipelining: 1.087 / 0.878 ms, Bubble: 76.431 ms, Compute: 248.008 ms, Comm: 29.763 ms, Imbalance: 23.160 ms
Node 5, Pre/Post-Pipelining: 1.089 / 0.848 ms, Bubble: 75.653 ms, Compute: 251.712 ms, Comm: 30.716 ms, Imbalance: 19.320 ms
Node 3, Pre/Post-Pipelining: 1.086 / 0.891 ms, Bubble: 75.733 ms, Compute: 252.566 ms, Comm: 26.643 ms, Imbalance: 22.249 ms
Node 7, Pre/Post-Pipelining: 1.087 / 16.960 ms, Bubble: 59.410 ms, Compute: 268.172 ms, Comm: 16.967 ms, Imbalance: 16.532 ms
Node 0, Pre/Post-Pipelining: 1.086 / 0.941 ms, Bubble: 76.664 ms, Compute: 263.101 ms, Comm: 16.994 ms, Imbalance: 20.050 ms
Node 4, Pre/Post-Pipelining: 1.085 / 0.883 ms, Bubble: 75.932 ms, Compute: 246.019 ms, Comm: 26.288 ms, Imbalance: 29.329 ms
Node 6, Pre/Post-Pipelining: 1.087 / 0.881 ms, Bubble: 75.414 ms, Compute: 252.518 ms, Comm: 29.126 ms, Imbalance: 20.317 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 1.086 ms
Cluster-Wide Average, Post-Pipelining Overhead: 0.941 ms
Cluster-Wide Average, Bubble: 76.664 ms
Cluster-Wide Average, Compute: 263.101 ms
Cluster-Wide Average, Communication: 16.994 ms
Cluster-Wide Average, Imbalance: 20.050 ms
Node 0, GPU memory consumption: 6.739 GB
Node 2, GPU memory consumption: 5.819 GB
Node 3, GPU memory consumption: 5.796 GB
Node 1, GPU memory consumption: 5.819 GB
Node 4, GPU memory consumption: 5.796 GB
Node 7, GPU memory consumption: 5.569 GB
Node 5, GPU memory consumption: 5.819 GB
Node 6, GPU memory consumption: 5.819 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.379521 s---------------
------------------------node id 1,  per-epoch time: 0.379521 s---------------
------------------------node id 2,  per-epoch time: 0.379521 s---------------
------------------------node id 3,  per-epoch time: 0.379521 s---------------
------------------------node id 4,  per-epoch time: 0.379521 s---------------
------------------------node id 5,  per-epoch time: 0.379521 s---------------
------------------------node id 6,  per-epoch time: 0.379521 s---------------
------------------------node id 7,  per-epoch time: 0.379521 s---------------
************ Profiling Results ************
	Bubble: 125.148430 (ms) (32.97 percentage)
	Compute: 250.874149 (ms) (66.08 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 3.603117 (ms) (0.95 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.215 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
	Total communication (cluster-wide, per-epoch): 1.216 GB
	Aggregated layer-level communication throughput: 408.973 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
