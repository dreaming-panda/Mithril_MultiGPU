gnerv1
Tue Aug  1 15:39:32 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   33C    P8    25W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   33C    P8    25W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   32C    P8    20W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   30C    P8    23W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 22%] Built target context
[ 36%] Built target core
[ 77%] Built target cudahelp
[ 88%] Built target OSDI2023_MULTI_NODES_graphsage
[ 88%] Built target OSDI2023_MULTI_NODES_gcnii
[ 88%] Built target OSDI2023_MULTI_NODES_gcn
[100%] Built target estimate_comm_volume
Initialized node 0 on machine gnerv1
Initialized node 1 on machine gnerv1
Initialized node 3 on machine gnerv1
Initialized node 2 on machine gnerv1
Initialized node 4 on machine gnerv2
Initialized node 7 on machine gnerv2
Initialized node 5 on machine gnerv2
Initialized node 6 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.010 seconds.
Building the CSC structure...
        It takes 2.061 seconds.
Building the CSC structure...
        It takes 2.071 seconds.
Building the CSC structure...
        It takes 2.362 seconds.
Building the CSC structure...
        It takes 2.363 seconds.
Building the CSC structure...
        It takes 2.395 seconds.
Building the CSC structure...
        It takes 2.436 seconds.
Building the CSC structure...
        It takes 2.678 seconds.
Building the CSC structure...
        It takes 1.849 seconds.
        It takes 1.923 seconds.
        It takes 1.934 seconds.
        It takes 2.251 seconds.
        It takes 2.298 seconds.
        It takes 2.380 seconds.
        It takes 2.421 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.405 seconds.
        It takes 0.294 seconds.
Building the Label Vector...
        It takes 0.040 seconds.
        It takes 0.271 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
Building the Feature Vector...
        It takes 0.265 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.254 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
        It takes 0.310 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.328 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
        It takes 0.313 seconds.
Building the Label Vector...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
        It takes 0.038 seconds.
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.282 seconds.
Building the Label Vector...
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
        It takes 0.033 seconds.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 9)
GPU 1, layer [9, 17)
GPU 2, layer [17, 25)
GPU 3, layer [25, 33)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.078 Gbps (per GPU), 448.622 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.820 Gbps (per GPU), 446.560 Gbps (aggregated)
The layer-level communication performance: 55.815 Gbps (per GPU), 446.524 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.585 Gbps (per GPU), 444.679 Gbps (aggregated)
The layer-level communication performance: 55.558 Gbps (per GPU), 444.467 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.372 Gbps (per GPU), 442.978 Gbps (aggregated)
The layer-level communication performance: 55.324 Gbps (per GPU), 442.592 Gbps (aggregated)
The layer-level communication performance: 55.300 Gbps (per GPU), 442.397 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.020 Gbps (per GPU), 1272.158 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.999 Gbps (per GPU), 1271.989 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.023 Gbps (per GPU), 1272.182 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.999 Gbps (per GPU), 1271.989 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.884 Gbps (per GPU), 1271.073 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.999 Gbps (per GPU), 1271.989 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.890 Gbps (per GPU), 1271.122 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.002 Gbps (per GPU), 1272.016 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.049 Gbps (per GPU), 832.388 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.049 Gbps (per GPU), 832.389 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.049 Gbps (per GPU), 832.396 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.049 Gbps (per GPU), 832.396 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.048 Gbps (per GPU), 832.382 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.045 Gbps (per GPU), 832.362 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.048 Gbps (per GPU), 832.382 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.045 Gbps (per GPU), 832.361 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.136 Gbps (per GPU), 265.089 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.136 Gbps (per GPU), 265.089 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.134 Gbps (per GPU), 265.074 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.134 Gbps (per GPU), 265.073 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.136 Gbps (per GPU), 265.087 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.135 Gbps (per GPU), 265.082 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.135 Gbps (per GPU), 265.081 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.134 Gbps (per GPU), 265.076 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.88ms  2.39ms  2.71ms  3.08  8.38K  3.53M
 chk_1  0.75ms  2.77ms  2.91ms  3.87  6.74K  3.60M
 chk_2  0.79ms  2.61ms  2.78ms  3.53  7.27K  3.53M
 chk_3  0.80ms  2.67ms  2.83ms  3.55  7.92K  3.61M
 chk_4  0.62ms  2.58ms  2.93ms  4.70  5.33K  3.68M
 chk_5  0.99ms  2.60ms  2.75ms  2.77 10.07K  3.45M
 chk_6  0.95ms  2.75ms  2.92ms  3.08  9.41K  3.48M
 chk_7  0.81ms  2.58ms  3.12ms  3.84  8.12K  3.60M
 chk_8  0.67ms  2.70ms  2.83ms  4.21  6.09K  3.64M
 chk_9  1.08ms  2.51ms  2.69ms  2.49 11.10K  3.38M
chk_10  0.64ms  2.73ms  2.89ms  4.49  5.67K  3.63M
chk_11  0.81ms  2.60ms  2.76ms  3.39  8.16K  3.54M
chk_12  0.79ms  2.80ms  2.95ms  3.76  7.24K  3.55M
chk_13  0.63ms  2.63ms  2.78ms  4.44  5.41K  3.68M
chk_14  0.77ms  2.89ms  3.03ms  3.94  7.14K  3.53M
chk_15  0.94ms  2.73ms  2.92ms  3.10  9.25K  3.49M
chk_16  0.59ms  2.57ms  2.73ms  4.60  4.78K  3.77M
chk_17  0.76ms  2.66ms  2.85ms  3.77  6.85K  3.60M
chk_18  0.80ms  2.48ms  2.67ms  3.34  7.47K  3.57M
chk_19  0.60ms  2.54ms  2.71ms  4.54  4.88K  3.75M
chk_20  0.76ms  2.57ms  2.73ms  3.59  7.00K  3.63M
chk_21  0.63ms  2.55ms  2.70ms  4.30  5.41K  3.68M
chk_22  1.09ms  2.78ms  2.97ms  2.73 11.07K  3.39M
chk_23  0.78ms  2.66ms  2.82ms  3.60  7.23K  3.64M
chk_24  1.00ms  2.70ms  2.88ms  2.88 10.13K  3.43M
chk_25  0.72ms  2.52ms  2.68ms  3.71  6.40K  3.57M
chk_26  0.65ms  2.71ms  2.88ms  4.41  5.78K  3.55M
chk_27  0.95ms  2.59ms  2.80ms  2.95  9.34K  3.48M
chk_28  0.72ms  2.88ms  3.07ms  4.27  6.37K  3.57M
chk_29  0.62ms  2.70ms  2.87ms  4.61  5.16K  3.78M
chk_30  0.63ms  2.58ms  2.75ms  4.37  5.44K  3.67M
chk_31  0.72ms  2.73ms  2.90ms  4.06  6.33K  3.63M
   Avg  0.78  2.65  2.84
   Max  1.09  2.89  3.12
   Min  0.59  2.39  2.67
 Ratio  1.83  1.21  1.17
   Var  0.02  0.01  0.01
Profiling takes 2.427 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 364.008 ms
Partition 0 [0, 5) has cost: 364.008 ms
Partition 1 [5, 9) has cost: 339.062 ms
Partition 2 [9, 13) has cost: 339.062 ms
Partition 3 [13, 17) has cost: 339.062 ms
Partition 4 [17, 21) has cost: 339.062 ms
Partition 5 [21, 25) has cost: 339.062 ms
Partition 6 [25, 29) has cost: 339.062 ms
Partition 7 [29, 33) has cost: 345.131 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 165.137 ms
GPU 0, Compute+Comm Time: 132.823 ms, Bubble Time: 28.821 ms, Imbalance Overhead: 3.494 ms
GPU 1, Compute+Comm Time: 125.853 ms, Bubble Time: 28.524 ms, Imbalance Overhead: 10.760 ms
GPU 2, Compute+Comm Time: 125.853 ms, Bubble Time: 28.613 ms, Imbalance Overhead: 10.671 ms
GPU 3, Compute+Comm Time: 125.853 ms, Bubble Time: 28.520 ms, Imbalance Overhead: 10.764 ms
GPU 4, Compute+Comm Time: 125.853 ms, Bubble Time: 28.494 ms, Imbalance Overhead: 10.790 ms
GPU 5, Compute+Comm Time: 125.853 ms, Bubble Time: 28.567 ms, Imbalance Overhead: 10.717 ms
GPU 6, Compute+Comm Time: 125.853 ms, Bubble Time: 28.833 ms, Imbalance Overhead: 10.451 ms
GPU 7, Compute+Comm Time: 127.221 ms, Bubble Time: 29.228 ms, Imbalance Overhead: 8.689 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 317.768 ms
GPU 0, Compute+Comm Time: 244.498 ms, Bubble Time: 56.624 ms, Imbalance Overhead: 16.646 ms
GPU 1, Compute+Comm Time: 239.796 ms, Bubble Time: 55.813 ms, Imbalance Overhead: 22.159 ms
GPU 2, Compute+Comm Time: 239.796 ms, Bubble Time: 55.198 ms, Imbalance Overhead: 22.773 ms
GPU 3, Compute+Comm Time: 239.796 ms, Bubble Time: 55.074 ms, Imbalance Overhead: 22.898 ms
GPU 4, Compute+Comm Time: 239.796 ms, Bubble Time: 55.096 ms, Imbalance Overhead: 22.876 ms
GPU 5, Compute+Comm Time: 239.796 ms, Bubble Time: 55.174 ms, Imbalance Overhead: 22.798 ms
GPU 6, Compute+Comm Time: 239.796 ms, Bubble Time: 54.915 ms, Imbalance Overhead: 23.057 ms
GPU 7, Compute+Comm Time: 257.773 ms, Bubble Time: 55.452 ms, Imbalance Overhead: 4.543 ms
The estimated cost of the whole pipeline: 507.050 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 703.070 ms
Partition 0 [0, 9) has cost: 703.070 ms
Partition 1 [9, 17) has cost: 678.123 ms
Partition 2 [17, 25) has cost: 678.123 ms
Partition 3 [25, 33) has cost: 684.193 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 171.515 ms
GPU 0, Compute+Comm Time: 141.844 ms, Bubble Time: 26.857 ms, Imbalance Overhead: 2.815 ms
GPU 1, Compute+Comm Time: 138.069 ms, Bubble Time: 26.455 ms, Imbalance Overhead: 6.992 ms
GPU 2, Compute+Comm Time: 138.069 ms, Bubble Time: 26.378 ms, Imbalance Overhead: 7.069 ms
GPU 3, Compute+Comm Time: 138.816 ms, Bubble Time: 26.135 ms, Imbalance Overhead: 6.563 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 316.851 ms
GPU 0, Compute+Comm Time: 256.162 ms, Bubble Time: 48.489 ms, Imbalance Overhead: 12.200 ms
GPU 1, Compute+Comm Time: 253.676 ms, Bubble Time: 48.772 ms, Imbalance Overhead: 14.403 ms
GPU 2, Compute+Comm Time: 253.676 ms, Bubble Time: 48.979 ms, Imbalance Overhead: 14.195 ms
GPU 3, Compute+Comm Time: 263.611 ms, Bubble Time: 50.000 ms, Imbalance Overhead: 3.240 ms
    The estimated cost with 2 DP ways is 512.785 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1381.193 ms
Partition 0 [0, 17) has cost: 1381.193 ms
Partition 1 [17, 33) has cost: 1362.316 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 217.159 ms
GPU 0, Compute+Comm Time: 188.609 ms, Bubble Time: 23.142 ms, Imbalance Overhead: 5.408 ms
GPU 1, Compute+Comm Time: 186.893 ms, Bubble Time: 23.736 ms, Imbalance Overhead: 6.529 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 354.343 ms
GPU 0, Compute+Comm Time: 306.308 ms, Bubble Time: 38.831 ms, Imbalance Overhead: 9.204 ms
GPU 1, Compute+Comm Time: 310.561 ms, Bubble Time: 38.217 ms, Imbalance Overhead: 5.564 ms
    The estimated cost with 4 DP ways is 600.076 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2743.509 ms
Partition 0 [0, 33) has cost: 2743.509 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 571.573 ms
GPU 0, Compute+Comm Time: 571.573 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 695.222 ms
GPU 0, Compute+Comm Time: 695.222 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1330.135 ms

*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 62)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 62)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [62, 118)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [62, 118)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [118, 174)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [118, 174)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [174, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [174, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 3 initializing the weights for op[62, 118)...
+++++++++ Node 1 initializing the weights for op[0, 62)...
+++++++++ Node 2 initializing the weights for op[62, 118)...
+++++++++ Node 0 initializing the weights for op[0, 62)...
+++++++++ Node 5 initializing the weights for op[118, 174)...
+++++++++ Node 6 initializing the weights for op[174, 233)...
+++++++++ Node 7 initializing the weights for op[174, 233)...
+++++++++ Node 4 initializing the weights for op[118, 174)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 2, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 4, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 2, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.5702
	Epoch 50:	Loss 1.8936
	Epoch 75:	Loss 1.5142
	Epoch 100:	Loss 1.2888
	Epoch 125:	Loss 1.1026
	Epoch 150:	Loss 1.0063
	Epoch 175:	Loss 0.9341
	Epoch 200:	Loss 0.8841
	Epoch 225:	Loss 0.8320
	Epoch 250:	Loss 0.7996
	Epoch 275:	Loss 0.7705
	Epoch 300:	Loss 0.7447
	Epoch 325:	Loss 0.7260
	Epoch 350:	Loss 0.7084
	Epoch 375:	Loss 0.6956
	Epoch 400:	Loss 0.6767
	Epoch 425:	Loss 0.6619
	Epoch 450:	Loss 0.6489
	Epoch 475:	Loss 0.6384
	Epoch 500:	Loss 0.6275
	Epoch 525:	Loss 0.6199
	Epoch 550:	Loss 0.6112
	Epoch 575:	Loss 0.6051
	Epoch 600:	Loss 0.5989
	Epoch 625:	Loss 0.5886
	Epoch 650:	Loss 0.5801
	Epoch 675:	Loss 0.5801
	Epoch 700:	Loss 0.5701
	Epoch 725:	Loss 0.5675
	Epoch 750:	Loss 0.5609
	Epoch 775:	Loss 0.5554
	Epoch 800:	Loss 0.5530
	Epoch 825:	Loss 0.5460
	Epoch 850:	Loss 0.5410
	Epoch 875:	Loss 0.5402
	Epoch 900:	Loss 0.5352
	Epoch 925:	Loss 0.5303
	Epoch 950:	Loss 0.5290
	Epoch 975:	Loss 0.5241
	Epoch 1000:	Loss 0.5214
	Epoch 1025:	Loss 0.5182
	Epoch 1050:	Loss 0.5214
	Epoch 1075:	Loss 0.5155
	Epoch 1100:	Loss 0.5080
	Epoch 1125:	Loss 0.5058
	Epoch 1150:	Loss 0.5077
	Epoch 1175:	Loss 0.5030
	Epoch 1200:	Loss 0.5027
	Epoch 1225:	Loss 0.4956
	Epoch 1250:	Loss 0.4944
	Epoch 1275:	Loss 0.4919
	Epoch 1300:	Loss 0.4887
	Epoch 1325:	Loss 0.4876
	Epoch 1350:	Loss 0.4855
	Epoch 1375:	Loss 0.4843
	Epoch 1400:	Loss 0.4813
	Epoch 1425:	Loss 0.4829
	Epoch 1450:	Loss 0.4829
	Epoch 1475:	Loss 0.4779
	Epoch 1500:	Loss 0.4743
	Epoch 1525:	Loss 0.4745
	Epoch 1550:	Loss 0.4740
	Epoch 1575:	Loss 0.4749
	Epoch 1600:	Loss 0.4702
	Epoch 1625:	Loss 0.4675
	Epoch 1650:	Loss 0.4666
	Epoch 1675:	Loss 0.4654
	Epoch 1700:	Loss 0.4661
	Epoch 1725:	Loss 0.4660
	Epoch 1750:	Loss 0.4623
	Epoch 1775:	Loss 0.4613
	Epoch 1800:	Loss 0.4609
	Epoch 1825:	Loss 0.4599
	Epoch 1850:	Loss 0.4614
	Epoch 1875:	Loss 0.4546
	Epoch 1900:	Loss 0.4530
	Epoch 1925:	Loss 0.4548
	Epoch 1950:	Loss 0.4523
	Epoch 1975:	Loss 0.4504
	Epoch 2000:	Loss 0.4502
	Epoch 2025:	Loss 0.4493
	Epoch 2050:	Loss 0.4461
	Epoch 2075:	Loss 0.4473
	Epoch 2100:	Loss 0.4474
	Epoch 2125:	Loss 0.4431
	Epoch 2150:	Loss 0.4426
	Epoch 2175:	Loss 0.4422
	Epoch 2200:	Loss 0.4416
	Epoch 2225:	Loss 0.4423
	Epoch 2250:	Loss 0.4411
	Epoch 2275:	Loss 0.4389
	Epoch 2300:	Loss 0.4395
	Epoch 2325:	Loss 0.4379
	Epoch 2350:	Loss 0.4372
	Epoch 2375:	Loss 0.4352
	Epoch 2400:	Loss 0.4349
	Epoch 2425:	Loss 0.4338
	Epoch 2450:	Loss 0.4338
	Epoch 2475:	Loss 0.4326
	Epoch 2500:	Loss 0.4335
	Epoch 2525:	Loss 0.4321
	Epoch 2550:	Loss 0.4300
	Epoch 2575:	Loss 0.4307
	Epoch 2600:	Loss 0.4303
	Epoch 2625:	Loss 0.4289
	Epoch 2650:	Loss 0.4306
	Epoch 2675:	Loss 0.4273
	Epoch 2700:	Loss 0.4296
	Epoch 2725:	Loss 0.4253
	Epoch 2750:	Loss 0.4267
	Epoch 2775:	Loss 0.4245
	Epoch 2800:	Loss 0.4233
	Epoch 2825:	Loss 0.4211
	Epoch 2850:	Loss 0.4217
	Epoch 2875:	Loss 0.4231
	Epoch 2900:	Loss 0.4206
	Epoch 2925:	Loss 0.4181
	Epoch 2950:	Loss 0.4186
	Epoch 2975:	Loss 0.4197
	Epoch 3000:	Loss 0.4182
	Epoch 3025:	Loss 0.4215
	Epoch 3050:	Loss 0.4162
	Epoch 3075:	Loss 0.4161
	Epoch 3100:	Loss 0.4169
	Epoch 3125:	Loss 0.4155
	Epoch 3150:	Loss 0.4108
	Epoch 3175:	Loss 0.4125
	Epoch 3200:	Loss 0.4146
	Epoch 3225:	Loss 0.4128
	Epoch 3250:	Loss 0.4139
	Epoch 3275:	Loss 0.4124
	Epoch 3300:	Loss 0.4091
	Epoch 3325:	Loss 0.4107
	Epoch 3350:	Loss 0.4119
	Epoch 3375:	Loss 0.4111
	Epoch 3400:	Loss 0.4081
	Epoch 3425:	Loss 0.4123
	Epoch 3450:	Loss 0.4105
	Epoch 3475:	Loss 0.4088
	Epoch 3500:	Loss 0.4075
	Epoch 3525:	Loss 0.4065
	Epoch 3550:	Loss 0.4082
	Epoch 3575:	Loss 0.4020
	Epoch 3600:	Loss 0.4066
	Epoch 3625:	Loss 0.4053
	Epoch 3650:	Loss 0.4075
	Epoch 3675:	Loss 0.4032
	Epoch 3700:	Loss 0.4051
	Epoch 3725:	Loss 0.4035
	Epoch 3750:	Loss 0.3970
	Epoch 3775:	Loss 0.4018
	Epoch 3800:	Loss 0.4058
	Epoch 3825:	Loss 0.4009
	Epoch 3850:	Loss 0.3980
	Epoch 3875:	Loss 0.3990
	Epoch 3900:	Loss 0.4019
	Epoch 3925:	Loss 0.3968
	Epoch 3950:	Loss 0.3993
	Epoch 3975:	Loss 0.3973
	Epoch 4000:	Loss 0.3970
	Epoch 4025:	Loss 0.3936
	Epoch 4050:	Loss 0.3975
	Epoch 4075:	Loss 0.3952
	Epoch 4100:	Loss 0.3967
	Epoch 4125:	Loss 0.3960
	Epoch 4150:	Loss 0.3917
	Epoch 4175:	Loss 0.3906
	Epoch 4200:	Loss 0.3921
	Epoch 4225:	Loss 0.3914
	Epoch 4250:	Loss 0.3917
	Epoch 4275:	Loss 0.3928
	Epoch 4300:	Loss 0.3934
	Epoch 4325:	Loss 0.3922
	Epoch 4350:	Loss 0.3925
	Epoch 4375:	Loss 0.3901
	Epoch 4400:	Loss 0.3901
	Epoch 4425:	Loss 0.3868
	Epoch 4450:	Loss 0.3921
	Epoch 4475:	Loss 0.3884
	Epoch 4500:	Loss 0.3853
	Epoch 4525:	Loss 0.3876
	Epoch 4550:	Loss 0.3859
	Epoch 4575:	Loss 0.3876
	Epoch 4600:	Loss 0.3854
	Epoch 4625:	Loss 0.3865
	Epoch 4650:	Loss 0.3859
	Epoch 4675:	Loss 0.3883
	Epoch 4700:	Loss 0.3859
	Epoch 4725:	Loss 0.3842
	Epoch 4750:	Loss 0.3808
	Epoch 4775:	Loss 0.3829
	Epoch 4800:	Loss 0.3816
	Epoch 4825:	Loss 0.3810
	Epoch 4850:	Loss 0.3814
	Epoch 4875:	Loss 0.3835
	Epoch 4900:	Loss 0.3798
	Epoch 4925:	Loss 0.3809
	Epoch 4950:	Loss 0.3811
	Epoch 4975:	Loss 0.3807
Node 2, Pre/Post-Pipelining: 2.841 / 3.451 ms, Bubble: 68.873 ms, Compute: 321.933 ms, Comm: 38.181 ms, Imbalance: 24.925 ms
Node 3, Pre/Post-Pipelining: 2.832 / 3.434 ms, Bubble: 68.916 ms, Compute: 325.242 ms, Comm: 34.549 ms, Imbalance: 25.246 ms
Node 6, Pre/Post-Pipelining: 3.028 / 11.301 ms, Bubble: 65.278 ms, Compute: 332.385 ms, Comm: 31.023 ms, Imbalance: 17.940 ms
Node 7, Pre/Post-Pipelining: 3.022 / 11.311 ms, Bubble: 65.084 ms, Compute: 337.078 ms, Comm: 26.010 ms, Imbalance: 18.274 ms
	Epoch 5000:	Loss 0.3811
Node 4, Pre/Post-Pipelining: 2.969 / 3.685 ms, Bubble: 70.815 ms, Compute: 319.725 ms, Comm: 36.723 ms, Imbalance: 27.685 ms
Node 5, Pre/Post-Pipelining: 2.963 / 3.685 ms, Bubble: 70.752 ms, Compute: 323.827 ms, Comm: 32.492 ms, Imbalance: 27.792 ms
Node 1, Pre/Post-Pipelining: 2.671 / 4.180 ms, Bubble: 68.747 ms, Compute: 346.804 ms, Comm: 29.021 ms, Imbalance: 7.885 ms
Node 0, Pre/Post-Pipelining: 2.675 / 4.167 ms, Bubble: 68.914 ms, Compute: 343.397 ms, Comm: 32.539 ms, Imbalance: 7.603 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 2.675 ms
Cluster-Wide Average, Post-Pipelining Overhead: 4.167 ms
Cluster-Wide Average, Bubble: 68.914 ms
Cluster-Wide Average, Compute: 343.397 ms
Cluster-Wide Average, Communication: 32.539 ms
Cluster-Wide Average, Imbalance: 7.603 ms
Node 0, GPU memory consumption: 9.405 GB
Node 3, GPU memory consumption: 7.096 GB
Node 1, GPU memory consumption: 8.118 GB
Node 6, GPU memory consumption: 7.337 GB
Node 7, GPU memory consumption: 7.210 GB
Node 4, GPU memory consumption: 7.196 GB
Node 5, GPU memory consumption: 7.120 GB
Node 2, GPU memory consumption: 7.219 GB
Node 0, Graph-Level Communication Throughput: 115.603 Gbps, Time: 52.306 ms
Node 1, Graph-Level Communication Throughput: 95.017 Gbps, Time: 57.146 ms
Node 4, Graph-Level Communication Throughput: 116.914 Gbps, Time: 51.719 ms
Node 2, Graph-Level Communication Throughput: 117.683 Gbps, Time: 51.382 ms
Node 5, Graph-Level Communication Throughput: 98.466 Gbps, Time: 55.145 ms
Node 3, Graph-Level Communication Throughput: 94.329 Gbps, Time: 57.563 ms
Node 6, Graph-Level Communication Throughput: 122.937 Gbps, Time: 49.185 ms
Node 7, Graph-Level Communication Throughput: 98.616 Gbps, Time: 55.061 ms
------------------------node id 0,  per-epoch time: 0.459658 s---------------
------------------------node id 1,  per-epoch time: 0.459658 s---------------
------------------------node id 2,  per-epoch time: 0.459657 s---------------
------------------------node id 3,  per-epoch time: 0.459658 s---------------
------------------------node id 4,  per-epoch time: 0.459657 s---------------
------------------------node id 5,  per-epoch time: 0.459657 s---------------
------------------------node id 6,  per-epoch time: 0.459657 s---------------
------------------------node id 7,  per-epoch time: 0.459657 s---------------
************ Profiling Results ************
	Bubble: 129.676413 (ms) (28.21 percentage)
	Compute: 259.986597 (ms) (56.55 percentage)
	GraphCommComputeOverhead: 9.969376 (ms) (2.17 percentage)
	GraphCommNetwork: 53.693564 (ms) (11.68 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 6.418594 (ms) (1.40 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.041 GB
	Graph-level communication (cluster-wide, per-epoch): 5.344 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
	Total communication (cluster-wide, per-epoch): 6.388 GB
	Aggregated layer-level communication throughput: 274.689 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 4] Success 
[MPI Rank 5] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
