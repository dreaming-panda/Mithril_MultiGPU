gnerv1
Tue Aug  1 17:37:41 2023
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   28C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   28C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   27C    P8    19W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   27C    P8    22W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 16%] Built target context
[ 36%] Built target core
[ 77%] Built target cudahelp
[ 88%] Built target estimate_comm_volume
[ 88%] Built target OSDI2023_MULTI_NODES_gcnii
[ 88%] Built target OSDI2023_MULTI_NODES_gcn
[100%] Built target OSDI2023_MULTI_NODES_graphsage
Running experiments...
gnerv2
gnerv2
gnerv2
gnerv2
gnerv3
gnerv3
gnerv3
gnerv3
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.873 seconds.
Building the CSC structure...
        It takes 1.893 seconds.
Building the CSC structure...
        It takes 1.918 seconds.
Building the CSC structure...
        It takes 1.931 seconds.
Building the CSC structure...
        It takes 2.042 seconds.
Building the CSC structure...
        It takes 2.055 seconds.
Building the CSC structure...
        It takes 2.070 seconds.
Building the CSC structure...
        It takes 2.592 seconds.
Building the CSC structure...
        It takes 1.791 seconds.
        It takes 1.820 seconds.
        It takes 1.844 seconds.
        It takes 1.887 seconds.
        It takes 1.848 seconds.
        It takes 1.888 seconds.
        It takes 1.888 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.343 seconds.
        It takes 0.286 seconds.
Building the Label Vector...
        It takes 0.299 seconds.
Building the Label Vector...
        It takes 0.044 seconds.
        It takes 0.043 seconds.
        It takes 0.302 seconds.
Building the Label Vector...
        It takes 0.289 seconds.
Building the Label Vector...
        It takes 0.043 seconds.
        It takes 0.033 seconds.
Building the Feature Vector...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
Building the Feature Vector...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
Building the Feature Vector...
        It takes 0.274 seconds.
Building the Label Vector...
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
        It takes 0.038 seconds.
        It takes 0.305 seconds.
Building the Label Vector...
        It takes 0.291 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
        It takes 0.031 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/32_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
232965, 114848857, 114848857
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
        It takes 0.261 seconds.
Building the Label Vector...
        It takes 0.031 seconds.
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
Chunks (number of global chunks: 32): 0-[0, 8381) 1-[8381, 15124) 2-[15124, 22398) 3-[22398, 30320) 4-[30320, 35655) 5-[35655, 45721) 6-[45721, 55131) 7-[55131, 63248) 8-[63248, 69343) ... 31-[226638, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
GPU 0, layer [0, 8)
GPU 1, layer [8, 16)
GPU 2, layer [16, 24)
GPU 3, layer [24, 32)
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.535 Gbps (per GPU), 452.282 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.282 Gbps (per GPU), 450.257 Gbps (aggregated)
The layer-level communication performance: 56.289 Gbps (per GPU), 450.311 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 56.078 Gbps (per GPU), 448.627 Gbps (aggregated)
The layer-level communication performance: 56.043 Gbps (per GPU), 448.345 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.848 Gbps (per GPU), 446.787 Gbps (aggregated)
The layer-level communication performance: 55.816 Gbps (per GPU), 446.529 Gbps (aggregated)
The layer-level communication performance: 55.775 Gbps (per GPU), 446.202 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.279 Gbps (per GPU), 1250.235 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.279 Gbps (per GPU), 1250.235 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.294 Gbps (per GPU), 1250.351 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.282 Gbps (per GPU), 1250.258 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.186 Gbps (per GPU), 1249.490 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.285 Gbps (per GPU), 1250.282 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.297 Gbps (per GPU), 1250.375 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.189 Gbps (per GPU), 1249.516 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.825 Gbps (per GPU), 806.603 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.827 Gbps (per GPU), 806.616 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.825 Gbps (per GPU), 806.603 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.828 Gbps (per GPU), 806.622 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.826 Gbps (per GPU), 806.610 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.828 Gbps (per GPU), 806.623 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.825 Gbps (per GPU), 806.603 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.828 Gbps (per GPU), 806.623 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 34.990 Gbps (per GPU), 279.922 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.990 Gbps (per GPU), 279.921 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.990 Gbps (per GPU), 279.920 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.990 Gbps (per GPU), 279.920 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.991 Gbps (per GPU), 279.929 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.991 Gbps (per GPU), 279.925 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.990 Gbps (per GPU), 279.923 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 34.991 Gbps (per GPU), 279.926 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  3.24ms  2.61ms  2.44ms  1.33  8.38K  3.53M
 chk_1  3.44ms  2.80ms  2.60ms  1.32  6.74K  3.60M
 chk_2  3.32ms  2.69ms  2.50ms  1.33  7.27K  3.53M
 chk_3  3.35ms  2.74ms  2.51ms  1.34  7.92K  3.61M
 chk_4  3.06ms  2.63ms  2.49ms  1.23  5.33K  3.68M
 chk_5  3.45ms  2.69ms  2.37ms  1.46 10.07K  3.45M
 chk_6  3.61ms  2.85ms  2.55ms  1.41  9.41K  3.48M
 chk_7  3.29ms  2.66ms  2.43ms  1.36  8.12K  3.60M
 chk_8  3.28ms  2.97ms  2.59ms  1.26  6.09K  3.64M
 chk_9  3.50ms  2.59ms  2.26ms  1.55 11.10K  3.38M
chk_10  3.27ms  2.79ms  2.64ms  1.24  5.67K  3.63M
chk_11  3.32ms  2.85ms  2.45ms  1.36  8.16K  3.54M
chk_12  3.50ms  2.85ms  2.68ms  1.31  7.24K  3.55M
chk_13  3.13ms  2.67ms  2.53ms  1.24  5.41K  3.68M
chk_14  3.60ms  2.94ms  2.75ms  1.31  7.14K  3.53M
chk_15  3.56ms  3.02ms  2.53ms  1.41  9.25K  3.49M
chk_16  3.02ms  2.58ms  2.45ms  1.23  4.78K  3.77M
chk_17  3.33ms  2.73ms  2.56ms  1.30  6.85K  3.60M
chk_18  3.20ms  2.68ms  2.36ms  1.35  7.47K  3.57M
chk_19  3.03ms  2.58ms  2.47ms  1.23  4.88K  3.75M
chk_20  3.26ms  2.64ms  2.44ms  1.34  7.00K  3.63M
chk_21  3.07ms  2.61ms  2.46ms  1.25  5.41K  3.68M
chk_22  3.78ms  2.97ms  2.53ms  1.49 11.07K  3.39M
chk_23  3.39ms  2.70ms  2.53ms  1.34  7.23K  3.64M
chk_24  3.61ms  2.80ms  2.48ms  1.46 10.13K  3.43M
chk_25  3.13ms  2.72ms  2.39ms  1.31  6.40K  3.57M
chk_26  3.30ms  2.79ms  2.62ms  1.26  5.78K  3.55M
chk_27  3.43ms  2.70ms  2.42ms  1.42  9.34K  3.48M
chk_28  3.55ms  2.97ms  2.78ms  1.27  6.37K  3.57M
chk_29  3.25ms  2.91ms  2.61ms  1.24  5.16K  3.78M
chk_30  3.11ms  2.62ms  2.50ms  1.24  5.44K  3.67M
chk_31  3.38ms  2.80ms  2.63ms  1.28  6.33K  3.63M
   Avg  3.34  2.75  2.52
   Max  3.78  3.02  2.78
   Min  3.02  2.58  2.26
 Ratio  1.25  1.17  1.23
   Var  0.04  0.02  0.01
Profiling takes 3.209 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 371.114 ms
Partition 0 [0, 4) has cost: 371.114 ms
Partition 1 [4, 8) has cost: 352.509 ms
Partition 2 [8, 12) has cost: 352.509 ms
Partition 3 [12, 16) has cost: 352.509 ms
Partition 4 [16, 20) has cost: 352.509 ms
Partition 5 [20, 24) has cost: 352.509 ms
Partition 6 [24, 28) has cost: 352.509 ms
Partition 7 [28, 32) has cost: 344.935 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 180.583 ms
GPU 0, Compute+Comm Time: 135.292 ms, Bubble Time: 30.759 ms, Imbalance Overhead: 14.532 ms
GPU 1, Compute+Comm Time: 130.222 ms, Bubble Time: 30.255 ms, Imbalance Overhead: 20.106 ms
GPU 2, Compute+Comm Time: 130.222 ms, Bubble Time: 29.920 ms, Imbalance Overhead: 20.441 ms
GPU 3, Compute+Comm Time: 130.222 ms, Bubble Time: 29.459 ms, Imbalance Overhead: 20.902 ms
GPU 4, Compute+Comm Time: 130.222 ms, Bubble Time: 29.068 ms, Imbalance Overhead: 21.293 ms
GPU 5, Compute+Comm Time: 130.222 ms, Bubble Time: 28.748 ms, Imbalance Overhead: 21.613 ms
GPU 6, Compute+Comm Time: 130.222 ms, Bubble Time: 28.975 ms, Imbalance Overhead: 21.386 ms
GPU 7, Compute+Comm Time: 127.373 ms, Bubble Time: 29.452 ms, Imbalance Overhead: 23.758 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 324.928 ms
GPU 0, Compute+Comm Time: 243.934 ms, Bubble Time: 57.743 ms, Imbalance Overhead: 23.251 ms
GPU 1, Compute+Comm Time: 248.659 ms, Bubble Time: 56.749 ms, Imbalance Overhead: 19.520 ms
GPU 2, Compute+Comm Time: 248.659 ms, Bubble Time: 56.262 ms, Imbalance Overhead: 20.007 ms
GPU 3, Compute+Comm Time: 248.659 ms, Bubble Time: 56.219 ms, Imbalance Overhead: 20.050 ms
GPU 4, Compute+Comm Time: 248.659 ms, Bubble Time: 56.154 ms, Imbalance Overhead: 20.115 ms
GPU 5, Compute+Comm Time: 248.659 ms, Bubble Time: 56.376 ms, Imbalance Overhead: 19.893 ms
GPU 6, Compute+Comm Time: 248.659 ms, Bubble Time: 56.334 ms, Imbalance Overhead: 19.935 ms
GPU 7, Compute+Comm Time: 262.194 ms, Bubble Time: 56.713 ms, Imbalance Overhead: 6.021 ms
The estimated cost of the whole pipeline: 530.787 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 723.623 ms
Partition 0 [0, 8) has cost: 723.623 ms
Partition 1 [8, 16) has cost: 705.018 ms
Partition 2 [16, 24) has cost: 705.018 ms
Partition 3 [24, 32) has cost: 697.444 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 186.605 ms
GPU 0, Compute+Comm Time: 147.531 ms, Bubble Time: 31.527 ms, Imbalance Overhead: 7.547 ms
GPU 1, Compute+Comm Time: 145.053 ms, Bubble Time: 29.405 ms, Imbalance Overhead: 12.148 ms
GPU 2, Compute+Comm Time: 145.053 ms, Bubble Time: 27.682 ms, Imbalance Overhead: 13.870 ms
GPU 3, Compute+Comm Time: 143.367 ms, Bubble Time: 26.031 ms, Imbalance Overhead: 17.208 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 325.296 ms
GPU 0, Compute+Comm Time: 261.205 ms, Bubble Time: 49.416 ms, Imbalance Overhead: 14.675 ms
GPU 1, Compute+Comm Time: 263.655 ms, Bubble Time: 49.503 ms, Imbalance Overhead: 12.138 ms
GPU 2, Compute+Comm Time: 263.655 ms, Bubble Time: 49.589 ms, Imbalance Overhead: 12.052 ms
GPU 3, Compute+Comm Time: 271.097 ms, Bubble Time: 50.743 ms, Imbalance Overhead: 3.456 ms
    The estimated cost with 2 DP ways is 537.496 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1428.641 ms
Partition 0 [0, 16) has cost: 1428.641 ms
Partition 1 [16, 32) has cost: 1402.462 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 232.344 ms
GPU 0, Compute+Comm Time: 200.766 ms, Bubble Time: 26.448 ms, Imbalance Overhead: 5.129 ms
GPU 1, Compute+Comm Time: 198.752 ms, Bubble Time: 27.439 ms, Imbalance Overhead: 6.152 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 364.498 ms
GPU 0, Compute+Comm Time: 314.600 ms, Bubble Time: 40.167 ms, Imbalance Overhead: 9.731 ms
GPU 1, Compute+Comm Time: 319.679 ms, Bubble Time: 39.084 ms, Imbalance Overhead: 5.735 ms
    The estimated cost with 4 DP ways is 626.684 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2831.103 ms
Partition 0 [0, 32) has cost: 2831.103 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 565.356 ms
GPU 0, Compute+Comm Time: 565.356 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 678.363 ms
GPU 0, Compute+Comm Time: 678.363 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1305.905 ms

*** Node 4, starting model training...
Num Stages: 4 / 4
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [145, 217)
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 73)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 5, starting model training...
Num Stages: 4 / 4
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [145, 217)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 73)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 6, starting model training...
Num Stages: 4 / 4
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [217, 287)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [73, 145)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 7, starting model training...
Num Stages: 4 / 4
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [217, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [73, 145)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 123307, Num Local Vertices: 109658
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 123307
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[145, 217)...
+++++++++ Node 0 initializing the weights for op[0, 73)...
+++++++++ Node 6 initializing the weights for op[217, 287)...
+++++++++ Node 4 initializing the weights for op[145, 217)...
+++++++++ Node 1 initializing the weights for op[0, 73)...
+++++++++ Node 2 initializing the weights for op[73, 145)...
+++++++++ Node 3 initializing the weights for op[73, 145)...
+++++++++ Node 7 initializing the weights for op[217, 287)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
Node 6, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 3.2591
	Epoch 50:	Loss 3.1123
	Epoch 75:	Loss 3.0418
	Epoch 100:	Loss 2.9828
	Epoch 125:	Loss 2.9277
	Epoch 150:	Loss 2.8209
	Epoch 175:	Loss 2.7637
	Epoch 200:	Loss 2.8470
	Epoch 225:	Loss 2.8163
	Epoch 250:	Loss 2.7496
	Epoch 275:	Loss 2.6813
	Epoch 300:	Loss 2.6077
	Epoch 325:	Loss 2.5450
	Epoch 350:	Loss 2.4883
	Epoch 375:	Loss 2.4434
	Epoch 400:	Loss 2.3908
	Epoch 425:	Loss 2.4720
	Epoch 450:	Loss 2.4416
	Epoch 475:	Loss 2.3271
	Epoch 500:	Loss 2.2052
	Epoch 525:	Loss 2.1197
	Epoch 550:	Loss 2.0537
	Epoch 575:	Loss 1.9496
	Epoch 600:	Loss 1.8541
	Epoch 625:	Loss 1.8032
	Epoch 650:	Loss 1.7513
	Epoch 675:	Loss 1.6986
	Epoch 700:	Loss 1.6478
	Epoch 725:	Loss 1.5933
	Epoch 750:	Loss 1.5413
	Epoch 775:	Loss 1.5530
	Epoch 800:	Loss 1.5120
	Epoch 825:	Loss 1.4839
	Epoch 850:	Loss 1.4501
	Epoch 875:	Loss 1.4127
	Epoch 900:	Loss 1.3778
	Epoch 925:	Loss 1.3539
	Epoch 950:	Loss 1.3351
	Epoch 975:	Loss 1.3123
	Epoch 1000:	Loss 1.3171
	Epoch 1025:	Loss 1.3032
	Epoch 1050:	Loss 1.2974
	Epoch 1075:	Loss 1.2830
	Epoch 1100:	Loss 1.2489
	Epoch 1125:	Loss 1.2404
	Epoch 1150:	Loss 1.2495
	Epoch 1175:	Loss 1.2455
	Epoch 1200:	Loss 1.2315
	Epoch 1225:	Loss 1.2385
	Epoch 1250:	Loss 1.2247
	Epoch 1275:	Loss 1.2072
	Epoch 1300:	Loss 1.1910
	Epoch 1325:	Loss 1.1961
	Epoch 1350:	Loss 1.1932
	Epoch 1375:	Loss 1.1958
	Epoch 1400:	Loss 1.1635
	Epoch 1425:	Loss 1.1568
	Epoch 1450:	Loss 1.1499
	Epoch 1475:	Loss 1.1338
	Epoch 1500:	Loss 1.1345
	Epoch 1525:	Loss 1.1154
	Epoch 1550:	Loss 1.1134
	Epoch 1575:	Loss 1.0968
	Epoch 1600:	Loss 1.0897
	Epoch 1625:	Loss 1.0769
	Epoch 1650:	Loss 1.0707
	Epoch 1675:	Loss 1.0636
	Epoch 1700:	Loss 1.0480
	Epoch 1725:	Loss 1.0636
	Epoch 1750:	Loss 1.0766
	Epoch 1775:	Loss 1.0842
	Epoch 1800:	Loss 1.0712
	Epoch 1825:	Loss 1.0366
	Epoch 1850:	Loss 1.0429
	Epoch 1875:	Loss 1.0209
	Epoch 1900:	Loss 1.0119
	Epoch 1925:	Loss 1.0119
	Epoch 1950:	Loss 0.9958
	Epoch 1975:	Loss 0.9767
	Epoch 2000:	Loss 0.9711
	Epoch 2025:	Loss 0.9751
	Epoch 2050:	Loss 0.9657
	Epoch 2075:	Loss 0.9707
	Epoch 2100:	Loss 0.9605
	Epoch 2125:	Loss 0.9609
	Epoch 2150:	Loss 0.9508
	Epoch 2175:	Loss 0.9555
	Epoch 2200:	Loss 0.9439
	Epoch 2225:	Loss 0.9592
	Epoch 2250:	Loss 0.9394
	Epoch 2275:	Loss 0.9273
	Epoch 2300:	Loss 0.9062
	Epoch 2325:	Loss 0.9317
	Epoch 2350:	Loss 0.8819
	Epoch 2375:	Loss 0.8803
	Epoch 2400:	Loss 0.8762
	Epoch 2425:	Loss 0.8778
	Epoch 2450:	Loss 0.8585
	Epoch 2475:	Loss 0.8469
	Epoch 2500:	Loss 0.8356
	Epoch 2525:	Loss 0.8345
	Epoch 2550:	Loss 0.8130
	Epoch 2575:	Loss 0.8084
	Epoch 2600:	Loss 0.7914
	Epoch 2625:	Loss 0.7906
	Epoch 2650:	Loss 0.7875
	Epoch 2675:	Loss 0.7739
	Epoch 2700:	Loss 0.7873
	Epoch 2725:	Loss 0.7710
	Epoch 2750:	Loss 0.7695
	Epoch 2775:	Loss 0.7551
	Epoch 2800:	Loss 0.7566
	Epoch 2825:	Loss 0.7465
	Epoch 2850:	Loss 0.7537
	Epoch 2875:	Loss 0.7410
	Epoch 2900:	Loss 0.7512
	Epoch 2925:	Loss 0.7624
	Epoch 2950:	Loss 0.7650
	Epoch 2975:	Loss 0.7594
	Epoch 3000:	Loss 0.7600
	Epoch 3025:	Loss 0.8422
	Epoch 3050:	Loss 0.7557
	Epoch 3075:	Loss 0.7433
	Epoch 3100:	Loss 0.7210
	Epoch 3125:	Loss 0.7168
	Epoch 3150:	Loss 0.7137
	Epoch 3175:	Loss 0.7074
	Epoch 3200:	Loss 0.7075
	Epoch 3225:	Loss 0.7210
	Epoch 3250:	Loss 0.7509
	Epoch 3275:	Loss 0.7019
	Epoch 3300:	Loss 0.6881
	Epoch 3325:	Loss 0.6901
	Epoch 3350:	Loss 0.6872
	Epoch 3375:	Loss 0.6767
	Epoch 3400:	Loss 0.6779
	Epoch 3425:	Loss 0.7062
	Epoch 3450:	Loss 0.6670
	Epoch 3475:	Loss 0.6816
	Epoch 3500:	Loss 0.6801
	Epoch 3525:	Loss 0.7231
	Epoch 3550:	Loss 0.6771
	Epoch 3575:	Loss 0.6566
	Epoch 3600:	Loss 0.6551
	Epoch 3625:	Loss 0.6643
	Epoch 3650:	Loss 0.6657
	Epoch 3675:	Loss 0.6507
	Epoch 3700:	Loss 0.6504
	Epoch 3725:	Loss 0.6638
	Epoch 3750:	Loss 0.6343
	Epoch 3775:	Loss 0.6389
	Epoch 3800:	Loss 0.6334
	Epoch 3825:	Loss 0.6799
	Epoch 3850:	Loss 0.6435
	Epoch 3875:	Loss 0.6394
	Epoch 3900:	Loss 0.6316
	Epoch 3925:	Loss 0.6371
	Epoch 3950:	Loss 0.6379
	Epoch 3975:	Loss 0.6277
	Epoch 4000:	Loss 0.6220
	Epoch 4025:	Loss 0.6789
	Epoch 4050:	Loss 0.6563
	Epoch 4075:	Loss 0.6408
	Epoch 4100:	Loss 0.6506
	Epoch 4125:	Loss 0.6585
	Epoch 4150:	Loss 0.6500
	Epoch 4175:	Loss 0.6332
	Epoch 4200:	Loss 0.6255
	Epoch 4225:	Loss 0.6078
	Epoch 4250:	Loss 0.6087
	Epoch 4275:	Loss 0.6049
	Epoch 4300:	Loss 0.6005
	Epoch 4325:	Loss 0.5955
	Epoch 4350:	Loss 0.5946
	Epoch 4375:	Loss 0.5836
	Epoch 4400:	Loss 0.5932
	Epoch 4425:	Loss 0.6175
	Epoch 4450:	Loss 0.5841
	Epoch 4475:	Loss 0.6043
	Epoch 4500:	Loss 0.5983
	Epoch 4525:	Loss 0.6366
	Epoch 4550:	Loss 0.5945
	Epoch 4575:	Loss 0.5899
	Epoch 4600:	Loss 0.5878
	Epoch 4625:	Loss 0.6047
	Epoch 4650:	Loss 0.5840
	Epoch 4675:	Loss 0.5794
	Epoch 4700:	Loss 0.5768
	Epoch 4725:	Loss 0.6003
	Epoch 4750:	Loss 0.5901
	Epoch 4775:	Loss 0.6151
	Epoch 4800:	Loss 0.6060
	Epoch 4825:	Loss 0.6691
	Epoch 4850:	Loss 0.6217
	Epoch 4875:	Loss 0.6045
	Epoch 4900:	Loss 0.5939
	Epoch 4925:	Loss 0.5946
	Epoch 4950:	Loss 0.5749
	Epoch 4975:	Loss 0.5745
Node 6, Pre/Post-Pipelining: 2.344 / 11.729 ms, Bubble: 84.378 ms, Compute: 458.537 ms, Comm: 17.164 ms, Imbalance: 11.383 ms
Node 2, Pre/Post-Pipelining: 2.267 / 3.546 ms, Bubble: 90.222 ms, Compute: 448.411 ms, Comm: 19.054 ms, Imbalance: 21.805 ms
Node 7, Pre/Post-Pipelining: 2.343 / 11.753 ms, Bubble: 84.234 ms, Compute: 461.702 ms, Comm: 13.913 ms, Imbalance: 11.401 ms
Node 3, Pre/Post-Pipelining: 2.268 / 3.518 ms, Bubble: 90.285 ms, Compute: 449.754 ms, Comm: 17.321 ms, Imbalance: 22.046 ms
Node 5, Pre/Post-Pipelining: 2.345 / 3.737 ms, Bubble: 91.353 ms, Compute: 446.536 ms, Comm: 17.848 ms, Imbalance: 24.212 ms
Node 4, Pre/Post-Pipelining: 2.344 / 3.800 ms, Bubble: 91.223 ms, Compute: 444.873 ms, Comm: 20.417 ms, Imbalance: 23.360 ms
	Epoch 5000:	Loss 0.5634
Node 0, Pre/Post-Pipelining: 2.208 / 4.218 ms, Bubble: 89.333 ms, Compute: 461.528 ms, Comm: 15.697 ms, Imbalance: 11.759 ms
Node 1, Pre/Post-Pipelining: 2.208 / 4.230 ms, Bubble: 89.339 ms, Compute: 463.523 ms, Comm: 13.868 ms, Imbalance: 11.578 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 2.208 ms
Cluster-Wide Average, Post-Pipelining Overhead: 4.218 ms
Cluster-Wide Average, Bubble: 89.333 ms
Cluster-Wide Average, Compute: 461.528 ms
Cluster-Wide Average, Communication: 15.697 ms
Cluster-Wide Average, Imbalance: 11.759 ms
Node 7, GPU memory consumption: 6.411 GB
Node 0, GPU memory consumption: 8.215 GB
Node 4, GPU memory consumption: 6.503 GB
Node 1, GPU memory consumption: 7.327 GB
Node 5, GPU memory consumption: 6.526 GB
Node 3, GPU memory consumption: 6.503 GB
Node 6, GPU memory consumption: 6.436 GB
Node 2, GPU memory consumption: 6.526 GB
Node 4, Graph-Level Communication Throughput: 125.955 Gbps, Time: 48.007 ms
Node 0, Graph-Level Communication Throughput: 107.055 Gbps, Time: 56.482 ms
Node 5, Graph-Level Communication Throughput: 85.300 Gbps, Time: 63.656 ms
Node 1, Graph-Level Communication Throughput: 100.643 Gbps, Time: 53.951 ms
Node 6, Graph-Level Communication Throughput: 115.188 Gbps, Time: 52.495 ms
Node 2, Graph-Level Communication Throughput: 117.109 Gbps, Time: 51.633 ms
Node 7, Graph-Level Communication Throughput: 98.672 Gbps, Time: 55.029 ms
Node 3, Graph-Level Communication Throughput: 98.559 Gbps, Time: 55.092 ms
------------------------node id 4,  per-epoch time: 0.585082 s---------------
------------------------node id 0,  per-epoch time: 0.585082 s---------------
------------------------node id 5,  per-epoch time: 0.585082 s---------------
------------------------node id 1,  per-epoch time: 0.585082 s---------------
------------------------node id 6,  per-epoch time: 0.585082 s---------------
------------------------node id 2,  per-epoch time: 0.585081 s---------------
------------------------node id 7,  per-epoch time: 0.585082 s---------------
------------------------node id 3,  per-epoch time: 0.585081 s---------------
************ Profiling Results ************
	Bubble: 131.987237 (ms) (22.56 percentage)
	Compute: 381.253207 (ms) (65.15 percentage)
	GraphCommComputeOverhead: 9.892882 (ms) (1.69 percentage)
	GraphCommNetwork: 54.548109 (ms) (9.32 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 7.479742 (ms) (1.28 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.521 GB
	Graph-level communication (cluster-wide, per-epoch): 5.344 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.005 GB
	Total communication (cluster-wide, per-epoch): 5.870 GB
	Aggregated layer-level communication throughput: 264.509 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 4] Success
[MPI Rank 0] Success
[MPI Rank 5] Success
[MPI Rank 1] Success
[MPI Rank 6] Success
[MPI Rank 2] Success
[MPI Rank 7] Success
[MPI Rank 3] Success
