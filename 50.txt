gnerv1
Mon Aug  7 18:16:23 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   29C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   29C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   28C    P8    20W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   27C    P8    23W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 22%] Built target context
[ 36%] Built target core
Scanning dependencies of target cudahelp
[ 38%] Building CXX object CMakeFiles/cudahelp.dir/core/src/cuda/cuda_hybrid_parallel.cc.o
[ 41%] Linking CXX static library libcudahelp.a
[ 77%] Built target cudahelp
[ 83%] Linking CXX executable graphsage
[ 83%] Linking CXX executable gcn
[ 83%] Linking CXX executable gcnii
[ 83%] Linking CXX executable estimate_comm_volume
[ 91%] Built target estimate_comm_volume
[ 94%] Built target OSDI2023_MULTI_NODES_graphsage
[100%] Built target OSDI2023_MULTI_NODES_gcnii
[100%] Built target OSDI2023_MULTI_NODES_gcn
Running experiments...
gnerv2
gnerv2
gnerv2
gnerv2
gnerv3
gnerv3
gnerv3
gnerv3
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 7 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.022 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.030 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.023 seconds.
        It takes 0.021 seconds.
        It takes 0.021 seconds.
        It takes 0.020 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.022 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.099 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.096 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.109 seconds.
Building the Label Vector...
        It takes 0.106 seconds.
Building the Label Vector...
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.107 seconds.
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 200
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Building the Label Vector...Number of GPUs: 8

        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.491 Gbps (per GPU), 483.926 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.218 Gbps (per GPU), 481.743 Gbps (aggregated)
The layer-level communication performance: 60.208 Gbps (per GPU), 481.666 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.961 Gbps (per GPU), 479.689 Gbps (aggregated)
The layer-level communication performance: 59.927 Gbps (per GPU), 479.418 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.719 Gbps (per GPU), 477.750 Gbps (aggregated)
The layer-level communication performance: 59.642 Gbps (per GPU), 477.133 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.671 Gbps (per GPU), 477.368 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.615 Gbps (per GPU), 1244.924 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.618 Gbps (per GPU), 1244.947 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.711 Gbps (per GPU), 1245.686 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.303 Gbps (per GPU), 1266.420 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.644 Gbps (per GPU), 1245.155 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.610 Gbps (per GPU), 1244.878 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.662 Gbps (per GPU), 1245.293 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.285 Gbps (per GPU), 1266.280 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 98.733 Gbps (per GPU), 789.863 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.731 Gbps (per GPU), 789.844 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.736 Gbps (per GPU), 789.888 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.734 Gbps (per GPU), 789.875 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.737 Gbps (per GPU), 789.894 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.734 Gbps (per GPU), 789.869 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.737 Gbps (per GPU), 789.893 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.734 Gbps (per GPU), 789.869 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.839 Gbps (per GPU), 262.709 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.838 Gbps (per GPU), 262.700 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.838 Gbps (per GPU), 262.700 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.839 Gbps (per GPU), 262.711 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.837 Gbps (per GPU), 262.699 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.832 Gbps (per GPU), 262.659 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.834 Gbps (per GPU), 262.672 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.835 Gbps (per GPU), 262.678 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.89ms  0.72ms  0.34ms  2.64  2.81K  0.03M
 chk_1  0.91ms  0.72ms  0.34ms  2.67  2.82K  0.03M
 chk_2  0.89ms  0.72ms  0.35ms  2.56  2.80K  0.03M
 chk_3  0.88ms  0.71ms  0.34ms  2.62  2.80K  0.03M
 chk_4  0.91ms  0.72ms  0.34ms  2.67  2.82K  0.03M
 chk_5  0.91ms  0.74ms  0.37ms  2.49  2.75K  0.03M
 chk_6  0.87ms  0.70ms  0.33ms  2.66  2.71K  0.03M
 chk_7  0.89ms  0.72ms  0.34ms  2.59  2.76K  0.03M
 chk_8  0.89ms  0.72ms  0.34ms  2.62  2.79K  0.03M
 chk_9  0.89ms  0.72ms  0.33ms  2.65  2.81K  0.03M
chk_10  0.88ms  0.71ms  0.32ms  2.71  2.81K  0.03M
chk_11  0.91ms  0.73ms  0.36ms  2.53  2.74K  0.03M
chk_12  0.90ms  0.73ms  0.36ms  2.54  2.76K  0.03M
chk_13  1.02ms  0.72ms  0.35ms  2.91  2.75K  0.03M
chk_14  0.89ms  0.71ms  0.33ms  2.68  2.81K  0.03M
chk_15  0.89ms  0.72ms  0.34ms  2.63  2.77K  0.03M
chk_16  0.89ms  0.72ms  0.34ms  2.60  2.78K  0.03M
chk_17  0.89ms  0.72ms  0.34ms  2.63  2.79K  0.03M
chk_18  0.91ms  0.72ms  0.34ms  2.68  2.82K  0.03M
chk_19  0.87ms  0.70ms  0.32ms  2.72  2.81K  0.03M
chk_20  0.90ms  0.73ms  0.37ms  2.43  2.77K  0.03M
chk_21  0.91ms  0.72ms  0.34ms  2.70  2.84K  0.02M
chk_22  0.89ms  0.71ms  0.34ms  2.62  2.78K  0.03M
chk_23  0.90ms  0.72ms  0.34ms  2.64  2.80K  0.03M
chk_24  0.90ms  0.72ms  0.34ms  2.61  2.80K  0.03M
chk_25  0.89ms  0.71ms  0.33ms  2.68  2.81K  0.03M
chk_26  0.88ms  0.71ms  0.33ms  2.68  2.81K  0.03M
chk_27  0.90ms  0.72ms  0.34ms  2.63  2.79K  0.03M
chk_28  0.90ms  0.74ms  0.35ms  2.55  2.77K  0.03M
chk_29  0.90ms  0.72ms  0.35ms  2.60  2.77K  0.03M
chk_30  0.91ms  0.72ms  0.34ms  2.66  2.80K  0.03M
chk_31  0.90ms  0.73ms  0.35ms  2.57  2.78K  0.03M
   Avg  0.90  0.72  0.34
   Max  1.02  0.74  0.37
   Min  0.87  0.70  0.32
 Ratio  1.17  1.06  1.16
   Var  0.00  0.00  0.00
Profiling takes 0.844 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 97.767 ms
Partition 0 [0, 4) has cost: 97.767 ms
Partition 1 [4, 8) has cost: 92.004 ms
Partition 2 [8, 12) has cost: 92.004 ms
Partition 3 [12, 16) has cost: 92.004 ms
Partition 4 [16, 20) has cost: 92.004 ms
Partition 5 [20, 24) has cost: 92.004 ms
Partition 6 [24, 28) has cost: 92.004 ms
Partition 7 [28, 32) has cost: 79.946 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 48.120 ms
GPU 0, Compute+Comm Time: 40.043 ms, Bubble Time: 8.077 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 36.992 ms, Bubble Time: 8.154 ms, Imbalance Overhead: 2.973 ms
GPU 2, Compute+Comm Time: 36.992 ms, Bubble Time: 8.248 ms, Imbalance Overhead: 2.880 ms
GPU 3, Compute+Comm Time: 36.992 ms, Bubble Time: 8.332 ms, Imbalance Overhead: 2.796 ms
GPU 4, Compute+Comm Time: 36.992 ms, Bubble Time: 8.403 ms, Imbalance Overhead: 2.725 ms
GPU 5, Compute+Comm Time: 36.992 ms, Bubble Time: 8.494 ms, Imbalance Overhead: 2.633 ms
GPU 6, Compute+Comm Time: 36.992 ms, Bubble Time: 8.601 ms, Imbalance Overhead: 2.526 ms
GPU 7, Compute+Comm Time: 33.580 ms, Bubble Time: 8.759 ms, Imbalance Overhead: 5.781 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 92.784 ms
GPU 0, Compute+Comm Time: 65.252 ms, Bubble Time: 16.773 ms, Imbalance Overhead: 10.760 ms
GPU 1, Compute+Comm Time: 73.897 ms, Bubble Time: 16.477 ms, Imbalance Overhead: 2.410 ms
GPU 2, Compute+Comm Time: 73.897 ms, Bubble Time: 16.369 ms, Imbalance Overhead: 2.518 ms
GPU 3, Compute+Comm Time: 73.897 ms, Bubble Time: 16.294 ms, Imbalance Overhead: 2.593 ms
GPU 4, Compute+Comm Time: 73.897 ms, Bubble Time: 16.247 ms, Imbalance Overhead: 2.640 ms
GPU 5, Compute+Comm Time: 73.897 ms, Bubble Time: 16.201 ms, Imbalance Overhead: 2.686 ms
GPU 6, Compute+Comm Time: 73.897 ms, Bubble Time: 16.160 ms, Imbalance Overhead: 2.727 ms
GPU 7, Compute+Comm Time: 76.610 ms, Bubble Time: 16.141 ms, Imbalance Overhead: 0.033 ms
The estimated cost of the whole pipeline: 147.950 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 189.772 ms
Partition 0 [0, 8) has cost: 189.772 ms
Partition 1 [8, 16) has cost: 184.009 ms
Partition 2 [16, 24) has cost: 184.009 ms
Partition 3 [24, 32) has cost: 171.950 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 54.355 ms
GPU 0, Compute+Comm Time: 46.075 ms, Bubble Time: 8.278 ms, Imbalance Overhead: 0.002 ms
GPU 1, Compute+Comm Time: 44.521 ms, Bubble Time: 8.349 ms, Imbalance Overhead: 1.484 ms
GPU 2, Compute+Comm Time: 44.521 ms, Bubble Time: 8.443 ms, Imbalance Overhead: 1.391 ms
GPU 3, Compute+Comm Time: 42.828 ms, Bubble Time: 8.646 ms, Imbalance Overhead: 2.882 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 98.192 ms
GPU 0, Compute+Comm Time: 77.279 ms, Bubble Time: 15.555 ms, Imbalance Overhead: 5.358 ms
GPU 1, Compute+Comm Time: 81.586 ms, Bubble Time: 15.194 ms, Imbalance Overhead: 1.412 ms
GPU 2, Compute+Comm Time: 81.586 ms, Bubble Time: 15.133 ms, Imbalance Overhead: 1.473 ms
GPU 3, Compute+Comm Time: 82.984 ms, Bubble Time: 15.086 ms, Imbalance Overhead: 0.121 ms
    The estimated cost with 2 DP ways is 160.174 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 373.780 ms
Partition 0 [0, 16) has cost: 373.780 ms
Partition 1 [16, 32) has cost: 355.959 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 82.143 ms
GPU 0, Compute+Comm Time: 73.218 ms, Bubble Time: 8.926 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 71.578 ms, Bubble Time: 9.102 ms, Imbalance Overhead: 1.463 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 123.841 ms
GPU 0, Compute+Comm Time: 107.535 ms, Bubble Time: 13.711 ms, Imbalance Overhead: 2.594 ms
GPU 1, Compute+Comm Time: 110.430 ms, Bubble Time: 13.411 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 216.283 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 729.739 ms
Partition 0 [0, 32) has cost: 729.739 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 274.644 ms
GPU 0, Compute+Comm Time: 274.644 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 311.547 ms
GPU 0, Compute+Comm Time: 311.547 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 615.501 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 37)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 4, starting model training...
Num Stages: 8 / 8
Node 4, Pipeline Input Tensor: OPERATOR_ADD
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_ADD
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [37, 73)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [181, 217)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [73, 109)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: OPERATOR_ADD
*** Node 6 owns the model-level partition [217, 253)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [109, 145)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [253, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 89250
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [145, 181)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[181, 217)...
+++++++++ Node 1 initializing the weights for op[37, 73)...
+++++++++ Node 6 initializing the weights for op[217, 253)...
+++++++++ Node 2 initializing the weights for op[73, 109)...
+++++++++ Node 4 initializing the weights for op[145, 181)...
+++++++++ Node 3 initializing the weights for op[109, 145)...
+++++++++ Node 7 initializing the weights for op[253, 287)...
+++++++++ Node 0 initializing the weights for op[0, 37)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 1.6158	TrainAcc 0.4244	ValidAcc 0.4278	TestAcc 0.4260	BestValid 0.4278
	Epoch 50:	Loss 1.5779	TrainAcc 0.4586	ValidAcc 0.4598	TestAcc 0.4594	BestValid 0.4598
	Epoch 75:	Loss 1.5316	TrainAcc 0.4898	ValidAcc 0.4917	TestAcc 0.4922	BestValid 0.4917
	Epoch 100:	Loss 1.5049	TrainAcc 0.5005	ValidAcc 0.5001	TestAcc 0.5020	BestValid 0.5001
	Epoch 125:	Loss 1.4739	TrainAcc 0.5052	ValidAcc 0.5032	TestAcc 0.5041	BestValid 0.5032
	Epoch 150:	Loss 1.4603	TrainAcc 0.5089	ValidAcc 0.5078	TestAcc 0.5069	BestValid 0.5078
	Epoch 175:	Loss 1.4385	TrainAcc 0.5122	ValidAcc 0.5088	TestAcc 0.5094	BestValid 0.5088
	Epoch 200:	Loss 1.4308	TrainAcc 0.5118	ValidAcc 0.5079	TestAcc 0.5093	BestValid 0.5088
	Epoch 225:	Loss 1.4169	TrainAcc 0.5131	ValidAcc 0.5107	TestAcc 0.5098	BestValid 0.5107
	Epoch 250:	Loss 1.4125	TrainAcc 0.5165	ValidAcc 0.5118	TestAcc 0.5122	BestValid 0.5118
	Epoch 275:	Loss 1.4014	TrainAcc 0.5189	ValidAcc 0.5134	TestAcc 0.5129	BestValid 0.5134
	Epoch 300:	Loss 1.3995	TrainAcc 0.5189	ValidAcc 0.5138	TestAcc 0.5133	BestValid 0.5138
	Epoch 325:	Loss 1.3900	TrainAcc 0.5225	ValidAcc 0.5164	TestAcc 0.5160	BestValid 0.5164
	Epoch 350:	Loss 1.3872	TrainAcc 0.5206	ValidAcc 0.5132	TestAcc 0.5142	BestValid 0.5164
	Epoch 375:	Loss 1.3802	TrainAcc 0.5256	ValidAcc 0.5181	TestAcc 0.5184	BestValid 0.5181
	Epoch 400:	Loss 1.3773	TrainAcc 0.5226	ValidAcc 0.5151	TestAcc 0.5151	BestValid 0.5181
	Epoch 425:	Loss 1.3748	TrainAcc 0.5156	ValidAcc 0.5077	TestAcc 0.5088	BestValid 0.5181
	Epoch 450:	Loss 1.3729	TrainAcc 0.5206	ValidAcc 0.5133	TestAcc 0.5129	BestValid 0.5181
	Epoch 475:	Loss 1.3682	TrainAcc 0.5249	ValidAcc 0.5170	TestAcc 0.5172	BestValid 0.5181
	Epoch 500:	Loss 1.3650	TrainAcc 0.5115	ValidAcc 0.5031	TestAcc 0.5043	BestValid 0.5181
	Epoch 525:	Loss 1.3645	TrainAcc 0.5243	ValidAcc 0.5153	TestAcc 0.5159	BestValid 0.5181
	Epoch 550:	Loss 1.3633	TrainAcc 0.5308	ValidAcc 0.5223	TestAcc 0.5221	BestValid 0.5223
	Epoch 575:	Loss 1.3604	TrainAcc 0.4999	ValidAcc 0.4919	TestAcc 0.4934	BestValid 0.5223
	Epoch 600:	Loss 1.3594	TrainAcc 0.5255	ValidAcc 0.5169	TestAcc 0.5167	BestValid 0.5223
	Epoch 625:	Loss 1.3546	TrainAcc 0.5101	ValidAcc 0.5005	TestAcc 0.5023	BestValid 0.5223
	Epoch 650:	Loss 1.3547	TrainAcc 0.5243	ValidAcc 0.5138	TestAcc 0.5143	BestValid 0.5223
	Epoch 675:	Loss 1.3555	TrainAcc 0.5357	ValidAcc 0.5259	TestAcc 0.5266	BestValid 0.5259
	Epoch 700:	Loss 1.3510	TrainAcc 0.5038	ValidAcc 0.4948	TestAcc 0.4972	BestValid 0.5259
	Epoch 725:	Loss 1.3515	TrainAcc 0.5248	ValidAcc 0.5144	TestAcc 0.5165	BestValid 0.5259
	Epoch 750:	Loss 1.3499	TrainAcc 0.5286	ValidAcc 0.5164	TestAcc 0.5194	BestValid 0.5259
	Epoch 775:	Loss 1.3458	TrainAcc 0.5185	ValidAcc 0.5087	TestAcc 0.5102	BestValid 0.5259
	Epoch 800:	Loss 1.3463	TrainAcc 0.5041	ValidAcc 0.4948	TestAcc 0.4972	BestValid 0.5259
	Epoch 825:	Loss 1.3441	TrainAcc 0.5176	ValidAcc 0.5069	TestAcc 0.5078	BestValid 0.5259
	Epoch 850:	Loss 1.3427	TrainAcc 0.5143	ValidAcc 0.5035	TestAcc 0.5054	BestValid 0.5259
	Epoch 875:	Loss 1.3412	TrainAcc 0.4902	ValidAcc 0.4815	TestAcc 0.4835	BestValid 0.5259
	Epoch 900:	Loss 1.3396	TrainAcc 0.5336	ValidAcc 0.5207	TestAcc 0.5242	BestValid 0.5259
	Epoch 925:	Loss 1.3389	TrainAcc 0.5144	ValidAcc 0.5041	TestAcc 0.5035	BestValid 0.5259
	Epoch 950:	Loss 1.3357	TrainAcc 0.5210	ValidAcc 0.5090	TestAcc 0.5112	BestValid 0.5259
	Epoch 975:	Loss 1.3320	TrainAcc 0.5212	ValidAcc 0.5096	TestAcc 0.5117	BestValid 0.5259
	Epoch 1000:	Loss 1.3338	TrainAcc 0.5202	ValidAcc 0.5087	TestAcc 0.5102	BestValid 0.5259
	Epoch 1025:	Loss 1.3302	TrainAcc 0.5254	ValidAcc 0.5127	TestAcc 0.5136	BestValid 0.5259
	Epoch 1050:	Loss 1.3337	TrainAcc 0.5236	ValidAcc 0.5104	TestAcc 0.5127	BestValid 0.5259
	Epoch 1075:	Loss 1.3283	TrainAcc 0.5319	ValidAcc 0.5188	TestAcc 0.5208	BestValid 0.5259
	Epoch 1100:	Loss 1.3276	TrainAcc 0.5224	ValidAcc 0.5104	TestAcc 0.5087	BestValid 0.5259
	Epoch 1125:	Loss 1.3234	TrainAcc 0.5086	ValidAcc 0.4965	TestAcc 0.4995	BestValid 0.5259
	Epoch 1150:	Loss 1.3259	TrainAcc 0.5416	ValidAcc 0.5242	TestAcc 0.5263	BestValid 0.5259
	Epoch 1175:	Loss 1.3239	TrainAcc 0.5422	ValidAcc 0.5239	TestAcc 0.5270	BestValid 0.5259
	Epoch 1200:	Loss 1.3225	TrainAcc 0.5171	ValidAcc 0.5038	TestAcc 0.5036	BestValid 0.5259
	Epoch 1225:	Loss 1.3213	TrainAcc 0.5343	ValidAcc 0.5175	TestAcc 0.5216	BestValid 0.5259
	Epoch 1250:	Loss 1.3220	TrainAcc 0.5492	ValidAcc 0.5312	TestAcc 0.5329	BestValid 0.5312
	Epoch 1275:	Loss 1.3203	TrainAcc 0.5480	ValidAcc 0.5280	TestAcc 0.5314	BestValid 0.5312
	Epoch 1300:	Loss 1.3206	TrainAcc 0.5249	ValidAcc 0.5088	TestAcc 0.5099	BestValid 0.5312
	Epoch 1325:	Loss 1.3143	TrainAcc 0.5407	ValidAcc 0.5209	TestAcc 0.5247	BestValid 0.5312
	Epoch 1350:	Loss 1.3136	TrainAcc 0.5228	ValidAcc 0.5059	TestAcc 0.5075	BestValid 0.5312
	Epoch 1375:	Loss 1.3112	TrainAcc 0.5340	ValidAcc 0.5166	TestAcc 0.5191	BestValid 0.5312
	Epoch 1400:	Loss 1.3126	TrainAcc 0.5414	ValidAcc 0.5217	TestAcc 0.5249	BestValid 0.5312
	Epoch 1425:	Loss 1.3113	TrainAcc 0.5146	ValidAcc 0.4986	TestAcc 0.5015	BestValid 0.5312
	Epoch 1450:	Loss 1.3125	TrainAcc 0.5273	ValidAcc 0.5085	TestAcc 0.5107	BestValid 0.5312
	Epoch 1475:	Loss 1.3085	TrainAcc 0.5568	ValidAcc 0.5354	TestAcc 0.5365	BestValid 0.5354
	Epoch 1500:	Loss 1.3053	TrainAcc 0.5188	ValidAcc 0.5019	TestAcc 0.5046	BestValid 0.5354
	Epoch 1525:	Loss 1.3038	TrainAcc 0.5489	ValidAcc 0.5258	TestAcc 0.5298	BestValid 0.5354
	Epoch 1550:	Loss 1.3035	TrainAcc 0.5216	ValidAcc 0.5046	TestAcc 0.5053	BestValid 0.5354
	Epoch 1575:	Loss 1.3042	TrainAcc 0.5359	ValidAcc 0.5160	TestAcc 0.5196	BestValid 0.5354
	Epoch 1600:	Loss 1.3046	TrainAcc 0.5324	ValidAcc 0.5125	TestAcc 0.5149	BestValid 0.5354
	Epoch 1625:	Loss 1.2995	TrainAcc 0.5289	ValidAcc 0.5089	TestAcc 0.5111	BestValid 0.5354
	Epoch 1650:	Loss 1.3009	TrainAcc 0.5514	ValidAcc 0.5277	TestAcc 0.5329	BestValid 0.5354
	Epoch 1675:	Loss 1.2991	TrainAcc 0.5084	ValidAcc 0.4913	TestAcc 0.4937	BestValid 0.5354
	Epoch 1700:	Loss 1.2993	TrainAcc 0.5347	ValidAcc 0.5129	TestAcc 0.5163	BestValid 0.5354
	Epoch 1725:	Loss 1.2966	TrainAcc 0.5131	ValidAcc 0.4947	TestAcc 0.4966	BestValid 0.5354
	Epoch 1750:	Loss 1.2985	TrainAcc 0.5361	ValidAcc 0.5139	TestAcc 0.5179	BestValid 0.5354
	Epoch 1775:	Loss 1.2970	TrainAcc 0.5526	ValidAcc 0.5283	TestAcc 0.5308	BestValid 0.5354
	Epoch 1800:	Loss 1.2965	TrainAcc 0.5184	ValidAcc 0.4989	TestAcc 0.5013	BestValid 0.5354
	Epoch 1825:	Loss 1.2926	TrainAcc 0.5509	ValidAcc 0.5264	TestAcc 0.5317	BestValid 0.5354
	Epoch 1850:	Loss 1.2918	TrainAcc 0.5420	ValidAcc 0.5188	TestAcc 0.5233	BestValid 0.5354
	Epoch 1875:	Loss 1.2929	TrainAcc 0.5557	ValidAcc 0.5285	TestAcc 0.5325	BestValid 0.5354
	Epoch 1900:	Loss 1.2894	TrainAcc 0.5528	ValidAcc 0.5278	TestAcc 0.5305	BestValid 0.5354
	Epoch 1925:	Loss 1.2885	TrainAcc 0.5478	ValidAcc 0.5217	TestAcc 0.5243	BestValid 0.5354
	Epoch 1950:	Loss 1.2864	TrainAcc 0.5129	ValidAcc 0.4934	TestAcc 0.4935	BestValid 0.5354
	Epoch 1975:	Loss 1.2848	TrainAcc 0.5438	ValidAcc 0.5190	TestAcc 0.5229	BestValid 0.5354
	Epoch 2000:	Loss 1.2867	TrainAcc 0.5680	ValidAcc 0.5355	TestAcc 0.5383	BestValid 0.5355
	Epoch 2025:	Loss 1.2844	TrainAcc 0.5257	ValidAcc 0.5042	TestAcc 0.5049	BestValid 0.5355
	Epoch 2050:	Loss 1.2831	TrainAcc 0.5574	ValidAcc 0.5289	TestAcc 0.5329	BestValid 0.5355
	Epoch 2075:	Loss 1.2837	TrainAcc 0.5392	ValidAcc 0.5130	TestAcc 0.5157	BestValid 0.5355
	Epoch 2100:	Loss 1.2806	TrainAcc 0.5718	ValidAcc 0.5377	TestAcc 0.5413	BestValid 0.5377
	Epoch 2125:	Loss 1.2756	TrainAcc 0.5637	ValidAcc 0.5324	TestAcc 0.5365	BestValid 0.5377
	Epoch 2150:	Loss 1.2780	TrainAcc 0.5728	ValidAcc 0.5364	TestAcc 0.5396	BestValid 0.5377
	Epoch 2175:	Loss 1.2761	TrainAcc 0.5477	ValidAcc 0.5179	TestAcc 0.5211	BestValid 0.5377
	Epoch 2200:	Loss 1.2769	TrainAcc 0.5709	ValidAcc 0.5368	TestAcc 0.5404	BestValid 0.5377
	Epoch 2225:	Loss 1.2780	TrainAcc 0.5666	ValidAcc 0.5335	TestAcc 0.5380	BestValid 0.5377
	Epoch 2250:	Loss 1.2704	TrainAcc 0.5297	ValidAcc 0.5028	TestAcc 0.5042	BestValid 0.5377
	Epoch 2275:	Loss 1.2717	TrainAcc 0.5742	ValidAcc 0.5392	TestAcc 0.5418	BestValid 0.5392
	Epoch 2300:	Loss 1.2685	TrainAcc 0.5496	ValidAcc 0.5172	TestAcc 0.5212	BestValid 0.5392
	Epoch 2325:	Loss 1.2714	TrainAcc 0.5410	ValidAcc 0.5079	TestAcc 0.5134	BestValid 0.5392
	Epoch 2350:	Loss 1.2682	TrainAcc 0.5694	ValidAcc 0.5381	TestAcc 0.5407	BestValid 0.5392
	Epoch 2375:	Loss 1.2706	TrainAcc 0.5658	ValidAcc 0.5303	TestAcc 0.5348	BestValid 0.5392
	Epoch 2400:	Loss 1.2661	TrainAcc 0.5657	ValidAcc 0.5325	TestAcc 0.5365	BestValid 0.5392
	Epoch 2425:	Loss 1.2653	TrainAcc 0.5305	ValidAcc 0.5030	TestAcc 0.5066	BestValid 0.5392
	Epoch 2450:	Loss 1.2640	TrainAcc 0.5576	ValidAcc 0.5231	TestAcc 0.5254	BestValid 0.5392
	Epoch 2475:	Loss 1.2618	TrainAcc 0.5696	ValidAcc 0.5337	TestAcc 0.5361	BestValid 0.5392
	Epoch 2500:	Loss 1.2634	TrainAcc 0.5537	ValidAcc 0.5200	TestAcc 0.5225	BestValid 0.5392
	Epoch 2525:	Loss 1.2596	TrainAcc 0.5710	ValidAcc 0.5341	TestAcc 0.5368	BestValid 0.5392
	Epoch 2550:	Loss 1.2604	TrainAcc 0.5490	ValidAcc 0.5126	TestAcc 0.5171	BestValid 0.5392
	Epoch 2575:	Loss 1.2583	TrainAcc 0.5568	ValidAcc 0.5218	TestAcc 0.5262	BestValid 0.5392
	Epoch 2600:	Loss 1.2606	TrainAcc 0.5886	ValidAcc 0.5432	TestAcc 0.5447	BestValid 0.5432
	Epoch 2625:	Loss 1.2593	TrainAcc 0.5904	ValidAcc 0.5463	TestAcc 0.5465	BestValid 0.5463
	Epoch 2650:	Loss 1.2566	TrainAcc 0.5801	ValidAcc 0.5390	TestAcc 0.5415	BestValid 0.5463
	Epoch 2675:	Loss 1.2530	TrainAcc 0.5347	ValidAcc 0.5036	TestAcc 0.5065	BestValid 0.5463
	Epoch 2700:	Loss 1.2531	TrainAcc 0.5599	ValidAcc 0.5221	TestAcc 0.5258	BestValid 0.5463
	Epoch 2725:	Loss 1.2511	TrainAcc 0.5629	ValidAcc 0.5247	TestAcc 0.5286	BestValid 0.5463
	Epoch 2750:	Loss 1.2476	TrainAcc 0.5697	ValidAcc 0.5251	TestAcc 0.5297	BestValid 0.5463
	Epoch 2775:	Loss 1.2503	TrainAcc 0.5599	ValidAcc 0.5172	TestAcc 0.5219	BestValid 0.5463
	Epoch 2800:	Loss 1.2455	TrainAcc 0.5775	ValidAcc 0.5316	TestAcc 0.5377	BestValid 0.5463
	Epoch 2825:	Loss 1.2459	TrainAcc 0.5510	ValidAcc 0.5110	TestAcc 0.5144	BestValid 0.5463
	Epoch 2850:	Loss 1.2465	TrainAcc 0.5932	ValidAcc 0.5436	TestAcc 0.5465	BestValid 0.5463
	Epoch 2875:	Loss 1.2498	TrainAcc 0.5644	ValidAcc 0.5245	TestAcc 0.5279	BestValid 0.5463
	Epoch 2900:	Loss 1.2454	TrainAcc 0.5925	ValidAcc 0.5447	TestAcc 0.5468	BestValid 0.5463
	Epoch 2925:	Loss 1.2461	TrainAcc 0.5901	ValidAcc 0.5366	TestAcc 0.5403	BestValid 0.5463
	Epoch 2950:	Loss 1.2397	TrainAcc 0.5724	ValidAcc 0.5264	TestAcc 0.5303	BestValid 0.5463
	Epoch 2975:	Loss 1.2407	TrainAcc 0.5662	ValidAcc 0.5214	TestAcc 0.5263	BestValid 0.5463
	Epoch 3000:	Loss 1.2347	TrainAcc 0.5677	ValidAcc 0.5264	TestAcc 0.5292	BestValid 0.5463
	Epoch 3025:	Loss 1.2368	TrainAcc 0.5992	ValidAcc 0.5434	TestAcc 0.5461	BestValid 0.5463
	Epoch 3050:	Loss 1.2360	TrainAcc 0.5578	ValidAcc 0.5138	TestAcc 0.5180	BestValid 0.5463
	Epoch 3075:	Loss 1.2337	TrainAcc 0.5853	ValidAcc 0.5341	TestAcc 0.5399	BestValid 0.5463
	Epoch 3100:	Loss 1.2339	TrainAcc 0.5961	ValidAcc 0.5381	TestAcc 0.5430	BestValid 0.5463
	Epoch 3125:	Loss 1.2322	TrainAcc 0.6014	ValidAcc 0.5443	TestAcc 0.5475	BestValid 0.5463
	Epoch 3150:	Loss 1.2333	TrainAcc 0.6064	ValidAcc 0.5480	TestAcc 0.5497	BestValid 0.5480
	Epoch 3175:	Loss 1.2311	TrainAcc 0.5722	ValidAcc 0.5208	TestAcc 0.5243	BestValid 0.5480
	Epoch 3200:	Loss 1.2286	TrainAcc 0.5821	ValidAcc 0.5280	TestAcc 0.5336	BestValid 0.5480
	Epoch 3225:	Loss 1.2249	TrainAcc 0.5944	ValidAcc 0.5396	TestAcc 0.5423	BestValid 0.5480
	Epoch 3250:	Loss 1.2277	TrainAcc 0.5977	ValidAcc 0.5467	TestAcc 0.5472	BestValid 0.5480
	Epoch 3275:	Loss 1.2313	TrainAcc 0.5592	ValidAcc 0.5122	TestAcc 0.5147	BestValid 0.5480
	Epoch 3300:	Loss 1.2236	TrainAcc 0.5658	ValidAcc 0.5144	TestAcc 0.5190	BestValid 0.5480
	Epoch 3325:	Loss 1.2228	TrainAcc 0.5987	ValidAcc 0.5428	TestAcc 0.5453	BestValid 0.5480
	Epoch 3350:	Loss 1.2213	TrainAcc 0.5878	ValidAcc 0.5296	TestAcc 0.5350	BestValid 0.5480
	Epoch 3375:	Loss 1.2209	TrainAcc 0.5955	ValidAcc 0.5394	TestAcc 0.5424	BestValid 0.5480
	Epoch 3400:	Loss 1.2196	TrainAcc 0.5980	ValidAcc 0.5420	TestAcc 0.5456	BestValid 0.5480
	Epoch 3425:	Loss 1.2187	TrainAcc 0.5705	ValidAcc 0.5223	TestAcc 0.5260	BestValid 0.5480
	Epoch 3450:	Loss 1.2178	TrainAcc 0.5514	ValidAcc 0.5053	TestAcc 0.5071	BestValid 0.5480
	Epoch 3475:	Loss 1.2173	TrainAcc 0.6089	ValidAcc 0.5464	TestAcc 0.5491	BestValid 0.5480
	Epoch 3500:	Loss 1.2156	TrainAcc 0.6025	ValidAcc 0.5361	TestAcc 0.5431	BestValid 0.5480
	Epoch 3525:	Loss 1.2131	TrainAcc 0.5959	ValidAcc 0.5349	TestAcc 0.5398	BestValid 0.5480
	Epoch 3550:	Loss 1.2198	TrainAcc 0.6204	ValidAcc 0.5499	TestAcc 0.5497	BestValid 0.5499
	Epoch 3575:	Loss 1.2151	TrainAcc 0.6052	ValidAcc 0.5424	TestAcc 0.5469	BestValid 0.5499
	Epoch 3600:	Loss 1.2096	TrainAcc 0.6105	ValidAcc 0.5461	TestAcc 0.5487	BestValid 0.5499
	Epoch 3625:	Loss 1.2091	TrainAcc 0.6072	ValidAcc 0.5424	TestAcc 0.5452	BestValid 0.5499
	Epoch 3650:	Loss 1.2058	TrainAcc 0.5833	ValidAcc 0.5227	TestAcc 0.5262	BestValid 0.5499
	Epoch 3675:	Loss 1.2130	TrainAcc 0.6204	ValidAcc 0.5489	TestAcc 0.5491	BestValid 0.5499
	Epoch 3700:	Loss 1.2070	TrainAcc 0.6133	ValidAcc 0.5424	TestAcc 0.5461	BestValid 0.5499
	Epoch 3725:	Loss 1.2052	TrainAcc 0.6197	ValidAcc 0.5444	TestAcc 0.5466	BestValid 0.5499
	Epoch 3750:	Loss 1.2056	TrainAcc 0.6052	ValidAcc 0.5332	TestAcc 0.5386	BestValid 0.5499
	Epoch 3775:	Loss 1.2085	TrainAcc 0.5956	ValidAcc 0.5278	TestAcc 0.5337	BestValid 0.5499
	Epoch 3800:	Loss 1.2019	TrainAcc 0.6121	ValidAcc 0.5371	TestAcc 0.5422	BestValid 0.5499
	Epoch 3825:	Loss 1.2009	TrainAcc 0.6026	ValidAcc 0.5313	TestAcc 0.5347	BestValid 0.5499
	Epoch 3850:	Loss 1.2026	TrainAcc 0.6265	ValidAcc 0.5476	TestAcc 0.5503	BestValid 0.5499
	Epoch 3875:	Loss 1.1961	TrainAcc 0.6125	ValidAcc 0.5407	TestAcc 0.5424	BestValid 0.5499
	Epoch 3900:	Loss 1.1958	TrainAcc 0.6004	ValidAcc 0.5256	TestAcc 0.5305	BestValid 0.5499
	Epoch 3925:	Loss 1.1952	TrainAcc 0.6157	ValidAcc 0.5358	TestAcc 0.5397	BestValid 0.5499
	Epoch 3950:	Loss 1.1951	TrainAcc 0.6110	ValidAcc 0.5388	TestAcc 0.5413	BestValid 0.5499
	Epoch 3975:	Loss 1.1953	TrainAcc 0.6301	ValidAcc 0.5515	TestAcc 0.5541	BestValid 0.5515
	Epoch 4000:	Loss 1.1910	TrainAcc 0.6044	ValidAcc 0.5396	TestAcc 0.5438	BestValid 0.5515
	Epoch 4025:	Loss 1.1932	TrainAcc 0.6227	ValidAcc 0.5454	TestAcc 0.5480	BestValid 0.5515
	Epoch 4050:	Loss 1.1931	TrainAcc 0.6126	ValidAcc 0.5334	TestAcc 0.5370	BestValid 0.5515
	Epoch 4075:	Loss 1.1959	TrainAcc 0.6124	ValidAcc 0.5356	TestAcc 0.5400	BestValid 0.5515
	Epoch 4100:	Loss 1.1939	TrainAcc 0.6087	ValidAcc 0.5343	TestAcc 0.5378	BestValid 0.5515
	Epoch 4125:	Loss 1.1909	TrainAcc 0.5806	ValidAcc 0.5120	TestAcc 0.5149	BestValid 0.5515
	Epoch 4150:	Loss 1.1901	TrainAcc 0.5610	ValidAcc 0.5020	TestAcc 0.5045	BestValid 0.5515
	Epoch 4175:	Loss 1.1952	TrainAcc 0.6266	ValidAcc 0.5437	TestAcc 0.5460	BestValid 0.5515
	Epoch 4200:	Loss 1.1887	TrainAcc 0.6348	ValidAcc 0.5450	TestAcc 0.5435	BestValid 0.5515
	Epoch 4225:	Loss 1.1844	TrainAcc 0.5918	ValidAcc 0.5186	TestAcc 0.5215	BestValid 0.5515
	Epoch 4250:	Loss 1.1877	TrainAcc 0.6343	ValidAcc 0.5481	TestAcc 0.5466	BestValid 0.5515
	Epoch 4275:	Loss 1.1900	TrainAcc 0.6255	ValidAcc 0.5442	TestAcc 0.5485	BestValid 0.5515
	Epoch 4300:	Loss 1.1818	TrainAcc 0.6375	ValidAcc 0.5511	TestAcc 0.5515	BestValid 0.5515
	Epoch 4325:	Loss 1.1806	TrainAcc 0.6169	ValidAcc 0.5382	TestAcc 0.5401	BestValid 0.5515
	Epoch 4350:	Loss 1.1802	TrainAcc 0.6324	ValidAcc 0.5435	TestAcc 0.5461	BestValid 0.5515
	Epoch 4375:	Loss 1.1788	TrainAcc 0.6155	ValidAcc 0.5333	TestAcc 0.5374	BestValid 0.5515
	Epoch 4400:	Loss 1.1810	TrainAcc 0.6372	ValidAcc 0.5478	TestAcc 0.5493	BestValid 0.5515
	Epoch 4425:	Loss 1.1774	TrainAcc 0.6286	ValidAcc 0.5359	TestAcc 0.5396	BestValid 0.5515
	Epoch 4450:	Loss 1.1731	TrainAcc 0.6253	ValidAcc 0.5389	TestAcc 0.5422	BestValid 0.5515
	Epoch 4475:	Loss 1.1733	TrainAcc 0.6372	ValidAcc 0.5491	TestAcc 0.5495	BestValid 0.5515
	Epoch 4500:	Loss 1.1705	TrainAcc 0.6421	ValidAcc 0.5510	TestAcc 0.5508	BestValid 0.5515
	Epoch 4525:	Loss 1.1704	TrainAcc 0.6000	ValidAcc 0.5182	TestAcc 0.5198	BestValid 0.5515
	Epoch 4550:	Loss 1.1712	TrainAcc 0.6365	ValidAcc 0.5418	TestAcc 0.5450	BestValid 0.5515
	Epoch 4575:	Loss 1.1697	TrainAcc 0.6382	ValidAcc 0.5476	TestAcc 0.5493	BestValid 0.5515
	Epoch 4600:	Loss 1.1710	TrainAcc 0.6453	ValidAcc 0.5507	TestAcc 0.5515	BestValid 0.5515
	Epoch 4625:	Loss 1.1691	TrainAcc 0.6427	ValidAcc 0.5497	TestAcc 0.5499	BestValid 0.5515
	Epoch 4650:	Loss 1.1726	TrainAcc 0.6248	ValidAcc 0.5371	TestAcc 0.5393	BestValid 0.5515
	Epoch 4675:	Loss 1.1667	TrainAcc 0.6177	ValidAcc 0.5322	TestAcc 0.5353	BestValid 0.5515
	Epoch 4700:	Loss 1.1670	TrainAcc 0.6233	ValidAcc 0.5319	TestAcc 0.5351	BestValid 0.5515
	Epoch 4725:	Loss 1.1710	TrainAcc 0.6083	ValidAcc 0.5196	TestAcc 0.5227	BestValid 0.5515
	Epoch 4750:	Loss 1.1647	TrainAcc 0.6433	ValidAcc 0.5388	TestAcc 0.5405	BestValid 0.5515
	Epoch 4775:	Loss 1.1685	TrainAcc 0.6429	ValidAcc 0.5462	TestAcc 0.5467	BestValid 0.5515
	Epoch 4800:	Loss 1.1618	TrainAcc 0.6097	ValidAcc 0.5238	TestAcc 0.5278	BestValid 0.5515
	Epoch 4825:	Loss 1.1629	TrainAcc 0.6435	ValidAcc 0.5433	TestAcc 0.5446	BestValid 0.5515
	Epoch 4850:	Loss 1.1647	TrainAcc 0.6325	ValidAcc 0.5374	TestAcc 0.5410	BestValid 0.5515
	Epoch 4875:	Loss 1.1689	TrainAcc 0.6384	ValidAcc 0.5443	TestAcc 0.5463	BestValid 0.5515
	Epoch 4900:	Loss 1.1678	TrainAcc 0.6305	ValidAcc 0.5418	TestAcc 0.5468	BestValid 0.5515
	Epoch 4925:	Loss 1.1597	TrainAcc 0.6363	ValidAcc 0.5415	TestAcc 0.5451	BestValid 0.5515
	Epoch 4950:	Loss 1.1637	TrainAcc 0.6328	ValidAcc 0.5423	TestAcc 0.5464	BestValid 0.5515
	Epoch 4975:	Loss 1.1603	TrainAcc 0.6464	ValidAcc 0.5473	TestAcc 0.5486	BestValid 0.5515
	Epoch 5000:	Loss 1.1594	TrainAcc 0.6519	ValidAcc 0.5477	TestAcc 0.5497	BestValid 0.5515
Node 1, Pre/Post-Pipelining: 0.893 / 1.219 ms, Bubble: 26.729 ms, Compute: 105.410 ms, Comm: 15.515 ms, Imbalance: 6.924 ms
Node 5, Pre/Post-Pipelining: 0.893 / 1.160 ms, Bubble: 27.392 ms, Compute: 104.762 ms, Comm: 16.324 ms, Imbalance: 6.223 ms
Node 3, Pre/Post-Pipelining: 0.888 / 1.037 ms, Bubble: 26.996 ms, Compute: 104.021 ms, Comm: 16.076 ms, Imbalance: 7.702 ms
Node 4, Pre/Post-Pipelining: 0.893 / 1.088 ms, Bubble: 27.214 ms, Compute: 105.875 ms, Comm: 16.290 ms, Imbalance: 5.425 ms
Node 2, Pre/Post-Pipelining: 0.892 / 1.047 ms, Bubble: 27.172 ms, Compute: 102.953 ms, Comm: 16.962 ms, Imbalance: 7.866 ms
Node 6, Pre/Post-Pipelining: 0.894 / 1.140 ms, Bubble: 27.945 ms, Compute: 103.797 ms, Comm: 15.111 ms, Imbalance: 8.019 ms
Node 7, Pre/Post-Pipelining: 0.891 / 4.147 ms, Bubble: 26.188 ms, Compute: 94.017 ms, Comm: 11.027 ms, Imbalance: 20.796 ms
Node 0, Pre/Post-Pipelining: 0.888 / 1.313 ms, Bubble: 26.801 ms, Compute: 110.619 ms, Comm: 11.430 ms, Imbalance: 5.518 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 0.888 ms
Cluster-Wide Average, Post-Pipelining Overhead: 1.313 ms
Cluster-Wide Average, Bubble: 26.801 ms
Cluster-Wide Average, Compute: 110.619 ms
Cluster-Wide Average, Communication: 11.430 ms
Cluster-Wide Average, Imbalance: 5.518 ms
Node 0, GPU memory consumption: 3.913 GB
Node 4, GPU memory consumption: 3.071 GB
Node 2, GPU memory consumption: 3.094 GB
Node 7, GPU memory consumption: 2.782 GB
Node 3, GPU memory consumption: 3.071 GB
Node 5, GPU memory consumption: 3.094 GB
Node 1, GPU memory consumption: 3.094 GB
Node 6, GPU memory consumption: 3.094 GB
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 0,  per-epoch time: 0.532340 s---------------
------------------------node id 4,  per-epoch time: 0.532341 s---------------
------------------------node id 1,  per-epoch time: 0.532340 s---------------
------------------------node id 5,  per-epoch time: 0.532341 s---------------
------------------------node id 2,  per-epoch time: 0.532340 s---------------
------------------------node id 6,  per-epoch time: 0.532341 s---------------
------------------------node id 3,  per-epoch time: 0.532340 s---------------
------------------------node id 7,  per-epoch time: 0.532341 s---------------
************ Profiling Results ************
	Bubble: 428.808953 (ms) (80.59 percentage)
	Compute: 99.472039 (ms) (18.70 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 3.794451 (ms) (0.71 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.931 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.010 GB
	Total communication (cluster-wide, per-epoch): 0.941 GB
	Aggregated layer-level communication throughput: 538.798 Gbps
Highest valid_acc: 0.5515
Target test_acc: 0.5541
Epoch to reach the target acc: 3974
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
