gnerv1
Thu Aug  3 22:56:08 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   30C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   29C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   29C    P8    19W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   28C    P8    23W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 30%] Built target context
[ 30%] Built target core
[ 77%] Built target cudahelp
[ 94%] Built target OSDI2023_MULTI_NODES_graphsage
[ 94%] Built target OSDI2023_MULTI_NODES_gcn
[ 94%] Built target OSDI2023_MULTI_NODES_gcnii
[ 94%] Built target estimate_comm_volume
Running experiments...
gnerv2
gnerv2
gnerv2
gnerv2
gnerv3
gnerv3
gnerv3
gnerv3
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.829 seconds.
Building the CSC structure...
        It takes 1.858 seconds.
Building the CSC structure...
        It takes 1.904 seconds.
Building the CSC structure...
        It takes 2.053 seconds.
Building the CSC structure...
        It takes 2.061 seconds.
Building the CSC structure...
        It takes 2.242 seconds.
Building the CSC structure...
        It takes 2.290 seconds.
Building the CSC structure...
        It takes 2.379 seconds.
Building the CSC structure...
        It takes 1.771 seconds.
        It takes 1.827 seconds.
        It takes 1.842 seconds.
        It takes 1.845 seconds.
        It takes 1.936 seconds.
        It takes 2.154 seconds.
        It takes 2.171 seconds.
        It takes 2.154 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.248 seconds.
Building the Label Vector...
        It takes 0.041 seconds.
        It takes 0.279 seconds.
Building the Label Vector...
        It takes 0.036 seconds.
        It takes 0.287 seconds.
Building the Feature Vector...
Building the Label Vector...
        It takes 0.042 seconds.
        It takes 0.266 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.268 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/reddit/8_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.293 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
GPU 0, layer [0, 32)
        It takes 0.254 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
GPU 0, layer [0, 32)
GPU 0, layer [0, 32)
Building the Feature Vector...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
        It takes 0.245 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
Chunks (number of global chunks: 8): 0-[0, 31876) 1-[31876, 56041) 2-[56041, 86963) 3-[86963, 111539) 4-[111539, 150873) 5-[150873, 175023) 6-[175023, 206335) 7-[206335, 232965)
GPU 0, layer [0, 32)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
GPU 0, layer [0, 32)
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 56.212 Gbps (per GPU), 449.693 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.961 Gbps (per GPU), 447.691 Gbps (aggregated)
The layer-level communication performance: 55.969 Gbps (per GPU), 447.752 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.754 Gbps (per GPU), 446.034 Gbps (aggregated)
The layer-level communication performance: 55.727 Gbps (per GPU), 445.818 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.534 Gbps (per GPU), 444.273 Gbps (aggregated)
The layer-level communication performance: 55.497 Gbps (per GPU), 443.975 Gbps (aggregated)
The layer-level communication performance: 55.465 Gbps (per GPU), 443.724 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.358 Gbps (per GPU), 1250.864 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.376 Gbps (per GPU), 1251.004 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.352 Gbps (per GPU), 1250.818 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.381 Gbps (per GPU), 1251.051 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.358 Gbps (per GPU), 1250.864 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.355 Gbps (per GPU), 1250.841 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.274 Gbps (per GPU), 1250.188 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.393 Gbps (per GPU), 1251.144 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.897 Gbps (per GPU), 807.179 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.899 Gbps (per GPU), 807.192 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.897 Gbps (per GPU), 807.173 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.893 Gbps (per GPU), 807.148 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.897 Gbps (per GPU), 807.179 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.881 Gbps (per GPU), 807.050 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.898 Gbps (per GPU), 807.185 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.881 Gbps (per GPU), 807.048 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 31.561 Gbps (per GPU), 252.487 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.561 Gbps (per GPU), 252.484 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.559 Gbps (per GPU), 252.471 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.561 Gbps (per GPU), 252.489 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.561 Gbps (per GPU), 252.486 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.560 Gbps (per GPU), 252.481 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.560 Gbps (per GPU), 252.477 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 31.559 Gbps (per GPU), 252.476 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  9.35ms  9.28ms  8.84ms  1.06 31.88K 14.26M
 chk_1  9.79ms  9.49ms  9.16ms  1.07 24.16K 14.58M
 chk_2 10.24ms  9.82ms  9.30ms  1.10 30.92K 14.38M
 chk_3 10.07ms  9.75ms  9.38ms  1.07 24.58K 14.59M
 chk_4 10.07ms  9.56ms  8.88ms  1.13 39.33K 13.77M
 chk_5  9.99ms  9.75ms  9.32ms  1.07 24.15K 14.35M
 chk_6 11.01ms 10.63ms 10.02ms  1.10 31.31K 14.22M
 chk_7 10.60ms  9.89ms  9.33ms  1.14 26.63K 14.46M
   Avg 10.14  9.77  9.28
   Max 11.01 10.63 10.02
   Min  9.35  9.28  8.84
 Ratio  1.18  1.15  1.13
   Var  0.22  0.14  0.12
Profiling takes 2.667 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 315.576 ms
Partition 0 [0, 4) has cost: 315.576 ms
Partition 1 [4, 8) has cost: 312.621 ms
Partition 2 [8, 12) has cost: 312.621 ms
Partition 3 [12, 16) has cost: 312.621 ms
Partition 4 [16, 20) has cost: 312.621 ms
Partition 5 [20, 24) has cost: 312.621 ms
Partition 6 [24, 28) has cost: 312.621 ms
Partition 7 [28, 32) has cost: 308.700 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 229.005 ms
GPU 0, Compute+Comm Time: 117.491 ms, Bubble Time: 109.645 ms, Imbalance Overhead: 1.869 ms
GPU 1, Compute+Comm Time: 116.876 ms, Bubble Time: 107.606 ms, Imbalance Overhead: 4.523 ms
GPU 2, Compute+Comm Time: 116.876 ms, Bubble Time: 105.782 ms, Imbalance Overhead: 6.347 ms
GPU 3, Compute+Comm Time: 116.876 ms, Bubble Time: 104.794 ms, Imbalance Overhead: 7.335 ms
GPU 4, Compute+Comm Time: 116.876 ms, Bubble Time: 103.683 ms, Imbalance Overhead: 8.446 ms
GPU 5, Compute+Comm Time: 116.876 ms, Bubble Time: 102.870 ms, Imbalance Overhead: 9.259 ms
GPU 6, Compute+Comm Time: 116.876 ms, Bubble Time: 102.087 ms, Imbalance Overhead: 10.042 ms
GPU 7, Compute+Comm Time: 115.523 ms, Bubble Time: 103.486 ms, Imbalance Overhead: 9.996 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 435.346 ms
GPU 0, Compute+Comm Time: 219.701 ms, Bubble Time: 196.951 ms, Imbalance Overhead: 18.693 ms
GPU 1, Compute+Comm Time: 222.269 ms, Bubble Time: 193.919 ms, Imbalance Overhead: 19.158 ms
GPU 2, Compute+Comm Time: 222.269 ms, Bubble Time: 195.671 ms, Imbalance Overhead: 17.406 ms
GPU 3, Compute+Comm Time: 222.269 ms, Bubble Time: 197.710 ms, Imbalance Overhead: 15.367 ms
GPU 4, Compute+Comm Time: 222.269 ms, Bubble Time: 199.869 ms, Imbalance Overhead: 13.208 ms
GPU 5, Compute+Comm Time: 222.269 ms, Bubble Time: 201.733 ms, Imbalance Overhead: 11.344 ms
GPU 6, Compute+Comm Time: 222.269 ms, Bubble Time: 204.973 ms, Imbalance Overhead: 8.104 ms
GPU 7, Compute+Comm Time: 224.610 ms, Bubble Time: 208.198 ms, Imbalance Overhead: 2.539 ms
The estimated cost of the whole pipeline: 697.569 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 628.197 ms
Partition 0 [0, 8) has cost: 628.197 ms
Partition 1 [8, 16) has cost: 625.242 ms
Partition 2 [16, 24) has cost: 625.242 ms
Partition 3 [24, 32) has cost: 621.321 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 233.937 ms
GPU 0, Compute+Comm Time: 129.661 ms, Bubble Time: 99.901 ms, Imbalance Overhead: 4.375 ms
GPU 1, Compute+Comm Time: 129.230 ms, Bubble Time: 98.363 ms, Imbalance Overhead: 6.345 ms
GPU 2, Compute+Comm Time: 129.230 ms, Bubble Time: 96.856 ms, Imbalance Overhead: 7.851 ms
GPU 3, Compute+Comm Time: 128.462 ms, Bubble Time: 99.748 ms, Imbalance Overhead: 5.727 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 426.042 ms
GPU 0, Compute+Comm Time: 234.329 ms, Bubble Time: 180.376 ms, Imbalance Overhead: 11.337 ms
GPU 1, Compute+Comm Time: 235.715 ms, Bubble Time: 174.451 ms, Imbalance Overhead: 15.875 ms
GPU 2, Compute+Comm Time: 235.715 ms, Bubble Time: 178.560 ms, Imbalance Overhead: 11.766 ms
GPU 3, Compute+Comm Time: 237.132 ms, Bubble Time: 182.731 ms, Imbalance Overhead: 6.179 ms
    The estimated cost with 2 DP ways is 692.978 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 1253.439 ms
Partition 0 [0, 16) has cost: 1253.439 ms
Partition 1 [16, 32) has cost: 1246.563 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 266.735 ms
GPU 0, Compute+Comm Time: 173.319 ms, Bubble Time: 79.006 ms, Imbalance Overhead: 14.411 ms
GPU 1, Compute+Comm Time: 172.711 ms, Bubble Time: 94.025 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 432.807 ms
GPU 0, Compute+Comm Time: 281.354 ms, Bubble Time: 151.453 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 282.951 ms, Bubble Time: 130.568 ms, Imbalance Overhead: 19.288 ms
    The estimated cost with 4 DP ways is 734.519 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 2500.003 ms
Partition 0 [0, 32) has cost: 2500.003 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 559.333 ms
GPU 0, Compute+Comm Time: 559.333 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 673.842 ms
GPU 0, Compute+Comm Time: 673.842 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 1294.834 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 190)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 111539, Num Local Vertices: 39334
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 190)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 31876
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 190)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 150873, Num Local Vertices: 24150
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 190)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 31876, Num Local Vertices: 24165
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 190)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 175023, Num Local Vertices: 31312
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 190)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 56041, Num Local Vertices: 30922
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 190)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 206335, Num Local Vertices: 26630
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 190)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 86963, Num Local Vertices: 24576
*** Node 7, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 190)...
+++++++++ Node 3 initializing the weights for op[0, 190)...
+++++++++ Node 7 initializing the weights for op[0, 190)...
+++++++++ Node 1 initializing the weights for op[0, 190)...
+++++++++ Node 4 initializing the weights for op[0, 190)...
+++++++++ Node 0 initializing the weights for op[0, 190)...
+++++++++ Node 6 initializing the weights for op[0, 190)...
+++++++++ Node 2 initializing the weights for op[0, 190)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 1094887
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 2.2807	TrainAcc 0.5365	ValidAcc 0.5664	TestAcc 0.5640	BestValid 0.5664
	Epoch 50:	Loss 1.2026	TrainAcc 0.7855	ValidAcc 0.8031	TestAcc 0.7987	BestValid 0.8031
	Epoch 75:	Loss 0.7014	TrainAcc 0.8692	ValidAcc 0.8812	TestAcc 0.8773	BestValid 0.8812
	Epoch 100:	Loss 0.5555	TrainAcc 0.8925	ValidAcc 0.9032	TestAcc 0.9000	BestValid 0.9032
	Epoch 125:	Loss 0.4929	TrainAcc 0.9046	ValidAcc 0.9136	TestAcc 0.9120	BestValid 0.9136
	Epoch 150:	Loss 0.4587	TrainAcc 0.9090	ValidAcc 0.9183	TestAcc 0.9169	BestValid 0.9183
	Epoch 175:	Loss 0.4359	TrainAcc 0.9126	ValidAcc 0.9206	TestAcc 0.9198	BestValid 0.9206
	Epoch 200:	Loss 0.4189	TrainAcc 0.9155	ValidAcc 0.9226	TestAcc 0.9224	BestValid 0.9226
	Epoch 225:	Loss 0.4062	TrainAcc 0.9176	ValidAcc 0.9243	TestAcc 0.9246	BestValid 0.9243
	Epoch 250:	Loss 0.3954	TrainAcc 0.9194	ValidAcc 0.9260	TestAcc 0.9255	BestValid 0.9260
	Epoch 275:	Loss 0.3865	TrainAcc 0.9210	ValidAcc 0.9266	TestAcc 0.9273	BestValid 0.9266
	Epoch 300:	Loss 0.3784	TrainAcc 0.9225	ValidAcc 0.9276	TestAcc 0.9282	BestValid 0.9276
	Epoch 325:	Loss 0.3711	TrainAcc 0.9238	ValidAcc 0.9289	TestAcc 0.9296	BestValid 0.9289
	Epoch 350:	Loss 0.3645	TrainAcc 0.9249	ValidAcc 0.9295	TestAcc 0.9302	BestValid 0.9295
	Epoch 375:	Loss 0.3580	TrainAcc 0.9259	ValidAcc 0.9310	TestAcc 0.9310	BestValid 0.9310
	Epoch 400:	Loss 0.3545	TrainAcc 0.9272	ValidAcc 0.9324	TestAcc 0.9323	BestValid 0.9324
	Epoch 425:	Loss 0.3492	TrainAcc 0.9280	ValidAcc 0.9332	TestAcc 0.9332	BestValid 0.9332
	Epoch 450:	Loss 0.3446	TrainAcc 0.9285	ValidAcc 0.9339	TestAcc 0.9335	BestValid 0.9339
	Epoch 475:	Loss 0.3411	TrainAcc 0.9294	ValidAcc 0.9347	TestAcc 0.9342	BestValid 0.9347
	Epoch 500:	Loss 0.3377	TrainAcc 0.9303	ValidAcc 0.9347	TestAcc 0.9350	BestValid 0.9347
	Epoch 525:	Loss 0.3340	TrainAcc 0.9309	ValidAcc 0.9352	TestAcc 0.9354	BestValid 0.9352
	Epoch 550:	Loss 0.3312	TrainAcc 0.9314	ValidAcc 0.9365	TestAcc 0.9360	BestValid 0.9365
	Epoch 575:	Loss 0.3273	TrainAcc 0.9323	ValidAcc 0.9363	TestAcc 0.9360	BestValid 0.9365
	Epoch 600:	Loss 0.3257	TrainAcc 0.9328	ValidAcc 0.9366	TestAcc 0.9369	BestValid 0.9366
	Epoch 625:	Loss 0.3219	TrainAcc 0.9332	ValidAcc 0.9371	TestAcc 0.9374	BestValid 0.9371
	Epoch 650:	Loss 0.3200	TrainAcc 0.9338	ValidAcc 0.9375	TestAcc 0.9380	BestValid 0.9375
	Epoch 675:	Loss 0.3183	TrainAcc 0.9342	ValidAcc 0.9376	TestAcc 0.9383	BestValid 0.9376
	Epoch 700:	Loss 0.3159	TrainAcc 0.9345	ValidAcc 0.9375	TestAcc 0.9385	BestValid 0.9376
	Epoch 725:	Loss 0.3143	TrainAcc 0.9347	ValidAcc 0.9377	TestAcc 0.9389	BestValid 0.9377
	Epoch 750:	Loss 0.3127	TrainAcc 0.9352	ValidAcc 0.9382	TestAcc 0.9393	BestValid 0.9382
	Epoch 775:	Loss 0.3098	TrainAcc 0.9353	ValidAcc 0.9386	TestAcc 0.9394	BestValid 0.9386
	Epoch 800:	Loss 0.3088	TrainAcc 0.9355	ValidAcc 0.9384	TestAcc 0.9396	BestValid 0.9386
	Epoch 825:	Loss 0.3075	TrainAcc 0.9361	ValidAcc 0.9389	TestAcc 0.9401	BestValid 0.9389
	Epoch 850:	Loss 0.3050	TrainAcc 0.9364	ValidAcc 0.9389	TestAcc 0.9401	BestValid 0.9389
	Epoch 875:	Loss 0.3043	TrainAcc 0.9366	ValidAcc 0.9386	TestAcc 0.9404	BestValid 0.9389
	Epoch 900:	Loss 0.3036	TrainAcc 0.9368	ValidAcc 0.9392	TestAcc 0.9407	BestValid 0.9392
	Epoch 925:	Loss 0.3010	TrainAcc 0.9370	ValidAcc 0.9395	TestAcc 0.9408	BestValid 0.9395
	Epoch 950:	Loss 0.2990	TrainAcc 0.9373	ValidAcc 0.9397	TestAcc 0.9412	BestValid 0.9397
	Epoch 975:	Loss 0.2984	TrainAcc 0.9377	ValidAcc 0.9398	TestAcc 0.9410	BestValid 0.9398
	Epoch 1000:	Loss 0.2980	TrainAcc 0.9377	ValidAcc 0.9396	TestAcc 0.9412	BestValid 0.9398
	Epoch 1025:	Loss 0.2965	TrainAcc 0.9381	ValidAcc 0.9400	TestAcc 0.9415	BestValid 0.9400
	Epoch 1050:	Loss 0.2954	TrainAcc 0.9382	ValidAcc 0.9399	TestAcc 0.9415	BestValid 0.9400
	Epoch 1075:	Loss 0.2943	TrainAcc 0.9382	ValidAcc 0.9407	TestAcc 0.9417	BestValid 0.9407
	Epoch 1100:	Loss 0.2921	TrainAcc 0.9386	ValidAcc 0.9403	TestAcc 0.9419	BestValid 0.9407
	Epoch 1125:	Loss 0.2916	TrainAcc 0.9386	ValidAcc 0.9406	TestAcc 0.9420	BestValid 0.9407
	Epoch 1150:	Loss 0.2910	TrainAcc 0.9390	ValidAcc 0.9406	TestAcc 0.9422	BestValid 0.9407
	Epoch 1175:	Loss 0.2889	TrainAcc 0.9389	ValidAcc 0.9409	TestAcc 0.9423	BestValid 0.9409
	Epoch 1200:	Loss 0.2885	TrainAcc 0.9394	ValidAcc 0.9409	TestAcc 0.9423	BestValid 0.9409
	Epoch 1225:	Loss 0.2879	TrainAcc 0.9395	ValidAcc 0.9410	TestAcc 0.9426	BestValid 0.9410
	Epoch 1250:	Loss 0.2874	TrainAcc 0.9398	ValidAcc 0.9411	TestAcc 0.9428	BestValid 0.9411
	Epoch 1275:	Loss 0.2856	TrainAcc 0.9397	ValidAcc 0.9411	TestAcc 0.9428	BestValid 0.9411
	Epoch 1300:	Loss 0.2861	TrainAcc 0.9399	ValidAcc 0.9411	TestAcc 0.9429	BestValid 0.9411
	Epoch 1325:	Loss 0.2839	TrainAcc 0.9402	ValidAcc 0.9415	TestAcc 0.9431	BestValid 0.9415
	Epoch 1350:	Loss 0.2837	TrainAcc 0.9402	ValidAcc 0.9418	TestAcc 0.9432	BestValid 0.9418
	Epoch 1375:	Loss 0.2828	TrainAcc 0.9407	ValidAcc 0.9413	TestAcc 0.9433	BestValid 0.9418
	Epoch 1400:	Loss 0.2819	TrainAcc 0.9409	ValidAcc 0.9415	TestAcc 0.9433	BestValid 0.9418
	Epoch 1425:	Loss 0.2808	TrainAcc 0.9410	ValidAcc 0.9418	TestAcc 0.9436	BestValid 0.9418
	Epoch 1450:	Loss 0.2792	TrainAcc 0.9411	ValidAcc 0.9418	TestAcc 0.9434	BestValid 0.9418
	Epoch 1475:	Loss 0.2793	TrainAcc 0.9412	ValidAcc 0.9418	TestAcc 0.9436	BestValid 0.9418
	Epoch 1500:	Loss 0.2782	TrainAcc 0.9415	ValidAcc 0.9421	TestAcc 0.9438	BestValid 0.9421
	Epoch 1525:	Loss 0.2774	TrainAcc 0.9415	ValidAcc 0.9418	TestAcc 0.9434	BestValid 0.9421
	Epoch 1550:	Loss 0.2775	TrainAcc 0.9415	ValidAcc 0.9425	TestAcc 0.9438	BestValid 0.9425
	Epoch 1575:	Loss 0.2757	TrainAcc 0.9418	ValidAcc 0.9421	TestAcc 0.9435	BestValid 0.9425
	Epoch 1600:	Loss 0.2741	TrainAcc 0.9421	ValidAcc 0.9425	TestAcc 0.9440	BestValid 0.9425
	Epoch 1625:	Loss 0.2746	TrainAcc 0.9421	ValidAcc 0.9421	TestAcc 0.9439	BestValid 0.9425
	Epoch 1650:	Loss 0.2742	TrainAcc 0.9422	ValidAcc 0.9431	TestAcc 0.9444	BestValid 0.9431
	Epoch 1675:	Loss 0.2731	TrainAcc 0.9426	ValidAcc 0.9429	TestAcc 0.9445	BestValid 0.9431
	Epoch 1700:	Loss 0.2725	TrainAcc 0.9424	ValidAcc 0.9428	TestAcc 0.9443	BestValid 0.9431
	Epoch 1725:	Loss 0.2716	TrainAcc 0.9425	ValidAcc 0.9425	TestAcc 0.9443	BestValid 0.9431
	Epoch 1750:	Loss 0.2707	TrainAcc 0.9423	ValidAcc 0.9424	TestAcc 0.9442	BestValid 0.9431
	Epoch 1775:	Loss 0.2701	TrainAcc 0.9428	ValidAcc 0.9435	TestAcc 0.9446	BestValid 0.9435
	Epoch 1800:	Loss 0.2698	TrainAcc 0.9431	ValidAcc 0.9428	TestAcc 0.9445	BestValid 0.9435
	Epoch 1825:	Loss 0.2688	TrainAcc 0.9431	ValidAcc 0.9428	TestAcc 0.9446	BestValid 0.9435
	Epoch 1850:	Loss 0.2683	TrainAcc 0.9433	ValidAcc 0.9436	TestAcc 0.9448	BestValid 0.9436
	Epoch 1875:	Loss 0.2678	TrainAcc 0.9433	ValidAcc 0.9434	TestAcc 0.9445	BestValid 0.9436
	Epoch 1900:	Loss 0.2672	TrainAcc 0.9434	ValidAcc 0.9436	TestAcc 0.9449	BestValid 0.9436
	Epoch 1925:	Loss 0.2666	TrainAcc 0.9436	ValidAcc 0.9434	TestAcc 0.9448	BestValid 0.9436
	Epoch 1950:	Loss 0.2656	TrainAcc 0.9436	ValidAcc 0.9433	TestAcc 0.9449	BestValid 0.9436
	Epoch 1975:	Loss 0.2655	TrainAcc 0.9438	ValidAcc 0.9437	TestAcc 0.9449	BestValid 0.9437
	Epoch 2000:	Loss 0.2654	TrainAcc 0.9436	ValidAcc 0.9439	TestAcc 0.9448	BestValid 0.9439
	Epoch 2025:	Loss 0.2653	TrainAcc 0.9439	ValidAcc 0.9440	TestAcc 0.9447	BestValid 0.9440
	Epoch 2050:	Loss 0.2638	TrainAcc 0.9440	ValidAcc 0.9444	TestAcc 0.9449	BestValid 0.9444
	Epoch 2075:	Loss 0.2640	TrainAcc 0.9440	ValidAcc 0.9446	TestAcc 0.9452	BestValid 0.9446
	Epoch 2100:	Loss 0.2638	TrainAcc 0.9442	ValidAcc 0.9441	TestAcc 0.9454	BestValid 0.9446
	Epoch 2125:	Loss 0.2619	TrainAcc 0.9442	ValidAcc 0.9446	TestAcc 0.9452	BestValid 0.9446
	Epoch 2150:	Loss 0.2621	TrainAcc 0.9443	ValidAcc 0.9449	TestAcc 0.9456	BestValid 0.9449
	Epoch 2175:	Loss 0.2613	TrainAcc 0.9447	ValidAcc 0.9446	TestAcc 0.9452	BestValid 0.9449
	Epoch 2200:	Loss 0.2608	TrainAcc 0.9444	ValidAcc 0.9451	TestAcc 0.9454	BestValid 0.9451
	Epoch 2225:	Loss 0.2600	TrainAcc 0.9448	ValidAcc 0.9447	TestAcc 0.9455	BestValid 0.9451
	Epoch 2250:	Loss 0.2595	TrainAcc 0.9446	ValidAcc 0.9447	TestAcc 0.9451	BestValid 0.9451
	Epoch 2275:	Loss 0.2590	TrainAcc 0.9450	ValidAcc 0.9450	TestAcc 0.9459	BestValid 0.9451
	Epoch 2300:	Loss 0.2583	TrainAcc 0.9451	ValidAcc 0.9449	TestAcc 0.9454	BestValid 0.9451
	Epoch 2325:	Loss 0.2588	TrainAcc 0.9453	ValidAcc 0.9449	TestAcc 0.9457	BestValid 0.9451
	Epoch 2350:	Loss 0.2575	TrainAcc 0.9451	ValidAcc 0.9446	TestAcc 0.9454	BestValid 0.9451
	Epoch 2375:	Loss 0.2573	TrainAcc 0.9451	ValidAcc 0.9451	TestAcc 0.9455	BestValid 0.9451
	Epoch 2400:	Loss 0.2569	TrainAcc 0.9454	ValidAcc 0.9456	TestAcc 0.9459	BestValid 0.9456
	Epoch 2425:	Loss 0.2558	TrainAcc 0.9454	ValidAcc 0.9456	TestAcc 0.9457	BestValid 0.9456
	Epoch 2450:	Loss 0.2564	TrainAcc 0.9455	ValidAcc 0.9453	TestAcc 0.9456	BestValid 0.9456
	Epoch 2475:	Loss 0.2554	TrainAcc 0.9457	ValidAcc 0.9455	TestAcc 0.9461	BestValid 0.9456
	Epoch 2500:	Loss 0.2553	TrainAcc 0.9456	ValidAcc 0.9457	TestAcc 0.9455	BestValid 0.9457
	Epoch 2525:	Loss 0.2541	TrainAcc 0.9459	ValidAcc 0.9460	TestAcc 0.9458	BestValid 0.9460
	Epoch 2550:	Loss 0.2536	TrainAcc 0.9459	ValidAcc 0.9451	TestAcc 0.9458	BestValid 0.9460
	Epoch 2575:	Loss 0.2536	TrainAcc 0.9460	ValidAcc 0.9462	TestAcc 0.9458	BestValid 0.9462
	Epoch 2600:	Loss 0.2539	TrainAcc 0.9461	ValidAcc 0.9459	TestAcc 0.9462	BestValid 0.9462
	Epoch 2625:	Loss 0.2523	TrainAcc 0.9463	ValidAcc 0.9462	TestAcc 0.9462	BestValid 0.9462
	Epoch 2650:	Loss 0.2520	TrainAcc 0.9463	ValidAcc 0.9462	TestAcc 0.9464	BestValid 0.9462
	Epoch 2675:	Loss 0.2510	TrainAcc 0.9463	ValidAcc 0.9459	TestAcc 0.9462	BestValid 0.9462
	Epoch 2700:	Loss 0.2517	TrainAcc 0.9465	ValidAcc 0.9464	TestAcc 0.9463	BestValid 0.9464
	Epoch 2725:	Loss 0.2505	TrainAcc 0.9467	ValidAcc 0.9461	TestAcc 0.9461	BestValid 0.9464
	Epoch 2750:	Loss 0.2508	TrainAcc 0.9467	ValidAcc 0.9463	TestAcc 0.9462	BestValid 0.9464
	Epoch 2775:	Loss 0.2501	TrainAcc 0.9468	ValidAcc 0.9460	TestAcc 0.9462	BestValid 0.9464
	Epoch 2800:	Loss 0.2491	TrainAcc 0.9469	ValidAcc 0.9461	TestAcc 0.9462	BestValid 0.9464
	Epoch 2825:	Loss 0.2490	TrainAcc 0.9469	ValidAcc 0.9468	TestAcc 0.9461	BestValid 0.9468
	Epoch 2850:	Loss 0.2488	TrainAcc 0.9467	ValidAcc 0.9462	TestAcc 0.9461	BestValid 0.9468
	Epoch 2875:	Loss 0.2492	TrainAcc 0.9471	ValidAcc 0.9461	TestAcc 0.9463	BestValid 0.9468
	Epoch 2900:	Loss 0.2484	TrainAcc 0.9472	ValidAcc 0.9463	TestAcc 0.9463	BestValid 0.9468
	Epoch 2925:	Loss 0.2490	TrainAcc 0.9472	ValidAcc 0.9468	TestAcc 0.9467	BestValid 0.9468
	Epoch 2950:	Loss 0.2471	TrainAcc 0.9473	ValidAcc 0.9466	TestAcc 0.9466	BestValid 0.9468
	Epoch 2975:	Loss 0.2467	TrainAcc 0.9473	ValidAcc 0.9463	TestAcc 0.9466	BestValid 0.9468
	Epoch 3000:	Loss 0.2473	TrainAcc 0.9474	ValidAcc 0.9465	TestAcc 0.9467	BestValid 0.9468
	Epoch 3025:	Loss 0.2456	TrainAcc 0.9474	ValidAcc 0.9465	TestAcc 0.9467	BestValid 0.9468
	Epoch 3050:	Loss 0.2463	TrainAcc 0.9476	ValidAcc 0.9460	TestAcc 0.9466	BestValid 0.9468
	Epoch 3075:	Loss 0.2451	TrainAcc 0.9474	ValidAcc 0.9462	TestAcc 0.9467	BestValid 0.9468
	Epoch 3100:	Loss 0.2448	TrainAcc 0.9477	ValidAcc 0.9465	TestAcc 0.9467	BestValid 0.9468
	Epoch 3125:	Loss 0.2449	TrainAcc 0.9479	ValidAcc 0.9465	TestAcc 0.9471	BestValid 0.9468
	Epoch 3150:	Loss 0.2445	TrainAcc 0.9476	ValidAcc 0.9467	TestAcc 0.9468	BestValid 0.9468
	Epoch 3175:	Loss 0.2444	TrainAcc 0.9477	ValidAcc 0.9463	TestAcc 0.9468	BestValid 0.9468
	Epoch 3200:	Loss 0.2433	TrainAcc 0.9480	ValidAcc 0.9466	TestAcc 0.9468	BestValid 0.9468
	Epoch 3225:	Loss 0.2439	TrainAcc 0.9479	ValidAcc 0.9469	TestAcc 0.9470	BestValid 0.9469
	Epoch 3250:	Loss 0.2431	TrainAcc 0.9481	ValidAcc 0.9465	TestAcc 0.9470	BestValid 0.9469
	Epoch 3275:	Loss 0.2432	TrainAcc 0.9482	ValidAcc 0.9469	TestAcc 0.9474	BestValid 0.9469
	Epoch 3300:	Loss 0.2428	TrainAcc 0.9483	ValidAcc 0.9466	TestAcc 0.9472	BestValid 0.9469
	Epoch 3325:	Loss 0.2425	TrainAcc 0.9483	ValidAcc 0.9465	TestAcc 0.9471	BestValid 0.9469
	Epoch 3350:	Loss 0.2416	TrainAcc 0.9484	ValidAcc 0.9465	TestAcc 0.9473	BestValid 0.9469
	Epoch 3375:	Loss 0.2414	TrainAcc 0.9483	ValidAcc 0.9469	TestAcc 0.9475	BestValid 0.9469
	Epoch 3400:	Loss 0.2411	TrainAcc 0.9484	ValidAcc 0.9468	TestAcc 0.9474	BestValid 0.9469
	Epoch 3425:	Loss 0.2410	TrainAcc 0.9484	ValidAcc 0.9467	TestAcc 0.9474	BestValid 0.9469
	Epoch 3450:	Loss 0.2403	TrainAcc 0.9484	ValidAcc 0.9467	TestAcc 0.9473	BestValid 0.9469
	Epoch 3475:	Loss 0.2399	TrainAcc 0.9483	ValidAcc 0.9470	TestAcc 0.9472	BestValid 0.9470
	Epoch 3500:	Loss 0.2401	TrainAcc 0.9485	ValidAcc 0.9470	TestAcc 0.9475	BestValid 0.9470
	Epoch 3525:	Loss 0.2404	TrainAcc 0.9486	ValidAcc 0.9468	TestAcc 0.9473	BestValid 0.9470
	Epoch 3550:	Loss 0.2401	TrainAcc 0.9487	ValidAcc 0.9467	TestAcc 0.9475	BestValid 0.9470
	Epoch 3575:	Loss 0.2385	TrainAcc 0.9489	ValidAcc 0.9472	TestAcc 0.9477	BestValid 0.9472
	Epoch 3600:	Loss 0.2384	TrainAcc 0.9486	ValidAcc 0.9467	TestAcc 0.9474	BestValid 0.9472
	Epoch 3625:	Loss 0.2379	TrainAcc 0.9490	ValidAcc 0.9473	TestAcc 0.9478	BestValid 0.9473
	Epoch 3650:	Loss 0.2377	TrainAcc 0.9489	ValidAcc 0.9473	TestAcc 0.9477	BestValid 0.9473
	Epoch 3675:	Loss 0.2377	TrainAcc 0.9488	ValidAcc 0.9471	TestAcc 0.9477	BestValid 0.9473
	Epoch 3700:	Loss 0.2372	TrainAcc 0.9489	ValidAcc 0.9470	TestAcc 0.9479	BestValid 0.9473
	Epoch 3725:	Loss 0.2366	TrainAcc 0.9490	ValidAcc 0.9472	TestAcc 0.9478	BestValid 0.9473
	Epoch 3750:	Loss 0.2365	TrainAcc 0.9492	ValidAcc 0.9472	TestAcc 0.9477	BestValid 0.9473
	Epoch 3775:	Loss 0.2362	TrainAcc 0.9489	ValidAcc 0.9470	TestAcc 0.9476	BestValid 0.9473
	Epoch 3800:	Loss 0.2365	TrainAcc 0.9494	ValidAcc 0.9473	TestAcc 0.9479	BestValid 0.9473
	Epoch 3825:	Loss 0.2365	TrainAcc 0.9493	ValidAcc 0.9475	TestAcc 0.9478	BestValid 0.9475
	Epoch 3850:	Loss 0.2350	TrainAcc 0.9493	ValidAcc 0.9476	TestAcc 0.9479	BestValid 0.9476
	Epoch 3875:	Loss 0.2354	TrainAcc 0.9494	ValidAcc 0.9474	TestAcc 0.9478	BestValid 0.9476
	Epoch 3900:	Loss 0.2348	TrainAcc 0.9494	ValidAcc 0.9473	TestAcc 0.9478	BestValid 0.9476
	Epoch 3925:	Loss 0.2352	TrainAcc 0.9494	ValidAcc 0.9472	TestAcc 0.9478	BestValid 0.9476
	Epoch 3950:	Loss 0.2350	TrainAcc 0.9494	ValidAcc 0.9473	TestAcc 0.9480	BestValid 0.9476
	Epoch 3975:	Loss 0.2344	TrainAcc 0.9495	ValidAcc 0.9472	TestAcc 0.9481	BestValid 0.9476
	Epoch 4000:	Loss 0.2342	TrainAcc 0.9497	ValidAcc 0.9472	TestAcc 0.9480	BestValid 0.9476
	Epoch 4025:	Loss 0.2347	TrainAcc 0.9498	ValidAcc 0.9476	TestAcc 0.9480	BestValid 0.9476
	Epoch 4050:	Loss 0.2335	TrainAcc 0.9497	ValidAcc 0.9473	TestAcc 0.9481	BestValid 0.9476
	Epoch 4075:	Loss 0.2331	TrainAcc 0.9497	ValidAcc 0.9478	TestAcc 0.9481	BestValid 0.9478
	Epoch 4100:	Loss 0.2333	TrainAcc 0.9497	ValidAcc 0.9474	TestAcc 0.9481	BestValid 0.9478
	Epoch 4125:	Loss 0.2336	TrainAcc 0.9498	ValidAcc 0.9478	TestAcc 0.9484	BestValid 0.9478
	Epoch 4150:	Loss 0.2332	TrainAcc 0.9496	ValidAcc 0.9472	TestAcc 0.9481	BestValid 0.9478
	Epoch 4175:	Loss 0.2325	TrainAcc 0.9498	ValidAcc 0.9478	TestAcc 0.9484	BestValid 0.9478
	Epoch 4200:	Loss 0.2321	TrainAcc 0.9500	ValidAcc 0.9480	TestAcc 0.9482	BestValid 0.9480
	Epoch 4225:	Loss 0.2320	TrainAcc 0.9499	ValidAcc 0.9479	TestAcc 0.9483	BestValid 0.9480
	Epoch 4250:	Loss 0.2314	TrainAcc 0.9501	ValidAcc 0.9481	TestAcc 0.9485	BestValid 0.9481
	Epoch 4275:	Loss 0.2309	TrainAcc 0.9498	ValidAcc 0.9478	TestAcc 0.9482	BestValid 0.9481
	Epoch 4300:	Loss 0.2311	TrainAcc 0.9501	ValidAcc 0.9480	TestAcc 0.9484	BestValid 0.9481
	Epoch 4325:	Loss 0.2317	TrainAcc 0.9502	ValidAcc 0.9480	TestAcc 0.9485	BestValid 0.9481
	Epoch 4350:	Loss 0.2308	TrainAcc 0.9501	ValidAcc 0.9473	TestAcc 0.9485	BestValid 0.9481
	Epoch 4375:	Loss 0.2305	TrainAcc 0.9501	ValidAcc 0.9475	TestAcc 0.9484	BestValid 0.9481
	Epoch 4400:	Loss 0.2294	TrainAcc 0.9503	ValidAcc 0.9474	TestAcc 0.9485	BestValid 0.9481
	Epoch 4425:	Loss 0.2305	TrainAcc 0.9504	ValidAcc 0.9478	TestAcc 0.9484	BestValid 0.9481
	Epoch 4450:	Loss 0.2301	TrainAcc 0.9503	ValidAcc 0.9479	TestAcc 0.9484	BestValid 0.9481
	Epoch 4475:	Loss 0.2288	TrainAcc 0.9503	ValidAcc 0.9476	TestAcc 0.9485	BestValid 0.9481
	Epoch 4500:	Loss 0.2297	TrainAcc 0.9505	ValidAcc 0.9483	TestAcc 0.9487	BestValid 0.9483
	Epoch 4525:	Loss 0.2288	TrainAcc 0.9507	ValidAcc 0.9480	TestAcc 0.9488	BestValid 0.9483
	Epoch 4550:	Loss 0.2292	TrainAcc 0.9504	ValidAcc 0.9480	TestAcc 0.9486	BestValid 0.9483
	Epoch 4575:	Loss 0.2291	TrainAcc 0.9505	ValidAcc 0.9478	TestAcc 0.9486	BestValid 0.9483
	Epoch 4600:	Loss 0.2286	TrainAcc 0.9506	ValidAcc 0.9479	TestAcc 0.9485	BestValid 0.9483
	Epoch 4625:	Loss 0.2277	TrainAcc 0.9504	ValidAcc 0.9483	TestAcc 0.9484	BestValid 0.9483
	Epoch 4650:	Loss 0.2282	TrainAcc 0.9508	ValidAcc 0.9478	TestAcc 0.9489	BestValid 0.9483
	Epoch 4675:	Loss 0.2282	TrainAcc 0.9508	ValidAcc 0.9477	TestAcc 0.9486	BestValid 0.9483
	Epoch 4700:	Loss 0.2283	TrainAcc 0.9507	ValidAcc 0.9478	TestAcc 0.9487	BestValid 0.9483
	Epoch 4725:	Loss 0.2278	TrainAcc 0.9510	ValidAcc 0.9480	TestAcc 0.9488	BestValid 0.9483
	Epoch 4750:	Loss 0.2269	TrainAcc 0.9508	ValidAcc 0.9481	TestAcc 0.9489	BestValid 0.9483
	Epoch 4775:	Loss 0.2272	TrainAcc 0.9508	ValidAcc 0.9480	TestAcc 0.9484	BestValid 0.9483
	Epoch 4800:	Loss 0.2273	TrainAcc 0.9508	ValidAcc 0.9477	TestAcc 0.9487	BestValid 0.9483
	Epoch 4825:	Loss 0.2266	TrainAcc 0.9510	ValidAcc 0.9483	TestAcc 0.9490	BestValid 0.9483
	Epoch 4850:	Loss 0.2264	TrainAcc 0.9507	ValidAcc 0.9478	TestAcc 0.9487	BestValid 0.9483
	Epoch 4875:	Loss 0.2264	TrainAcc 0.9509	ValidAcc 0.9481	TestAcc 0.9490	BestValid 0.9483
	Epoch 4900:	Loss 0.2269	TrainAcc 0.9511	ValidAcc 0.9481	TestAcc 0.9488	BestValid 0.9483
	Epoch 4925:	Loss 0.2263	TrainAcc 0.9510	ValidAcc 0.9482	TestAcc 0.9491	BestValid 0.9483
	Epoch 4950:	Loss 0.2256	TrainAcc 0.9509	ValidAcc 0.9479	TestAcc 0.9488	BestValid 0.9483
	Epoch 4975:	Loss 0.2250	TrainAcc 0.9505	ValidAcc 0.9483	TestAcc 0.9485	BestValid 0.9483
Node 5, Pre/Post-Pipelining: 8.578 / 17.419 ms, Bubble: 0.417 ms, Compute: 938.810 ms, Comm: 0.009 ms, Imbalance: 0.018 ms
	Epoch 5000:	Loss 0.2250	TrainAcc 0.9513	ValidAcc 0.9480	TestAcc 0.9491	BestValid 0.9483
Node 6, Pre/Post-Pipelining: 8.581 / 17.445 ms, Bubble: 0.139 ms, Compute: 939.058 ms, Comm: 0.011 ms, Imbalance: 0.018 ms
Node 0, Pre/Post-Pipelining: 8.578 / 17.413 ms, Bubble: 0.433 ms, Compute: 938.802 ms, Comm: 0.010 ms, Imbalance: 0.018 ms
Node 7, Pre/Post-Pipelining: 8.581 / 17.437 ms, Bubble: 0.151 ms, Compute: 939.048 ms, Comm: 0.010 ms, Imbalance: 0.018 ms
Node 1, Pre/Post-Pipelining: 8.577 / 17.409 ms, Bubble: 0.503 ms, Compute: 938.738 ms, Comm: 0.008 ms, Imbalance: 0.017 ms
Node 4, Pre/Post-Pipelining: 8.582 / 17.443 ms, Bubble: 0.348 ms, Compute: 938.845 ms, Comm: 0.011 ms, Imbalance: 0.019 ms
Node 2, Pre/Post-Pipelining: 8.581 / 17.411 ms, Bubble: 0.065 ms, Compute: 939.180 ms, Comm: 0.008 ms, Imbalance: 0.016 ms
Node 3, Pre/Post-Pipelining: 8.576 / 17.821 ms, Bubble: 0.056 ms, Compute: 938.776 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 8.578 ms
Cluster-Wide Average, Post-Pipelining Overhead: 17.413 ms
Cluster-Wide Average, Bubble: 0.433 ms
Cluster-Wide Average, Compute: 938.802 ms
Cluster-Wide Average, Communication: 0.010 ms
Cluster-Wide Average, Imbalance: 0.018 ms
Node 4, GPU memory consumption: 16.247 GB
Node 0, GPU memory consumption: 16.756 GB
Node 7, GPU memory consumption: 15.993 GB
Node 2, GPU memory consumption: 16.016 GB
Node 6, GPU memory consumption: 16.018 GB
Node 3, GPU memory consumption: 15.864 GB
Node 5, GPU memory consumption: 15.887 GB
Node 1, GPU memory consumption: 15.887 GB
Node 4, Graph-Level Communication Throughput: 37.831 Gbps, Time: 676.902 ms
Node 0, Graph-Level Communication Throughput: 47.195 Gbps, Time: 685.047 ms
Node 5, Graph-Level Communication Throughput: 37.576 Gbps, Time: 676.324 ms
Node 1, Graph-Level Communication Throughput: 31.965 Gbps, Time: 683.475 ms
Node 6, Graph-Level Communication Throughput: 48.302 Gbps, Time: 647.776 ms
Node 2, Graph-Level Communication Throughput: 49.682 Gbps, Time: 668.301 ms
Node 7, Graph-Level Communication Throughput: 41.015 Gbps, Time: 666.643 ms
Node 3, Graph-Level Communication Throughput: 40.711 Gbps, Time: 668.128 ms
------------------------node id 4,  per-epoch time: 1.348324 s---------------
------------------------node id 0,  per-epoch time: 1.348324 s---------------
------------------------node id 5,  per-epoch time: 1.348324 s---------------
------------------------node id 1,  per-epoch time: 1.348324 s---------------
------------------------node id 6,  per-epoch time: 1.348324 s---------------
------------------------node id 2,  per-epoch time: 1.348324 s---------------
------------------------node id 7,  per-epoch time: 1.348324 s---------------
------------------------node id 3,  per-epoch time: 1.348324 s---------------
************ Profiling Results ************
	Bubble: 409.315321 (ms) (30.37 percentage)
	Compute: 217.912491 (ms) (16.17 percentage)
	GraphCommComputeOverhead: 31.832532 (ms) (2.36 percentage)
	GraphCommNetwork: 671.572718 (ms) (49.82 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 17.329002 (ms) (1.29 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 26.104 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.011 GB
	Total communication (cluster-wide, per-epoch): 26.115 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.9483
Target test_acc: 0.9487
Epoch to reach the target acc: 4499
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
