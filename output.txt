g001.anvil.rcac.purdue.edu
Fri May 26 15:47:46 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:41:00.0 Off |                    0 |
| N/A   29C    P0    50W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2    9) numactl/2.0.14
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0    10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0   11) openmpi/4.0.6
  4) gmp/6.2.1              8) zlib/1.2.11  12) boost/1.74.0

 

[ 11%] Built target context
[ 36%] Built target core
[ 77%] Built target cudahelp
[ 83%] Built target estimate_comm_volume
[ 88%] Built target OSDI2023_MULTI_NODES_gcnii
[ 97%] Built target OSDI2023_MULTI_NODES_graphsage
[100%] Built target OSDI2023_MULTI_NODES_gcn
Initialized node 1 on machine g009.anvil.rcac.purdue.edu
Initialized node 0 on machine g001.anvil.rcac.purdue.edu
Building the CSR structure...
Building the CSR structure...
        It takes 1.967 seconds.
Building the CSC structure...
        It takes 1.986 seconds.
Building the CSC structure...
        It takes 1.730 seconds.
        It takes 1.752 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.403 seconds.
Building the Label Vector...
        It takes 0.068 seconds.
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/reddit
The number of GCNII layers: 4
The number of hidden units: 256
The number of training epoches: 200
Learning rate: 0.010000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 2
        It takes 0.439 seconds.
Building the Label Vector...
        It takes 0.035 seconds.
train nodes 153431, valid nodes 23831, test nodes 55703
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
*** Node 0, starting model training...
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 18)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_DROPOUT, output tensors: 1
    Op 2: type OPERATOR_WEIGHT, output tensors: 2
    Op 3: type OPERATOR_MATMUL, output tensors: 3
    Op 4: type OPERATOR_WEIGHT, output tensors: 4
    Op 5: type OPERATOR_MATMUL, output tensors: 5
    Op 6: type OPERATOR_AGGREGATION, output tensors: 6
    Op 7: type OPERATOR_ADD, output tensors: 7
    Op 8: type OPERATOR_RELU, output tensors: 8
    Op 9: type OPERATOR_DROPOUT, output tensors: 9
    Op 10: type OPERATOR_WEIGHT, output tensors: 10
    Op 11: type OPERATOR_MATMUL, output tensors: 11
    Op 12: type OPERATOR_AGGREGATION, output tensors: 12
    Op 13: type OPERATOR_WEIGHT, output tensors: 13
    Op 14: type OPERATOR_MATMUL, output tensors: 14
    Op 15: type OPERATOR_ADD, output tensors: 15
    Op 16: type OPERATOR_RELU, output tensors: 16
    Op 17: type OPERATOR_DROPOUT, output tensors: 17
    Op 18: type OPERATOR_WEIGHT, output tensors: 18
    Op 19: type OPERATOR_MATMUL, output tensors: 19
    Op 20: type OPERATOR_AGGREGATION, output tensors: 20
    Op 21: type OPERATOR_WEIGHT, output tensors: 21
    Op 22: type OPERATOR_MATMUL, output tensors: 22
    Op 23: type OPERATOR_ADD, output tensors: 23
    Op 24: type OPERATOR_RELU, output tensors: 24
    Op 25: type OPERATOR_DROPOUT, output tensors: 25
    Op 26: type OPERATOR_WEIGHT, output tensors: 26
    Op 27: type OPERATOR_MATMUL, output tensors: 27
    Op 28: type OPERATOR_AGGREGATION, output tensors: 28
    Op 29: type OPERATOR_WEIGHT, output tensors: 29
    Op 30: type OPERATOR_MATMUL, output tensors: 30
    Op 31: type OPERATOR_ADD, output tensors: 31
    Op 32: type OPERATOR_SOFTMAX, output tensors: 32
Boundaries: 0 0 232965 232965
Fragments: [0, 232965)
Chunks (number of global chunks: 8): 0-[0, 29121) 1-[29121, 58242) 2-[58242, 87363) 3-[87363, 116484) 4-[116484, 145605) 5-[145605, 174726) 6-[174726, 203847) 7-[203847, 232965)
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
*** Node 1, starting model training...
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [18, 33)
WARNING: the current version only applies to linear GNN models!
*** Node 1, constructing the helper classes...
232965, 114848857, 114848857
Number of vertices per chunk: 29121
232965, 114848857, 114848857
Number of vertices per chunk: 29121
csr in-out ready !*** Node 0, setting up some other necessary information...
csr in-out ready !*** Node 1, setting up some other necessary information...
*** Node 1, starting the helper threads...
+++++++++ Node 1 initializing the weights for op[18, 33)...
+++++++++ Node 1, mapping weight op 18
+++++++++ Node 1, mapping weight op 21
+++++++++ Node 1, mapping weight op 26
+++++++++ Node 1, mapping weight op 29
*** Node 0, starting the helper threads...
+++++++++ Node 0 initializing the weights for op[0, 18)...
+++++++++ Node 0, mapping weight op 2
+++++++++ Node 0, mapping weight op 4
+++++++++ Node 0, mapping weight op 10
+++++++++ Node 0, mapping weight op 13
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.010000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.010000000
	Epoch 10:	Loss 4.5256
	Epoch 20:	Loss 2.4284
	Epoch 30:	Loss 2.0196
	Epoch 40:	Loss 1.5750
	Epoch 50:	Loss 1.2865
	Epoch 60:	Loss 1.0429
	Epoch 70:	Loss 0.8636
	Epoch 80:	Loss 0.6777
	Epoch 90:	Loss 0.8076
	Epoch 100:	Loss 0.7528
	Epoch 110:	Loss 0.6601
	Epoch 120:	Loss 0.5257
	Epoch 130:	Loss 0.4525
	Epoch 140:	Loss 0.4285
	Epoch 150:	Loss 0.4178
	Epoch 160:	Loss 0.4017
	Epoch 170:	Loss 0.3901
	Epoch 180:	Loss 0.3775
	Epoch 190:	Loss 0.3709
Node 0, Layer-level comm throughput (act): -nan GBps
Node 1, Layer-level comm throughput (act): 9.586 GBps
Node 1, Layer-level comm throughput (grad): -nan GBps
Node 0, Layer-level comm throughput (grad): 9.844 GBps
	Epoch 200:	Loss 0.3632
Node 0, GPU memory consumption: 6.341 GB
Node 0, compression time: 0.198s, compression size: 44.435GB, throughput: 224.170GBps
Node 0, decompression time: 0.130s, compression size: 44.435GB, throughput: 341.692GBps
Node 0, pure compute time: 45.205 s, total compute time: 45.534 s
Node 0, wait_for_task_time: 6.443 s, wait_for_other_gpus_time: 0.002 s
------------------------node id 0,  per-epoch time: 0.266589 s---------------
Node 1, GPU memory consumption: 5.101 GB
Node 1, compression time: 0.300s, compression size: 44.435GB, throughput: 148.082GBps
Node 1, decompression time: 0.128s, compression size: 44.435GB, throughput: 347.660GBps
Node 1, pure compute time: 40.655 s, total compute time: 41.083 s
Node 1, wait_for_task_time: 3.874 s, wait_for_other_gpus_time: 0.013 s
------------------------node id 1,  per-epoch time: 0.266587 s---------------
************ Profiling Results ************
	Bubble: 7.858838 (s) (13.99 percentage)
	Compute: 44.215614 (s) (78.72 percentage)
	GradSync: 0.518751 (s) (0.92 percentage)
	GraphComm: 0.011999 (s) (0.02 percentage)
	Imbalance: 3.500627 (s) (6.23 percentage)
	LayerComm: 0.061698 (s) (0.11 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.017 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.004 GB
	Total communication (cluster-wide, per-epoch): 0.021 GB
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 1] Success 
[MPI Rank 0] Success 
