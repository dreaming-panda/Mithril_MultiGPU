g012.anvil.rcac.purdue.edu
Tue Apr 18 14:00:49 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:C1:00.0 Off |                    0 |
| N/A   28C    P0    54W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2       9) zlib/1.2.11
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0       10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0      11) openmpi/4.0.6
  4) gmp/6.2.1              8) numactl/2.0.14  12) boost/1.74.0

 

Consolidate compiler generated dependencies of target context
Consolidate compiler generated dependencies of target parallel
Consolidate compiler generated dependencies of target core
Consolidate compiler generated dependencies of target cudahelp
[  4%] Built target context
[ 15%] Built target parallel
[ 20%] Built target core
[ 21%] Building CXX object CMakeFiles/cudahelp.dir/core/src/cuda/cuda_executor.cc.o
[ 22%] Linking CXX static library libcudahelp.a
[ 40%] Built target cudahelp
Consolidate compiler generated dependencies of target test_mpi_gpu_hybrid
Consolidate compiler generated dependencies of target OSDI2023_SINGLE_NODE_gcn
Consolidate compiler generated dependencies of target test_mpi_gpu_pipelined_model_parallel
Consolidate compiler generated dependencies of target test_cuda_graph
Consolidate compiler generated dependencies of target OSDI2023_MULTI_NODES_gcn
Consolidate compiler generated dependencies of target test_trivial
Consolidate compiler generated dependencies of target test_cuda_model_parallel
Consolidate compiler generated dependencies of target test_hello_world
Consolidate compiler generated dependencies of target test_cuda_pipeline_parallel
Consolidate compiler generated dependencies of target test_mpi_loader
Consolidate compiler generated dependencies of target test_mpi_gpu_model_parallel
Consolidate compiler generated dependencies of target test_mpi_structual_graph
Consolidate compiler generated dependencies of target test_two_layer_hybrid_parallelism_designer
Consolidate compiler generated dependencies of target test_full_structual_graph
Consolidate compiler generated dependencies of target test_single_node_fullgpu_training
Consolidate compiler generated dependencies of target test_cuda
Consolidate compiler generated dependencies of target test_graph
Consolidate compiler generated dependencies of target OSDI2023_SINGLE_NODE_gcn_inference
Consolidate compiler generated dependencies of target test_nccl_thread
Consolidate compiler generated dependencies of target test_nccl_mpi
Consolidate compiler generated dependencies of target test_mpi_combined
Consolidate compiler generated dependencies of target test_single_node_gpu_training
Consolidate compiler generated dependencies of target test_mpi_non_structual_graph
Consolidate compiler generated dependencies of target test_full_non_structual_graph
Consolidate compiler generated dependencies of target test_single_node_training
Consolidate compiler generated dependencies of target estimate_comm_volume
Consolidate compiler generated dependencies of target test_mpi_model_parallel
Consolidate compiler generated dependencies of target test_mpi_pipelined_model_parallel
[ 44%] Linking CXX executable test_cuda
[ 44%] Linking CXX executable test_hello_world
[ 44%] Linking CXX executable test_full_structual_graph
[ 50%] Linking CXX executable test_graph
[ 50%] Linking CXX executable test_cuda_graph
[ 50%] Linking CXX executable test_mpi_structual_graph
[ 50%] Linking CXX executable test_mpi_loader
[ 57%] Linking CXX executable test_mpi_pipelined_model_parallel
[ 58%] Linking CXX executable test_cuda_model_parallel
[ 58%] Linking CXX executable test_mpi_non_structual_graph
[ 58%] Linking CXX executable estimate_comm_volume
[ 63%] Linking CXX executable test_single_node_fullgpu_training
[ 58%] Linking CXX executable test_nccl_thread
[ 65%] Linking CXX executable test_mpi_gpu_pipelined_model_parallel
[ 65%] Linking CXX executable test_mpi_gpu_hybrid
[ 65%] Linking CXX executable test_full_non_structual_graph
[ 65%] Linking CXX executable test_trivial
[ 65%] Linking CXX executable test_mpi_model_parallel
[ 58%] Linking CXX executable test_mpi_combined
[ 65%] Linking CXX executable test_cuda_pipeline_parallel
[ 65%] Linking CXX executable test_single_node_gpu_training
[ 65%] Linking CXX executable test_mpi_gpu_model_parallel
[ 58%] Linking CXX executable test_nccl_mpi
[ 65%] Linking CXX executable test_single_node_training
[ 67%] Linking CXX executable gcn_inference
[ 70%] Linking CXX executable gcn
[ 70%] Linking CXX executable test_two_layer_hybrid_parallelism_designer
[ 70%] Linking CXX executable gcn
[ 71%] Built target test_mpi_loader
[ 72%] Built target test_graph
[ 73%] Built target test_trivial
[ 74%] Built target test_mpi_combined
[ 75%] Built target test_mpi_structual_graph
[ 76%] Built target test_mpi_model_parallel
[ 79%] Built target test_full_structual_graph
[ 79%] Built target estimate_comm_volume
[ 81%] Built target test_full_non_structual_graph
[ 82%] Built target test_cuda_graph
[ 82%] Built target test_cuda
[ 82%] Built target test_hello_world
[ 84%] Built target test_mpi_non_structual_graph
[ 85%] Built target test_two_layer_hybrid_parallelism_designer
[ 86%] Built target test_single_node_training
[ 87%] Built target test_single_node_gpu_training
[ 88%] Built target test_mpi_pipelined_model_parallel
[ 89%] Built target test_mpi_gpu_model_parallel
[ 90%] Built target test_single_node_fullgpu_training
[ 92%] Built target test_mpi_gpu_hybrid
[ 92%] Built target test_mpi_gpu_pipelined_model_parallel
[ 93%] Built target OSDI2023_SINGLE_NODE_gcn_inference
[ 94%] Built target OSDI2023_SINGLE_NODE_gcn
[ 95%] Built target test_cuda_pipeline_parallel
[ 96%] Built target OSDI2023_MULTI_NODES_gcn
[ 97%] Built target test_nccl_thread
[ 98%] Built target test_nccl_mpi
[100%] Built target test_cuda_model_parallel
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 16
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.003000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 3
The scaling down factor of out-of-chunk gradients: 0.100000
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 16
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.003000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 3
The scaling down factor of out-of-chunk gradients: 0.100000
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 16
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.003000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 3
The scaling down factor of out-of-chunk gradients: 0.100000
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 16
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.003000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 3
The scaling down factor of out-of-chunk gradients: 0.100000
Initialized node 3 on machine g015.anvil.rcac.purdue.edu
Initialized node 0 on machine g012.anvil.rcac.purdue.edu
Initialized node 2 on machine g014.anvil.rcac.purdue.edu
Initialized node 1 on machine g013.anvil.rcac.purdue.edu
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.974 seconds.
Building the CSC structure...
        It takes 1.972 seconds.
Building the CSC structure...
        It takes 1.962 seconds.
Building the CSC structure...
        It takes 2.038 seconds.
Building the CSC structure...
        It takes 1.880 seconds.
        It takes 1.882 seconds.
        It takes 1.884 seconds.
        It takes 1.957 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.928 seconds.
Building the Label Vector...
        It takes 0.798 seconds.
        It takes 0.937 seconds.
        It takes 0.939 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
        It takes 0.403 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
        It takes 0.404 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
        It takes 0.407 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
        It takes 0.409 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
WARNING: the current version only applies to linear GNN models!
*** Node 3, starting model training...
Number of operators: 80
0 2449029 0 21
0 2449029 21 41
0 2449029 41 61
0 2449029 61 80
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the partition [61, 80) x [0, 2449029)
*** Node 3, constructing the helper classes...
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
WARNING: the current version only applies to linear GNN models!
*** Node 1, starting model training...
Number of operators: 80
0 2449029 0 21
0 2449029 21 41
0 2449029 41 61
0 2449029 61 80
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the partition [21, 41) x [0, 2449029)
*** Node 1, constructing the helper classes...
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
*** Node 2, starting model training...
Number of operators: 80
0 2449029 0 21
0 2449029 21 41
0 2449029 41 61
0 2449029 61 80
WARNING: the current version only applies to linear GNN models!
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the partition [41, 61) x [0, 2449029)
*** Node 2, constructing the helper classes...
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
WARNING: the current version only applies to linear GNN models!
*** Node 0, starting model training...
Number of operators: 80
0 2449029 0 21
0 2449029 21 41
0 2449029 41 61
0 2449029 61 80
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the partition [0, 21) x [0, 2449029)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_WEIGHT, output tensors: 1
    Op 2: type OPERATOR_MATMUL, output tensors: 2
    Op 3: type OPERATOR_AGGREGATION, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_AGGREGATION, output tensors: 6
    Op 7: type OPERATOR_WEIGHT, output tensors: 7
    Op 8: type OPERATOR_MATMUL, output tensors: 8
    Op 9: type OPERATOR_RELU, output tensors: 9
    Op 10: type OPERATOR_DROPOUT, output tensors: 10
    Op 11: type OPERATOR_AGGREGATION, output tensors: 11
    Op 12: type OPERATOR_WEIGHT, output tensors: 12
    Op 13: type OPERATOR_MATMUL, output tensors: 13
    Op 14: type OPERATOR_RELU, output tensors: 14
    Op 15: type OPERATOR_DROPOUT, output tensors: 15
    Op 16: type OPERATOR_AGGREGATION, output tensors: 16
    Op 17: type OPERATOR_WEIGHT, output tensors: 17
    Op 18: type OPERATOR_MATMUL, output tensors: 18
    Op 19: type OPERATOR_RELU, output tensors: 19
    Op 20: type OPERATOR_DROPOUT, output tensors: 20
    Op 21: type OPERATOR_AGGREGATION, output tensors: 21
    Op 22: type OPERATOR_WEIGHT, output tensors: 22
    Op 23: type OPERATOR_MATMUL, output tensors: 23
    Op 24: type OPERATOR_RELU, output tensors: 24
    Op 25: type OPERATOR_DROPOUT, output tensors: 25
    Op 26: type OPERATOR_AGGREGATION, output tensors: 26
    Op 27: type OPERATOR_WEIGHT, output tensors: 27
    Op 28: type OPERATOR_MATMUL, output tensors: 28
    Op 29: type OPERATOR_RELU, output tensors: 29
    Op 30: type OPERATOR_DROPOUT, output tensors: 30
    Op 31: type OPERATOR_AGGREGATION, output tensors: 31
    Op 32: type OPERATOR_WEIGHT, output tensors: 32
    Op 33: type OPERATOR_MATMUL, output tensors: 33
    Op 34: type OPERATOR_RELU, output tensors: 34
    Op 35: type OPERATOR_DROPOUT, output tensors: 35
    Op 36: type OPERATOR_AGGREGATION, output tensors: 36
    Op 37: type OPERATOR_WEIGHT, output tensors: 37
    Op 38: type OPERATOR_MATMUL, output tensors: 38
    Op 39: type OPERATOR_RELU, output tensors: 39
    Op 40: type OPERATOR_DROPOUT, output tensors: 40
    Op 41: type OPERATOR_AGGREGATION, output tensors: 41
    Op 42: type OPERATOR_WEIGHT, output tensors: 42
    Op 43: type OPERATOR_MATMUL, output tensors: 43
    Op 44: type OPERATOR_RELU, output tensors: 44
    Op 45: type OPERATOR_DROPOUT, output tensors: 45
    Op 46: type OPERATOR_AGGREGATION, output tensors: 46
    Op 47: type OPERATOR_WEIGHT, output tensors: 47
    Op 48: type OPERATOR_MATMUL, output tensors: 48
    Op 49: type OPERATOR_RELU, output tensors: 49
    Op 50: type OPERATOR_DROPOUT, output tensors: 50
    Op 51: type OPERATOR_AGGREGATION, output tensors: 51
    Op 52: type OPERATOR_WEIGHT, output tensors: 52
    Op 53: type OPERATOR_MATMUL, output tensors: 53
    Op 54: type OPERATOR_RELU, output tensors: 54
    Op 55: type OPERATOR_DROPOUT, output tensors: 55
    Op 56: type OPERATOR_AGGREGATION, output tensors: 56
    Op 57: type OPERATOR_WEIGHT, output tensors: 57
    Op 58: type OPERATOR_MATMUL, output tensors: 58
    Op 59: type OPERATOR_RELU, output tensors: 59
    Op 60: type OPERATOR_DROPOUT, output tensors: 60
    Op 61: type OPERATOR_AGGREGATION, output tensors: 61
    Op 62: type OPERATOR_WEIGHT, output tensors: 62
    Op 63: type OPERATOR_MATMUL, output tensors: 63
    Op 64: type OPERATOR_RELU, output tensors: 64
    Op 65: type OPERATOR_DROPOUT, output tensors: 65
    Op 66: type OPERATOR_AGGREGATION, output tensors: 66
    Op 67: type OPERATOR_WEIGHT, output tensors: 67
    Op 68: type OPERATOR_MATMUL, output tensors: 68
    Op 69: type OPERATOR_RELU, output tensors: 69
    Op 70: type OPERATOR_DROPOUT, output tensors: 70
    Op 71: type OPERATOR_AGGREGATION, output tensors: 71
    Op 72: type OPERATOR_WEIGHT, output tensors: 72
    Op 73: type OPERATOR_MATMUL, output tensors: 73
    Op 74: type OPERATOR_RELU, output tensors: 74
    Op 75: type OPERATOR_DROPOUT, output tensors: 75
    Op 76: type OPERATOR_AGGREGATION, output tensors: 76
    Op 77: type OPERATOR_WEIGHT, output tensors: 77
    Op 78: type OPERATOR_MATMUL, output tensors: 78
    Op 79: type OPERATOR_SOFTMAX, output tensors: 79
(Forwarding) Node 3 (fragment 0) depends on nodes: 2 (Tensor: 60)
(Backwarding) Node 3 (fragment 0) depends on nodes:
(I-link dependencies): node 3 should send activation to nodes:
(I-link dependencies): node 3 should receive activation from nodes: 2 (tensor: 60)
(I-link dependencies): node 3 should send gradient to nodes:
(I-link dependencies): node 3 should receive gradient from nodes:
(Forwarding) Node 1 (fragment 0) depends on nodes: 0 (Tensor: 20)
(Backwarding) Node 1 (fragment 0) depends on nodes: 2 (Tensor: 40)
(I-link dependencies): node 1 should send activation to nodes: 2 (tensor: 40)
(I-link dependencies): node 1 should receive activation from nodes: 0 (tensor: 20)
(I-link dependencies): node 1 should send gradient to nodes:
(I-link dependencies): node 1 should receive gradient from nodes:
(Forwarding) Node 2 (fragment 0) depends on nodes: 1 (Tensor: 40)
(Backwarding) Node 2 (fragment 0) depends on nodes: 3 (Tensor: 60)
(I-link dependencies): node 2 should send activation to nodes: 3 (tensor: 60)
(I-link dependencies): node 2 should receive activation from nodes: 1 (tensor: 40)
(I-link dependencies): node 2 should send gradient to nodes:
(I-link dependencies): node 2 should receive gradient from nodes:
Boundaries: 0 0 0 0 2449029 2449029 2449029 2449029
Fragments: [0, 2449029)
Chunks (number of global chunks: 24): 0-[0, 102043) 1-[102043, 204086) 2-[204086, 306129) 3-[306129, 408172) 4-[408172, 510215) 5-[510215, 612258) 6-[612258, 714301) 7-[714301, 816344) 8-[816344, 918387) ... 23-[2346989, 2449029)
(Forwarding) Node 0 (fragment 0) depends on nodes:
(Backwarding) Node 0 (fragment 0) depends on nodes: 1 (Tensor: 20)
(I-link dependencies): node 0 should send activation to nodes: 1 (tensor: 20)
(I-link dependencies): node 0 should receive activation from nodes:
(I-link dependencies): node 0 should send gradient to nodes:
(I-link dependencies): node 0 should receive gradient from nodes:
2449029, 126167053, 126167053
Number of vertices per chunk: 102043
2449029, 126167053, 126167053
Number of vertices per chunk: 102043
2449029, 126167053, 126167053
Number of vertices per chunk: 102043
2449029, 126167053, 126167053
Number of vertices per chunk: 102043
csr in-out ready !*** Node 0, setting up some other necessary information...
csr in-out ready !*** Node 3, setting up some other necessary information...
csr in-out ready !*** Node 2, setting up some other necessary information...
csr in-out ready !*** Node 1, setting up some other necessary information...
*** Node 3, starting the helper threads...
*** Node 0, starting the helper threads...
*** Node 2, starting the helper threads...
*** Node 1, starting the helper threads...
+++++++++ Node 3 initializing the weights for op[61, 80)...
+++++++++ Node 3, mapping weight op 62
+++++++++ Node 0 initializing the weights for op[0, 21)...
+++++++++ Node 0, mapping weight op 1
+++++++++ Node 3, mapping weight op 67
+++++++++ Node 0, mapping weight op 7
+++++++++ Node 3, mapping weight op 72
+++++++++ Node 0, mapping weight op 12
+++++++++ Node 3, mapping weight op 77
+++++++++ Node 0, mapping weight op 17
+++++++++ Node 2 initializing the weights for op[41, 61)...
+++++++++ Node 2, mapping weight op 42
+++++++++ Node 1 initializing the weights for op[21, 41)...
+++++++++ Node 1, mapping weight op 22
+++++++++ Node 2, mapping weight op 47
+++++++++ Node 1, mapping weight op 27
+++++++++ Node 2, mapping weight op 52
+++++++++ Node 1, mapping weight op 32
+++++++++ Node 2, mapping weight op 57
+++++++++ Node 1, mapping weight op 37
RANDOMLY DISPATCH THE CHUNKS...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.003000000
*** Node 3, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.003000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.003000000
The learning rate specified by the user: 0.003000000
    Epoch 9:	Loss 3.32452	TrainAcc 0.2651	ValidAcc 0.2470	TestAcc 0.1839
    Epoch 19:	Loss 2.76582	TrainAcc 0.1584	ValidAcc 0.1540	TestAcc 0.1272
    Epoch 29:	Loss 2.47184	TrainAcc 0.3083	ValidAcc 0.3082	TestAcc 0.2683
    Epoch 39:	Loss 2.31751	TrainAcc 0.3078	ValidAcc 0.3050	TestAcc 0.2506
    Epoch 49:	Loss 2.20809	TrainAcc 0.3080	ValidAcc 0.3063	TestAcc 0.2583
    Epoch 59:	Loss 2.16735	TrainAcc 0.3072	ValidAcc 0.3023	TestAcc 0.2444
    Epoch 69:	Loss 2.16042	TrainAcc 0.3074	ValidAcc 0.3024	TestAcc 0.2451
    Epoch 79:	Loss 2.13023	TrainAcc 0.3076	ValidAcc 0.3038	TestAcc 0.2499
    Epoch 89:	Loss 2.09443	TrainAcc 0.3053	ValidAcc 0.2967	TestAcc 0.2290
Node 0, Layer-level comm throughput (act): -nan GBps
Node 1, Layer-level comm throughput (act): 10.902 GBps
Node 2, Layer-level comm throughput (act): 10.986 GBps
Node 3, Layer-level comm throughput (act): 11.253 GBps
Node 3, Layer-level comm throughput (grad): -nan GBps
Node 2, Layer-level comm throughput (grad): 11.290 GBps
Node 1, Layer-level comm throughput (grad): 10.977 GBps
Node 0, Layer-level comm throughput (grad): 10.923 GBps
    Epoch 99:	Loss 1.95183	TrainAcc 0.3800	ValidAcc 0.3731	TestAcc 0.2810
Node 0, GPU memory consumption: 33.117 GB
Node 0, compression time: 0.438s, compression size: 116.779GB, throughput: 266.910GBps
Node 0, decompression time: 0.922s, compression size: 116.779GB, throughput: 126.716GBps
Node 0, pure compute time: 26.122 s, total compute time: 27.481 s
Node 0, wait_for_task_time: 8.889 s, wait_for_other_gpus_time: 0.018 s
------------------------node id 0,  per-epoch time: 0.384869 s---------------
Node 1, GPU memory consumption: 33.394 GB
Node 1, compression time: 1.138s, compression size: 233.558GB, throughput: 205.180GBps
Node 1, decompression time: 4.116s, compression size: 233.558GB, throughput: 56.749GBps
Node 3, GPU memory consumption: 32.419 GB
Node 3, compression time: 0.717s, compression size: 116.779GB, throughput: 162.972GBps
Node 3, decompression time: 1.094s, compression size: 116.779GB, throughput: 106.758GBps
Node 3, pure compute time: 24.658 s, total compute time: 26.469 s
Node 3, wait_for_task_time: 4.255 s, wait_for_other_gpus_time: 0.001 s
------------------------node id 3,  per-epoch time: 0.384869 s---------------
Node 2, GPU memory consumption: 33.394 GB
Node 2, compression time: 1.148s, compression size: 233.558GB, throughput: 203.475GBps
Node 2, decompression time: 4.255s, compression size: 233.558GB, throughput: 54.894GBps
Node 2, pure compute time: 25.068 s, total compute time: 30.471 s
Node 2, wait_for_task_time: 3.941 s, wait_for_other_gpus_time: 0.016 s
------------------------node id 2,  per-epoch time: 0.384869 s---------------
Node 1, pure compute time: 25.023 s, total compute time: 30.277 s
Node 1, wait_for_task_time: 5.151 s, wait_for_other_gpus_time: 0.015 s
------------------------node id 1,  per-epoch time: 0.384869 s---------------
************ Profiling Results ************
	Bubble: 4.894784 (s) (12.54 percentage)
	Compute: 30.931700 (s) (79.26 percentage)
	GradSync: 0.463297 (s) (1.19 percentage)
	GraphComm: 0.016173 (s) (0.04 percentage)
	Imbalance: 1.859973 (s) (4.77 percentage)
	LayerComm: 0.859241 (s) (2.20 percentage)
	Layer-level communication (cluster-wide, per epoch): 1.319 GB
Highest valid_acc: 0.3731
Target test_acc: 0.2810
Epoch to reach the target acc: 100
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 0] Success 
