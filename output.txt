Fri Sep 15 13:34:01 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   28C    P8    16W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   29C    P8    16W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   29C    P8    17W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   30C    P8    17W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
gnerv4
gnerv4
gnerv4
gnerv4
[ 23%] Built target context
[ 34%] Built target core
[ 73%] Built target cudahelp
[ 89%] Built target OSDI2023_MULTI_NODES_resgcn
[ 89%] Built target OSDI2023_MULTI_NODES_graphsage
[ 89%] Built target OSDI2023_MULTI_NODES_gcnii
[100%] Built target estimate_comm_volume
[100%] Built target OSDI2023_MULTI_NODES_gcn
Running experiments...
Initializing the runtime environment
DONE MPI INIT
Initialized node 0 on machine gnerv4
Building the CSR structure...
        It takes 2.484 seconds.
Building the CSC structure...
        It takes 2.350 seconds.
Building the Feature Vector...
        It takes 0.244 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/reddit/2_parts
The number of GCNII layers: 12
The number of hidden units: 128
The number of training epoches: 1500
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 1
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 12)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 2): 0-[0, 116483) 1-[116483, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 116483
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 600526.760 Gbps (per GPU), 600526.760 Gbps (aggregated)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0103.41ms 95.60ms 96.47ms  1.08116.48K 68.42M
 chk_1 84.40ms 76.65ms 77.51ms  1.10116.48K 46.19M
   Avg 93.90 86.12 86.99
   Max103.41 95.60 96.47
   Min 84.40 76.65 77.51
 Ratio  1.23  1.25  1.24
   Var 90.41 89.74 89.85
Profiling takes 6.038 s
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 151)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 151)...
Node 0, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 9.6758	TrainAcc 0.0960	ValidAcc 0.1181	TestAcc 0.1198	BestValid 0.1181
	Epoch 50:	Loss 0.6649	TrainAcc 0.6710	ValidAcc 0.6949	TestAcc 0.6885	BestValid 0.6949
	Epoch 100:	Loss 0.4031	TrainAcc 0.7288	ValidAcc 0.7502	TestAcc 0.7441	BestValid 0.7502
	Epoch 150:	Loss 0.3257	TrainAcc 0.7511	ValidAcc 0.7710	TestAcc 0.7647	BestValid 0.7710
	Epoch 200:	Loss 0.2901	TrainAcc 0.7647	ValidAcc 0.7818	TestAcc 0.7755	BestValid 0.7818
	Epoch 250:	Loss 0.2624	TrainAcc 0.7654	ValidAcc 0.7816	TestAcc 0.7758	BestValid 0.7818
	Epoch 300:	Loss 0.2474	TrainAcc 0.7664	ValidAcc 0.7819	TestAcc 0.7774	BestValid 0.7819
	Epoch 350:	Loss 0.2329	TrainAcc 0.7740	ValidAcc 0.7878	TestAcc 0.7841	BestValid 0.7878
	Epoch 400:	Loss 0.2220	TrainAcc 0.7750	ValidAcc 0.7885	TestAcc 0.7848	BestValid 0.7885
	Epoch 450:	Loss 0.2121	TrainAcc 0.7732	ValidAcc 0.7869	TestAcc 0.7826	BestValid 0.7885
	Epoch 500:	Loss 0.2043	TrainAcc 0.7779	ValidAcc 0.7906	TestAcc 0.7871	BestValid 0.7906
	Epoch 550:	Loss 0.1987	TrainAcc 0.7786	ValidAcc 0.7909	TestAcc 0.7876	BestValid 0.7909
	Epoch 600:	Loss 0.1939	TrainAcc 0.7822	ValidAcc 0.7935	TestAcc 0.7896	BestValid 0.7935
	Epoch 650:	Loss 0.1885	TrainAcc 0.7829	ValidAcc 0.7924	TestAcc 0.7891	BestValid 0.7935
	Epoch 700:	Loss 0.1843	TrainAcc 0.7860	ValidAcc 0.7967	TestAcc 0.7923	BestValid 0.7967
	Epoch 750:	Loss 0.1802	TrainAcc 0.7852	ValidAcc 0.7945	TestAcc 0.7911	BestValid 0.7967
	Epoch 800:	Loss 0.1743	TrainAcc 0.7865	ValidAcc 0.7960	TestAcc 0.7924	BestValid 0.7967
	Epoch 850:	Loss 0.1726	TrainAcc 0.7893	ValidAcc 0.7962	TestAcc 0.7933	BestValid 0.7967
	Epoch 900:	Loss 0.1699	TrainAcc 0.7881	ValidAcc 0.7952	TestAcc 0.7926	BestValid 0.7967
	Epoch 950:	Loss 0.1669	TrainAcc 0.7888	ValidAcc 0.7965	TestAcc 0.7934	BestValid 0.7967
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd-gnerv4: error: *** STEP 3368.1 ON gnerv4 CANCELLED AT 2023-09-15T14:04:22 DUE TO TIME LIMIT ***
slurmstepd-gnerv4: error: *** JOB 3368 ON gnerv4 CANCELLED AT 2023-09-15T14:04:22 DUE TO TIME LIMIT ***
