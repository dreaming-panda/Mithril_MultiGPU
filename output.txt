g000.anvil.rcac.purdue.edu
Wed Apr  5 21:37:58 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:81:00.0 Off |                    0 |
| N/A   39C    P0    52W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2       9) zlib/1.2.11
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0       10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0      11) openmpi/4.0.6
  4) gmp/6.2.1              8) numactl/2.0.14  12) boost/1.74.0

 

[  4%] Built target context
[ 19%] Built target parallel
[ 20%] Built target core
[ 40%] Built target cudahelp
[ 42%] Built target test_mpi_gpu_hybrid
[ 46%] Built target test_cuda_pipeline_parallel
[ 46%] Built target test_cuda_model_parallel
[ 51%] Built target test_cuda_graph
[ 51%] Built target test_nccl_thread
[ 53%] Built target test_trivial
[ 57%] Built target test_mpi_combined
[ 64%] Built target test_mpi_gpu_model_parallel
[ 67%] Built target test_hello_world
[ 68%] Built target test_mpi_non_structual_graph
[ 64%] Built target test_cuda
[ 68%] Built target test_mpi_gpu_pipelined_model_parallel
[ 71%] Built target test_full_non_structual_graph
[ 75%] Built target test_single_node_fullgpu_training
[ 75%] Built target test_mpi_loader
[ 78%] Built target test_graph
[ 81%] Built target test_full_structual_graph
[ 81%] Built target test_mpi_structual_graph
[ 82%] Built target test_mpi_pipelined_model_parallel
[ 85%] Built target test_nccl_mpi
[ 87%] Built target test_single_node_training
[ 89%] Built target test_single_node_gpu_training
[ 91%] Built target estimate_comm_volume
[ 91%] Built target test_mpi_model_parallel
[ 98%] Built target OSDI2023_SINGLE_NODE_gcn_inference
[100%] Built target OSDI2023_MULTI_NODES_gcn
[100%] Built target test_two_layer_hybrid_parallelism_designer
[100%] Built target OSDI2023_SINGLE_NODE_gcn
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 16
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.003000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 3
The scaling down factor of out-of-chunk gradients: 0.100000
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 16
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.003000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 3
The scaling down factor of out-of-chunk gradients: 0.100000
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 16
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.003000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 3
The scaling down factor of out-of-chunk gradients: 0.100000
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 16
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.003000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 3
The scaling down factor of out-of-chunk gradients: 0.100000
Initialized node 1 on machine g001.anvil.rcac.purdue.edu
Initialized node 0 on machine g000.anvil.rcac.purdue.edu
Initialized node 3 on machine g004.anvil.rcac.purdue.edu
Initialized node 2 on machine g003.anvil.rcac.purdue.edu
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.933 seconds.
Building the CSC structure...
        It takes 1.985 seconds.
Building the CSC structure...
        It takes 1.987 seconds.
Building the CSC structure...
        It takes 1.983 seconds.
Building the CSC structure...
        It takes 1.894 seconds.
        It takes 1.903 seconds.
        It takes 1.956 seconds.
        It takes 1.957 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.613 seconds.
Building the Label Vector...
        It takes 0.640 seconds.
Building the Label Vector...
        It takes 0.632 seconds.
Building the Label Vector...
        It takes 0.627 seconds.
Building the Label Vector...
        It takes 0.339 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
        It takes 0.348 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
        It takes 0.342 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
        It takes 0.340 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
WARNING: the current version only applies to linear GNN models!
*** Node 0, starting model training...
Number of operators: 80
0 2449029 0 21
0 2449029 21 41
0 2449029 41 61
0 2449029 61 80
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the partition [0, 21) x [0, 2449029)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_WEIGHT, output tensors: 1
    Op 2: type OPERATOR_MATMUL, output tensors: 2
    Op 3: type OPERATOR_AGGREGATION, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_AGGREGATION, output tensors: 6
    Op 7: type OPERATOR_WEIGHT, output tensors: 7
    Op 8: type OPERATOR_MATMUL, output tensors: 8
    Op 9: type OPERATOR_RELU, output tensors: 9
    Op 10: type OPERATOR_DROPOUT, output tensors: 10
    Op 11: type OPERATOR_AGGREGATION, output tensors: 11
    Op 12: type OPERATOR_WEIGHT, output tensors: 12
    Op 13: type OPERATOR_MATMUL, output tensors: 13
    Op 14: type OPERATOR_RELU, output tensors: 14
    Op 15: type OPERATOR_DROPOUT, output tensors: 15
    Op 16: type OPERATOR_AGGREGATION, output tensors: 16
    Op 17: type OPERATOR_WEIGHT, output tensors: 17
    Op 18: type OPERATOR_MATMUL, output tensors: 18
    Op 19: type OPERATOR_RELU, output tensors: 19
    Op 20: type OPERATOR_DROPOUT, output tensors: 20
    Op 21: type OPERATOR_AGGREGATION, output tensors: 21
    Op 22: type OPERATOR_WEIGHT, output tensors: 22
    Op 23: type OPERATOR_MATMUL, output tensors: 23
    Op 24: type OPERATOR_RELU, output tensors: 24
    Op 25: type OPERATOR_DROPOUT, output tensors: 25
    Op 26: type OPERATOR_AGGREGATION, output tensors: 26
    Op 27: type OPERATOR_WEIGHT, output tensors: 27
    Op 28: type OPERATOR_MATMUL, output tensors: 28
    Op 29: type OPERATOR_RELU, output tensors: 29
    Op 30: type OPERATOR_DROPOUT, output tensors: 30
    Op 31: type OPERATOR_AGGREGATION, output tensors: 31
    Op 32: type OPERATOR_WEIGHT, output tensors: 32
    Op 33: type OPERATOR_MATMUL, output tensors: 33
    Op 34: type OPERATOR_RELU, output tensors: 34
    Op 35: type OPERATOR_DROPOUT, output tensors: 35
    Op 36: type OPERATOR_AGGREGATION, output tensors: 36
    Op 37: type OPERATOR_WEIGHT, output tensors: 37
    Op 38: type OPERATOR_MATMUL, output tensors: 38
    Op 39: type OPERATOR_RELU, output tensors: 39
    Op 40: type OPERATOR_DROPOUT, output tensors: 40
    Op 41: type OPERATOR_AGGREGATION, output tensors: 41
    Op 42: type OPERATOR_WEIGHT, output tensors: 42
    Op 43: type OPERATOR_MATMUL, output tensors: 43
    Op 44: type OPERATOR_RELU, output tensors: 44
    Op 45: type OPERATOR_DROPOUT, output tensors: 45
    Op 46: type OPERATOR_AGGREGATION, output tensors: 46
    Op 47: type OPERATOR_WEIGHT, output tensors: 47
    Op 48: type OPERATOR_MATMUL, output tensors: 48
    Op 49: type OPERATOR_RELU, output tensors: 49
    Op 50: type OPERATOR_DROPOUT, output tensors: 50
    Op 51: type OPERATOR_AGGREGATION, output tensors: 51
    Op 52: type OPERATOR_WEIGHT, output tensors: 52
    Op 53: type OPERATOR_MATMUL, output tensors: 53
    Op 54: type OPERATOR_RELU, output tensors: 54
    Op 55: type OPERATOR_DROPOUT, output tensors: 55
    Op 56: type OPERATOR_AGGREGATION, output tensors: 56
    Op 57: type OPERATOR_WEIGHT, output tensors: 57
    Op 58: type OPERATOR_MATMUL, output tensors: 58
    Op 59: type OPERATOR_RELU, output tensors: 59
    Op 60: type OPERATOR_DROPOUT, output tensors: 60
    Op 61: type OPERATOR_AGGREGATION, output tensors: 61
    Op 62: type OPERATOR_WEIGHT, output tensors: 62
    Op 63: type OPERATOR_MATMUL, output tensors: 63
    Op 64: type OPERATOR_RELU, output tensors: 64
    Op 65: type OPERATOR_DROPOUT, output tensors: 65
    Op 66: type OPERATOR_AGGREGATION, output tensors: 66
    Op 67: type OPERATOR_WEIGHT, output tensors: 67
    Op 68: type OPERATOR_MATMUL, output tensors: 68
    Op 69: type OPERATOR_RELU, output tensors: 69
    Op 70: type OPERATOR_DROPOUT, output tensors: 70
    Op 71: type OPERATOR_AGGREGATION, output tensors: 71
    Op 72: type OPERATOR_WEIGHT, output tensors: 72
    Op 73: type OPERATOR_MATMUL, output tensors: 73
    Op 74: type OPERATOR_RELU, output tensors: 74
    Op 75: type OPERATOR_DROPOUT, output tensors: 75
    Op 76: type OPERATOR_AGGREGATION, output tensors: 76
    Op 77: type OPERATOR_WEIGHT, output tensors: 77
    Op 78: type OPERATOR_MATMUL, output tensors: 78
    Op 79: type OPERATOR_SOFTMAX, output tensors: 79
Boundaries: 0 0 0 0 2449029 2449029 2449029 2449029
Fragments: [0, 2449029)
Chunks (number of global chunks: 24): 0-[0, 102043) 1-[102043, 204086) 2-[204086, 306129) 3-[306129, 408172) 4-[408172, 510215) 5-[510215, 612258) 6-[612258, 714301) 7-[714301, 816344) 8-[816344, 918387) ... 23-[2346989, 2449029)
(Forwarding) Node 0 (fragment 0) depends on nodes:
(Backwarding) Node 0 (fragment 0) depends on nodes: 1 (Tensor: 20)
(I-link dependencies): node 0 should send activation to nodes: 1 (tensor: 20)
(I-link dependencies): node 0 should receive activation from nodes:
(I-link dependencies): node 0 should send gradient to nodes:
(I-link dependencies): node 0 should receive gradient from nodes:
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
WARNING: the current version only applies to linear GNN models!
*** Node 2, starting model training...
Number of operators: 80
0 2449029 0 21
0 2449029 21 41
0 2449029 41 61
0 2449029 61 80
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the partition [41, 61) x [0, 2449029)
*** Node 2, constructing the helper classes...
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
*** Node 1, starting model training...
Number of operators: 80
0 2449029 0 21
0 2449029 21 41
0 2449029 41 61
0 2449029 61 80
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the partition [21, 41) x [0, 2449029)
WARNING: the current version only applies to linear GNN models!
*** Node 1, constructing the helper classes...
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
WARNING: the current version only applies to linear GNN models!
*** Node 3, starting model training...
Number of operators: 80
0 2449029 0 21
0 2449029 21 41
0 2449029 41 61
0 2449029 61 80
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the partition [61, 80) x [0, 2449029)
*** Node 3, constructing the helper classes...
(Forwarding) Node 2 (fragment 0) depends on nodes: 1 (Tensor: 40)
(Backwarding) Node 2 (fragment 0) depends on nodes: 3 (Tensor: 60)
(I-link dependencies): node 2 should send activation to nodes: 3 (tensor: 60)
(I-link dependencies): node 2 should receive activation from nodes: 1 (tensor: 40)
(I-link dependencies): node 2 should send gradient to nodes:
(I-link dependencies): node 2 should receive gradient from nodes:
(Forwarding) Node 1 (fragment 0) depends on nodes: 0 (Tensor: 20)
(Backwarding) Node 1 (fragment 0) depends on nodes: 2 (Tensor: 40)
(I-link dependencies): node 1 should send activation to nodes: 2 (tensor: 40)
(I-link dependencies): node 1 should receive activation from nodes: 0 (tensor: 20)
(I-link dependencies): node 1 should send gradient to nodes:
(I-link dependencies): node 1 should receive gradient from nodes:
(Forwarding) Node 3 (fragment 0) depends on nodes: 2 (Tensor: 60)
(Backwarding) Node 3 (fragment 0) depends on nodes:
(I-link dependencies): node 3 should send activation to nodes:
(I-link dependencies): node 3 should receive activation from nodes: 2 (tensor: 60)
(I-link dependencies): node 3 should send gradient to nodes:
(I-link dependencies): node 3 should receive gradient from nodes:
2449029, 126167053, 126167053
Number of vertices per chunk: 102043
2449029, 126167053, 126167053
Number of vertices per chunk: 102043
2449029, 126167053, 126167053
Number of vertices per chunk: 102043
2449029, 126167053, 126167053
Number of vertices per chunk: 102043
csr in-out ready !*** Node 0, setting up some other necessary information...
csr in-out ready !*** Node 3, setting up some other necessary information...
csr in-out ready !*** Node 1, setting up some other necessary information...
csr in-out ready !*** Node 2, setting up some other necessary information...
*** Node 0, starting the helper threads...
*** Node 1, starting the helper threads...
*** Node 2, starting the helper threads...
*** Node 3, starting the helper threads...
+++++++++ Node 0 initializing the weights for op[0, 21)...
+++++++++ Node 0, mapping weight op 1
+++++++++ Node 3 initializing the weights for op[61, 80)...
+++++++++ Node 3, mapping weight op 62
+++++++++ Node 0, mapping weight op 7
+++++++++ Node 0, mapping weight op 12
+++++++++ Node 3, mapping weight op 67
+++++++++ Node 0, mapping weight op 17
+++++++++ Node 3, mapping weight op 72
+++++++++ Node 3, mapping weight op 77
+++++++++ Node 1 initializing the weights for op[21, 41)...
+++++++++ Node 1, mapping weight op 22
+++++++++ Node 2 initializing the weights for op[41, 61)...
+++++++++ Node 2, mapping weight op 57
+++++++++ Node 1, mapping weight op 27
+++++++++ Node 2, mapping weight op 42
+++++++++ Node 1, mapping weight op 32
+++++++++ Node 1, mapping weight op 37
+++++++++ Node 2, mapping weight op 47
+++++++++ Node 2, mapping weight op 52
RANDOMLY DISPATCH THE CHUNKS...
*** Node 0, starting task scheduling...
*** Node 3, starting task scheduling...



The learning rate specified by the user: 0.003000000
The learning rate specified by the user: 0.003000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.003000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.003000000
    Epoch 9:	Loss 3.32308	TrainAcc 0.2647	ValidAcc 0.2467	TestAcc 0.1836
    Epoch 19:	Loss 2.76476	TrainAcc 0.1519	ValidAcc 0.1486	TestAcc 0.1252
    Epoch 29:	Loss 2.47823	TrainAcc 0.3084	ValidAcc 0.3082	TestAcc 0.2683
    Epoch 39:	Loss 2.31836	TrainAcc 0.3078	ValidAcc 0.3052	TestAcc 0.2509
    Epoch 49:	Loss 2.20626	TrainAcc 0.3079	ValidAcc 0.3056	TestAcc 0.2557
    Epoch 59:	Loss 2.16611	TrainAcc 0.3070	ValidAcc 0.3022	TestAcc 0.2434
    Epoch 69:	Loss 2.16935	TrainAcc 0.3079	ValidAcc 0.3054	TestAcc 0.2539
    Epoch 79:	Loss 2.12938	TrainAcc 0.3074	ValidAcc 0.3032	TestAcc 0.2482
    Epoch 89:	Loss 2.03929	TrainAcc 0.3605	ValidAcc 0.3478	TestAcc 0.2479
Node 0, Layer-level comm throughput (act): -nan GBps
Node 1, Layer-level comm throughput (act): 10.613 GBps
Node 2, Layer-level comm throughput (act): 10.087 GBps
Node 3, Layer-level comm throughput (act): 10.323 GBps
Node 3, Layer-level comm throughput (grad): -nan GBps
Node 2, Layer-level comm throughput (grad): 10.930 GBps
Node 1, Layer-level comm throughput (grad): 9.881 GBps
Node 0, Layer-level comm throughput (grad): 10.036 GBps
    Epoch 99:	Loss 1.89939	TrainAcc 0.3822	ValidAcc 0.3757	TestAcc 0.2829
Node 0, GPU memory consumption: 33.117 GB
Node 0, compression time: 0.448s, compression size: 116.779GB, throughput: 260.434GBps
Node 0, decompression time: 1.025s, compression size: 116.779GB, throughput: 113.879GBps
Node 0, pure compute time: 26.225 s, total compute time: 27.699 s
Node 0, wait_for_task_time: 26.272 s, wait_for_other_gpus_time: 0.044 s
------------------------node id 0,  per-epoch time: 0.555092 s---------------
Node 1, GPU memory consumption: 33.394 GB
Node 1, compression time: 4.892s, compression size: 233.558GB, throughput: 47.739GBps
Node 1, decompression time: 10.313s, compression size: 233.558GB, throughput: 22.647GBps
Node 1, pure compute time: 28.587 s, total compute time: 43.792 s
Node 1, wait_for_task_time: 8.415 s, wait_for_other_gpus_time: 0.004 s
------------------------node id 1,  per-epoch time: 0.555092 s---------------
Node 2, GPU memory consumption: 33.394 GB
Node 2, compression time: 1.825s, compression size: 233.558GB, throughput: 127.998GBps
Node 3, GPU memory consumption: 32.419 GB
Node 3, compression time: 0.719s, compression size: 116.779GB, throughput: 162.455GBps
Node 3, decompression time: 1.011s, compression size: 116.779GB, throughput: 115.529GBps
Node 3, pure compute time: 24.617 s, total compute time: 26.347 s
Node 3, wait_for_task_time: 12.892 s, wait_for_other_gpus_time: 0.042 s
------------------------node id 3,  per-epoch time: 0.555092 s---------------
Node 2, decompression time: 4.942s, compression size: 233.558GB, throughput: 47.256GBps
Node 2, pure compute time: 25.876 s, total compute time: 32.643 s
Node 2, wait_for_task_time: 11.390 s, wait_for_other_gpus_time: 0.041 s
------------------------node id 2,  per-epoch time: 0.555092 s---------------
************ Profiling Results ************
	Bubble: 13.955033 (s) (24.48 percentage)
	Compute: 34.967403 (s) (61.34 percentage)
	GradSync: 0.647968 (s) (1.14 percentage)
	GraphComm: 0.166933 (s) (0.29 percentage)
	Imbalance: 6.194246 (s) (10.87 percentage)
	LayerComm: 1.076730 (s) (1.89 percentage)
	Layer-level communication (cluster-wide, per epoch): 1.316 GB
Highest valid_acc: 0.3757
Target test_acc: 0.2829
Epoch to reach the target acc: 100
[MPI Rank 2] Success 
[MPI Rank 1] Success 
[MPI Rank 0] Success 
[MPI Rank 3] Success 
