g006.anvil.rcac.purdue.edu
Fri Feb 17 03:09:46 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:01:00.0 Off |                    0 |
| N/A   30C    P0    51W / 400W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2    9) numactl/2.0.14
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0    10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0   11) openmpi/4.0.6
  4) gmp/6.2.1              8) zlib/1.2.11  12) boost/1.74.0

 

[  4%] Built target context
[ 17%] Built target parallel
[ 19%] Built target core
[ 39%] Built target cudahelp
[ 80%] Built target test_mpi_gpu_model_parallel
[ 87%] Built target estimate_comm_volume
[ 82%] Built target test_trivial
[ 82%] Built target test_cuda_model_parallel
[ 82%] Built target test_cuda_pipeline_parallel
[ 91%] Built target test_single_node_gpu_training
[ 82%] Built target test_hello_world
[ 82%] Built target test_full_non_structual_graph
[ 82%] Built target test_mpi_combined
[ 91%] Built target test_nccl_mpi
[ 82%] Built target test_mpi_gpu_hybrid
[ 91%] Built target test_cuda
[ 91%] Built target test_mpi_model_parallel
[ 82%] Built target test_graph
[ 91%] Built target test_mpi_structual_graph
[ 91%] Built target test_single_node_training
[ 91%] Built target test_mpi_loader
[ 91%] Built target test_mpi_pipelined_model_parallel
[ 91%] Built target test_full_structual_graph
[ 91%] Built target test_mpi_gpu_pipelined_model_parallel
[ 91%] Built target test_single_node_fullgpu_training
[ 82%] Built target test_cuda_graph
[ 82%] Built target test_nccl_thread
[ 82%] Built target test_cuda_data_compression
[ 91%] Built target test_mpi_non_structual_graph
[ 97%] Built target OSDI2023_MULTI_NODES_gcn
[100%] Built target OSDI2023_SINGLE_NODE_gcn
[100%] Built target test_two_layer_hybrid_parallelism_designer
[100%] Built target OSDI2023_SINGLE_NODE_gcn_inference
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_arxiv
The number of GCN layers: 8
The number of hidden units: 256
The number of training epoches: 500
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 17
The scaling down factor of out-of-chunk gradients: 0.100000
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_arxiv
The number of GCN layers: 8
The number of hidden units: 256
The number of training epoches: 500
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 17
The scaling down factor of out-of-chunk gradients: 0.100000
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_arxiv
The number of GCN layers: 8
The number of hidden units: 256
The number of training epoches: 500
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 17
The scaling down factor of out-of-chunk gradients: 0.100000
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_arxiv
The number of GCN layers: 8
The number of hidden units: 256
The number of training epoches: 500
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 17
The scaling down factor of out-of-chunk gradients: 0.100000
Initialized node 0 on machine g006.anvil.rcac.purdue.edu
Initialized node 1 on machine g007.anvil.rcac.purdue.edu
Initialized node 2 on machine g008.anvil.rcac.purdue.edu
Initialized node 3 on machine g009.anvil.rcac.purdue.edu
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.046 seconds.
Building the CSC structure...
Building the CSR structure...
        It takes 0.046 seconds.
Building the CSC structure...
        It takes 0.047 seconds.
Building the CSC structure...
        It takes 0.037 seconds.
        It takes 0.037 seconds.
        It takes 0.038 seconds.
        It takes 0.042 seconds.
Building the CSC structure...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.038 seconds.
Building the Feature Vector...
        It takes 0.069 seconds.
        It takes 0.070 seconds.
Building the Label Vector...
        It takes 0.067 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.028 seconds.
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
        It takes 0.030 seconds.
        It takes 0.030 seconds.
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
        It takes 0.059 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
train nodes 90941, valid nodes 29799, test nodes 48603
Number of GPUs: 4
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
GPU 2, layer [4, 6)
GPU 3, layer [6, 8)
*** Node 3, starting model training...
Number of operators: 40
0 169343 0 11
0 169343 11 21
0 169343 21 31
0 169343 31 40
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the partition [31, 40) x [0, 169343)
*** Node 3, constructing the helper classes...
(Forwarding) Node 3 (fragment 0) depends on nodes: 2 (Tensor: 30)
(Backwarding) Node 3 (fragment 0) depends on nodes:
(I-link dependencies): node 3 should send activation to nodes:
(I-link dependencies): node 3 should receive activation from nodes: 2 (tensor: 30)
(I-link dependencies): node 3 should send gradient to nodes:
(I-link dependencies): node 3 should receive gradient from nodes:
169343, 2484941, 2484941
Number of vertices per chunk: 10584
train nodes 90941, valid nodes 29799, test nodes 48603
Number of GPUs: 4
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
GPU 2, layer [4, 6)
GPU 3, layer [6, 8)
WARNING: the current version only applies to linear GNN models!
*** Node 0, starting model training...
Number of operators: 40
0 169343 0 11
0 169343 11 21
0 169343 21 31
0 169343 31 40
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the partition [0, 11) x [0, 169343)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_WEIGHT, output tensors: 1
    Op 2: type OPERATOR_MATMUL, output tensors: 2
    Op 3: type OPERATOR_AGGREGATION, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_AGGREGATION, output tensors: 6
    Op 7: type OPERATOR_WEIGHT, output tensors: 7
    Op 8: type OPERATOR_MATMUL, output tensors: 8
    Op 9: type OPERATOR_RELU, output tensors: 9
    Op 10: type OPERATOR_DROPOUT, output tensors: 10
    Op 11: type OPERATOR_AGGREGATION, output tensors: 11
    Op 12: type OPERATOR_WEIGHT, output tensors: 12
    Op 13: type OPERATOR_MATMUL, output tensors: 13
    Op 14: type OPERATOR_RELU, output tensors: 14
    Op 15: type OPERATOR_DROPOUT, output tensors: 15
    Op 16: type OPERATOR_AGGREGATION, output tensors: 16
    Op 17: type OPERATOR_WEIGHT, output tensors: 17
    Op 18: type OPERATOR_MATMUL, output tensors: 18
    Op 19: type OPERATOR_RELU, output tensors: 19
    Op 20: type OPERATOR_DROPOUT, output tensors: 20
    Op 21: type OPERATOR_AGGREGATION, output tensors: 21
    Op 22: type OPERATOR_WEIGHT, output tensors: 22
    Op 23: type OPERATOR_MATMUL, output tensors: 23
    Op 24: type OPERATOR_RELU, output tensors: 24
    Op 25: type OPERATOR_DROPOUT, output tensors: 25
    Op 26: type OPERATOR_AGGREGATION, output tensors: 26
    Op 27: type OPERATOR_WEIGHT, output tensors: 27
    Op 28: type OPERATOR_MATMUL, output tensors: 28
    Op 29: type OPERATOR_RELU, output tensors: 29
    Op 30: type OPERATOR_DROPOUT, output tensors: 30
    Op 31: type OPERATOR_AGGREGATION, output tensors: 31
    Op 32: type OPERATOR_WEIGHT, output tensors: 32
    Op 33: type OPERATOR_MATMUL, output tensors: 33
    Op 34: type OPERATOR_RELU, output tensors: 34
    Op 35: type OPERATOR_DROPOUT, output tensors: 35
    Op 36: type OPERATOR_AGGREGATION, output tensors: 36
    Op 37: type OPERATOR_WEIGHT, output tensors: 37
    Op 38: type OPERATOR_MATMUL, output tensors: 38
    Op 39: type OPERATOR_SOFTMAX, output tensors: 39
Boundaries: 0 0 0 0 169343 169343 169343 169343
Fragments: [0, 169343)
Chunks (number of global chunks: 16): 0-[0, 10584) 1-[10584, 21168) 2-[21168, 31752) 3-[31752, 42336) 4-[42336, 52920) 5-[52920, 63504) 6-[63504, 74088) 7-[74088, 84672) 8-[84672, 95256) ... 15-[158760, 169343)
(Forwarding) Node 0 (fragment 0) depends on nodes:
(Backwarding) Node 0 (fragment 0) depends on nodes: 1 (Tensor: 10)
(I-link dependencies): node 0 should send activation to nodes: 1 (tensor: 10)
(I-link dependencies): node 0 should receive activation from nodes:
(I-link dependencies): node 0 should send gradient to nodes:
(I-link dependencies): node 0 should receive gradient from nodes:
169343, 2484941, 2484941
Number of vertices per chunk: 10584
train nodes 90941, valid nodes 29799, test nodes 48603
Number of GPUs: 4
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
GPU 2, layer [4, 6)
GPU 3, layer [6, 8)
*** Node 1, starting model training...
Number of operators: 40
0 169343 0 11
0 169343 11 21
0 169343 21 31
0 169343 31 40
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the partition [11, 21) x [0, 169343)
*** Node 1, constructing the helper classes...
(Forwarding) Node 1 (fragment 0) depends on nodes: 0 (Tensor: 10)
(Backwarding) Node 1 (fragment 0) depends on nodes: 2 (Tensor: 20)
(I-link dependencies): node 1 should send activation to nodes: 2 (tensor: 20)
(I-link dependencies): node 1 should receive activation from nodes: 0 (tensor: 10)
(I-link dependencies): node 1 should send gradient to nodes:
(I-link dependencies): node 1 should receive gradient from nodes:
169343, 2484941, 2484941
Number of vertices per chunk: 10584
train nodes 90941, valid nodes 29799, test nodes 48603
Number of GPUs: 4
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
GPU 2, layer [4, 6)
GPU 3, layer [6, 8)
WARNING: the current version only applies to linear GNN models!
*** Node 2, starting model training...
Number of operators: 40
0 169343 0 11
0 169343 11 21
0 169343 21 31
0 169343 31 40
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the partition [21, 31) x [0, 169343)
*** Node 2, constructing the helper classes...
(Forwarding) Node 2 (fragment 0) depends on nodes: 1 (Tensor: 20)
(Backwarding) Node 2 (fragment 0) depends on nodes: 3 (Tensor: 30)
(I-link dependencies): node 2 should send activation to nodes: 3 (tensor: 30)
(I-link dependencies): node 2 should receive activation from nodes: 1 (tensor: 20)
(I-link dependencies): node 2 should send gradient to nodes:
(I-link dependencies): node 2 should receive gradient from nodes:
169343, 2484941, 2484941
Number of vertices per chunk: 10584
csr in-out ready !*** Node 0, setting up some other necessary information...
csr in-out ready !*** Node 1, setting up some other necessary information...
csr in-out ready !*** Node 2, setting up some other necessary information...
csr in-out ready !*** Node 3, setting up some other necessary information...
*** Node 0, starting the helper threads...
*** Node 1, starting the helper threads...
*** Node 3, starting the helper threads...
*** Node 2, starting the helper threads...
+++++++++ Node 1 initializing the weights for op[11, 21)...
+++++++++ Node 1, mapping weight op 12
+++++++++ Node 2 initializing the weights for op[21, 31)...
+++++++++ Node 2, mapping weight op 22
+++++++++ Node 1, mapping weight op 17
+++++++++ Node 2, mapping weight op 27
+++++++++ Node 3 initializing the weights for op[31, 40)...
+++++++++ Node 3, mapping weight op 32
+++++++++ Node 0 initializing the weights for op[0, 11)...
+++++++++ Node 0, mapping weight op 1
+++++++++ Node 3, mapping weight op 37
+++++++++ Node 0, mapping weight op 7
RANDOMLY DISPATCH THE CHUNKS...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
    Epoch 9:	Loss 3.45613	TrainAcc 0.0666	ValidAcc 0.0398	TestAcc 0.0328
    Epoch 19:	Loss 3.26039	TrainAcc 0.1789	ValidAcc 0.0763	TestAcc 0.0586
    Epoch 29:	Loss 3.08869	TrainAcc 0.1787	ValidAcc 0.0765	TestAcc 0.0587
    Epoch 39:	Loss 2.99930	TrainAcc 0.1847	ValidAcc 0.0831	TestAcc 0.0646
    Epoch 49:	Loss 2.91804	TrainAcc 0.2142	ValidAcc 0.1241	TestAcc 0.1004
    Epoch 59:	Loss 2.77662	TrainAcc 0.2940	ValidAcc 0.2374	TestAcc 0.2129
    Epoch 69:	Loss 2.56781	TrainAcc 0.3365	ValidAcc 0.2641	TestAcc 0.2461
    Epoch 79:	Loss 2.33264	TrainAcc 0.3918	ValidAcc 0.3749	TestAcc 0.3568
    Epoch 89:	Loss 2.20873	TrainAcc 0.4276	ValidAcc 0.4404	TestAcc 0.4298
    Epoch 99:	Loss 2.10624	TrainAcc 0.4593	ValidAcc 0.4675	TestAcc 0.4362
    Epoch 109:	Loss 1.90262	TrainAcc 0.4828	ValidAcc 0.4948	TestAcc 0.4598
    Epoch 119:	Loss 1.81797	TrainAcc 0.5100	ValidAcc 0.5295	TestAcc 0.5120
    Epoch 129:	Loss 1.75811	TrainAcc 0.5258	ValidAcc 0.5416	TestAcc 0.5284
    Epoch 139:	Loss 1.70160	TrainAcc 0.5382	ValidAcc 0.5544	TestAcc 0.5364
    Epoch 149:	Loss 1.65744	TrainAcc 0.5513	ValidAcc 0.5671	TestAcc 0.5521
    Epoch 159:	Loss 1.60782	TrainAcc 0.5631	ValidAcc 0.5752	TestAcc 0.5645
    Epoch 169:	Loss 1.56339	TrainAcc 0.5772	ValidAcc 0.5873	TestAcc 0.5750
    Epoch 179:	Loss 1.52836	TrainAcc 0.5850	ValidAcc 0.5925	TestAcc 0.5827
    Epoch 189:	Loss 1.49753	TrainAcc 0.5917	ValidAcc 0.5966	TestAcc 0.5881
    Epoch 199:	Loss 1.46114	TrainAcc 0.6013	ValidAcc 0.6026	TestAcc 0.5925
    Epoch 209:	Loss 1.40807	TrainAcc 0.6149	ValidAcc 0.6179	TestAcc 0.6096
    Epoch 219:	Loss 1.38127	TrainAcc 0.6203	ValidAcc 0.6204	TestAcc 0.6108
    Epoch 229:	Loss 1.36193	TrainAcc 0.6244	ValidAcc 0.6255	TestAcc 0.6140
    Epoch 239:	Loss 1.34153	TrainAcc 0.6315	ValidAcc 0.6290	TestAcc 0.6189
    Epoch 249:	Loss 1.32216	TrainAcc 0.6361	ValidAcc 0.6319	TestAcc 0.6227
    Epoch 259:	Loss 1.30687	TrainAcc 0.6395	ValidAcc 0.6362	TestAcc 0.6284
    Epoch 269:	Loss 1.29119	TrainAcc 0.6441	ValidAcc 0.6388	TestAcc 0.6318
    Epoch 279:	Loss 1.28153	TrainAcc 0.6474	ValidAcc 0.6442	TestAcc 0.6343
    Epoch 289:	Loss 1.26648	TrainAcc 0.6515	ValidAcc 0.6454	TestAcc 0.6356
    Epoch 299:	Loss 1.24854	TrainAcc 0.6545	ValidAcc 0.6502	TestAcc 0.6427
    Epoch 309:	Loss 1.22582	TrainAcc 0.6622	ValidAcc 0.6556	TestAcc 0.6491
    Epoch 319:	Loss 1.21530	TrainAcc 0.6647	ValidAcc 0.6571	TestAcc 0.6533
    Epoch 329:	Loss 1.20037	TrainAcc 0.6699	ValidAcc 0.6623	TestAcc 0.6564
    Epoch 339:	Loss 1.18907	TrainAcc 0.6715	ValidAcc 0.6626	TestAcc 0.6555
    Epoch 349:	Loss 1.18479	TrainAcc 0.6712	ValidAcc 0.6621	TestAcc 0.6552
    Epoch 359:	Loss 1.17507	TrainAcc 0.6741	ValidAcc 0.6645	TestAcc 0.6592
    Epoch 369:	Loss 1.16885	TrainAcc 0.6755	ValidAcc 0.6662	TestAcc 0.6617
    Epoch 379:	Loss 1.16308	TrainAcc 0.6781	ValidAcc 0.6669	TestAcc 0.6593
    Epoch 389:	Loss 1.15084	TrainAcc 0.6786	ValidAcc 0.6700	TestAcc 0.6612
    Epoch 399:	Loss 1.14687	TrainAcc 0.6803	ValidAcc 0.6692	TestAcc 0.6663
    Epoch 409:	Loss 1.14586	TrainAcc 0.6830	ValidAcc 0.6711	TestAcc 0.6599
    Epoch 419:	Loss 1.13254	TrainAcc 0.6843	ValidAcc 0.6700	TestAcc 0.6658
    Epoch 429:	Loss 1.12709	TrainAcc 0.6855	ValidAcc 0.6753	TestAcc 0.6691
    Epoch 439:	Loss 1.12105	TrainAcc 0.6871	ValidAcc 0.6769	TestAcc 0.6695
    Epoch 449:	Loss 1.11497	TrainAcc 0.6873	ValidAcc 0.6747	TestAcc 0.6707
    Epoch 459:	Loss 1.11051	TrainAcc 0.6891	ValidAcc 0.6768	TestAcc 0.6680
    Epoch 469:	Loss 1.10702	TrainAcc 0.6902	ValidAcc 0.6767	TestAcc 0.6693
    Epoch 479:	Loss 1.10456	TrainAcc 0.6902	ValidAcc 0.6778	TestAcc 0.6698
    Epoch 489:	Loss 1.10249	TrainAcc 0.6908	ValidAcc 0.6779	TestAcc 0.6691
Node 0, Layer-level comm throughput (act): -nan GBps
Node 1, Layer-level comm throughput (act): 10.754 GBps
Node 2, Layer-level comm throughput (act): 10.422 GBps
Node 3, Layer-level comm throughput (act): 10.679 GBps
Node 3, Layer-level comm throughput (grad): -nan GBps
Node 2, Layer-level comm throughput (grad): 10.693 GBps
Node 1, Layer-level comm throughput (grad): 10.531 GBps
Node 0, Layer-level comm throughput (grad): 10.908 GBps
    Epoch 499:	Loss 1.09649	TrainAcc 0.6910	ValidAcc 0.6800	TestAcc 0.6727
Node 0, compression time: 0.651s, compression size: 80.749GB, throughput: 124.007GBps
Node 0, decompression time: 1.191s, compression size: 80.749GB, throughput: 67.791GBps
Node 0, pure compute time: 14.885 s, total compute time: 16.727 s
Node 0, wait_for_task_time: 8.459 s, wait_for_other_gpus_time: 0.007 s
------------------------node id 0,  per-epoch time: 0.054258 s---------------
Node 1, compression time: 1.315s, compression size: 161.498GB, throughput: 122.790GBps
Node 1, decompression time: 3.439s, compression size: 161.498GB, throughput: 46.955GBps
Node 1, pure compute time: 14.942 s, total compute time: 19.697 s
Node 1, wait_for_task_time: 4.306 s, wait_for_other_gpus_time: 0.005 s
------------------------node id 1,  per-epoch time: 0.054258 s---------------
Node 3, compression time: 0.764s, compression size: 80.749GB, throughput: 105.725GBps
Node 3, decompression time: 1.221s, compression size: 80.749GB, throughput: 66.152GBps
Node 3, pure compute time: 10.543 s, total compute time: 12.528 s
Node 3, wait_for_task_time: 3.777 s, wait_for_other_gpus_time: 0.009 s
------------------------node id 3,  per-epoch time: 0.054258 s---------------
Node 2, compression time: 1.336s, compression size: 161.498GB, throughput: 120.877GBps
Node 2, decompression time: 3.393s, compression size: 161.498GB, throughput: 47.591GBps
Node 2, pure compute time: 14.948 s, total compute time: 19.678 s
Node 2, wait_for_task_time: 3.044 s, wait_for_other_gpus_time: 0.009 s
------------------------node id 2,  per-epoch time: 0.054258 s---------------
************ Profiling Results ************
	Bubble: 3.913947 (s) (14.21 percentage)
	Compute: 18.804756 (s) (68.29 percentage)
	GradSync: 0.824574 (s) (2.99 percentage)
	GraphComm: 0.021099 (s) (0.08 percentage)
	Imbalance: 2.796334 (s) (10.16 percentage)
	LayerComm: 1.173962 (s) (4.26 percentage)
	Layer-level communication (cluster-wide, per epoch): 0.302 GB
Highest valid_acc: 0.6800
Target test_acc: 0.6727
Epoch to reach the target acc: 500
[MPI Rank 3] Success 
[MPI Rank 0] Success 
[MPI Rank 2] Success 
[MPI Rank 1] Success 
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_arxiv
The number of GCN layers: 8
The number of hidden units: 256
The number of training epoches: 0
Learning rate: 0.000000
Initialized node g006.anvil.rcac.purdue.edu
Building the CSR structure...
        It takes 0.039 seconds.
Building the CSC structure...
        It takes 0.038 seconds.
Building the Feature Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.021 seconds.
Number of classes: 40
Number of feature dimensions: 128
Dropout: 0.000 
train nodes 90941, valid nodes 29799, test nodes 48603
*** Allocating resources for all tensors...
    OP_TYPE: OPERATOR_INPUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_SOFTMAX
*** Done allocating resource.
*** Preparing the input tensor...
*** Done preparing the input tensor.
*** Preparing the STD tensor...
    Number of labels: 40
    Number of vertices: 169343
*** Done preparing the STD tensor.
Version 0	TrainAcc 0.0597	ValidAcc 0.0345	TestAcc 0.0291
Version 1	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586
Version 2	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586
Version 3	TrainAcc 0.1899	ValidAcc 0.0839	TestAcc 0.0648
Version 4	TrainAcc 0.2205	ValidAcc 0.1100	TestAcc 0.0890
Version 5	TrainAcc 0.3589	ValidAcc 0.3513	TestAcc 0.3156
Version 6	TrainAcc 0.3852	ValidAcc 0.3598	TestAcc 0.3236
Version 7	TrainAcc 0.4125	ValidAcc 0.3820	TestAcc 0.3426
Version 8	TrainAcc 0.4506	ValidAcc 0.4189	TestAcc 0.3759
Version 9	TrainAcc 0.4708	ValidAcc 0.4389	TestAcc 0.3968
Version 10	TrainAcc 0.4858	ValidAcc 0.4726	TestAcc 0.4311
Version 11	TrainAcc 0.5248	ValidAcc 0.5178	TestAcc 0.4911
Version 12	TrainAcc 0.5488	ValidAcc 0.5584	TestAcc 0.5504
Version 13	TrainAcc 0.5649	ValidAcc 0.5783	TestAcc 0.5731
Version 14	TrainAcc 0.5744	ValidAcc 0.5762	TestAcc 0.5623
Version 15	TrainAcc 0.5806	ValidAcc 0.5723	TestAcc 0.5490
Version 16	TrainAcc 0.5845	ValidAcc 0.5708	TestAcc 0.5424
Version 17	TrainAcc 0.5969	ValidAcc 0.5835	TestAcc 0.5530
Version 18	TrainAcc 0.5978	ValidAcc 0.5689	TestAcc 0.5244
Version 19	TrainAcc 0.6027	ValidAcc 0.5653	TestAcc 0.5139
Version 20	TrainAcc 0.6269	ValidAcc 0.6071	TestAcc 0.5713
Version 21	TrainAcc 0.6349	ValidAcc 0.6191	TestAcc 0.5896
Version 22	TrainAcc 0.6384	ValidAcc 0.6175	TestAcc 0.5827
Version 23	TrainAcc 0.6462	ValidAcc 0.6347	TestAcc 0.6077
Version 24	TrainAcc 0.6506	ValidAcc 0.6261	TestAcc 0.5949
Version 25	TrainAcc 0.6572	ValidAcc 0.6385	TestAcc 0.6104
Version 26	TrainAcc 0.6605	ValidAcc 0.6395	TestAcc 0.6107
Version 27	TrainAcc 0.6622	ValidAcc 0.6367	TestAcc 0.6074
Version 28	TrainAcc 0.6658	ValidAcc 0.6408	TestAcc 0.6145
Version 29	TrainAcc 0.6635	ValidAcc 0.6220	TestAcc 0.5839
Version 30	TrainAcc 0.6675	ValidAcc 0.6339	TestAcc 0.6007
Version 31	TrainAcc 0.6725	ValidAcc 0.6328	TestAcc 0.5942
Version 32	TrainAcc 0.6771	ValidAcc 0.6457	TestAcc 0.6142
Version 33	TrainAcc 0.6777	ValidAcc 0.6381	TestAcc 0.6009
Version 34	TrainAcc 0.6769	ValidAcc 0.6353	TestAcc 0.5952
Version 35	TrainAcc 0.6800	ValidAcc 0.6411	TestAcc 0.6052
Version 36	TrainAcc 0.6783	ValidAcc 0.6327	TestAcc 0.5907
Version 37	TrainAcc 0.6823	ValidAcc 0.6417	TestAcc 0.6041
Version 38	TrainAcc 0.6812	ValidAcc 0.6371	TestAcc 0.5958
Version 39	TrainAcc 0.6844	ValidAcc 0.6390	TestAcc 0.5984
Version 40	TrainAcc 0.6941	ValidAcc 0.6700	TestAcc 0.6513
Version 41	TrainAcc 0.6951	ValidAcc 0.6770	TestAcc 0.6648
Version 42	TrainAcc 0.6976	ValidAcc 0.6755	TestAcc 0.6590
Version 43	TrainAcc 0.7003	ValidAcc 0.6813	TestAcc 0.6663
Version 44	TrainAcc 0.6960	ValidAcc 0.6702	TestAcc 0.6493
Version 45	TrainAcc 0.7017	ValidAcc 0.6801	TestAcc 0.6622
Version 46	TrainAcc 0.6988	ValidAcc 0.6721	TestAcc 0.6506
Version 47	TrainAcc 0.7009	ValidAcc 0.6771	TestAcc 0.6586
Version 48	TrainAcc 0.7037	ValidAcc 0.6800	TestAcc 0.6617
Version 49	TrainAcc 0.7044	ValidAcc 0.6828	TestAcc 0.6668
Version 49 achieved the highest validation accuracy 0.6828 (test accuracy: 0.6668)
