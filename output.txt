g000.anvil.rcac.purdue.edu
Wed Mar 29 10:59:00 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:41:00.0 Off |                    0 |
| N/A   31C    P0    50W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2    9) numactl/2.0.14
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0    10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0   11) openmpi/4.0.6
  4) gmp/6.2.1              8) zlib/1.2.11  12) boost/1.74.0

 

[  4%] Built target context
[ 16%] Built target parallel
[ 19%] Built target core
[ 39%] Built target cudahelp
[ 44%] Built target test_mpi_gpu_pipelined_model_parallel
[ 47%] Built target test_mpi_gpu_hybrid
[ 47%] Built target test_cuda_data_compression
[ 51%] Built target test_mpi_gpu_model_parallel
[ 54%] Built target test_cuda_graph
[ 57%] Built target test_cuda
[ 57%] Built target test_mpi_combined
[ 58%] Built target test_cuda_model_parallel
[ 70%] Built target test_nccl_thread
[ 72%] Built target test_cuda_pipeline_parallel
[ 72%] Built target test_trivial
[ 85%] Built target test_full_non_structual_graph
[ 79%] Built target test_mpi_structual_graph
[ 86%] Built target test_graph
[ 72%] Built target test_hello_world
[ 89%] Built target test_mpi_pipelined_model_parallel
[ 86%] Built target test_nccl_mpi
[ 86%] Built target test_full_structual_graph
[ 86%] Built target test_single_node_fullgpu_training
[ 86%] Built target test_mpi_model_parallel
[ 91%] Built target estimate_comm_volume
[ 91%] Built target test_single_node_training
[ 91%] Built target test_mpi_loader
[ 91%] Built target test_mpi_non_structual_graph
[ 91%] Built target test_single_node_gpu_training
[ 98%] Built target OSDI2023_MULTI_NODES_gcn
[ 98%] Built target OSDI2023_SINGLE_NODE_gcn
[100%] Built target OSDI2023_SINGLE_NODE_gcn_inference
[100%] Built target test_two_layer_hybrid_parallelism_designer
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 16
The number of hidden units: 64
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.003000
The partition strategy: model
The dropout rate: 0.700
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 3
The scaling down factor of out-of-chunk gradients: 0.100000
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 16
The number of hidden units: 64
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.003000
The partition strategy: model
The dropout rate: 0.700
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 3
The scaling down factor of out-of-chunk gradients: 0.100000
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 16
The number of hidden units: 64
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.003000
The partition strategy: model
The dropout rate: 0.700
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 3
The scaling down factor of out-of-chunk gradients: 0.100000
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 16
The number of hidden units: 64
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.003000
The partition strategy: model
The dropout rate: 0.700
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 3
The scaling down factor of out-of-chunk gradients: 0.100000
Initialized node 1 on machine g001.anvil.rcac.purdue.edu
Initialized node 3 on machine g006.anvil.rcac.purdue.edu
Initialized node 0 on machine g000.anvil.rcac.purdue.edu
Initialized node 2 on machine g003.anvil.rcac.purdue.edu
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.964 seconds.
Building the CSC structure...
        It takes 1.961 seconds.
Building the CSC structure...
        It takes 1.980 seconds.
Building the CSC structure...
        It takes 1.994 seconds.
Building the CSC structure...
        It takes 1.926 seconds.
        It takes 1.933 seconds.
        It takes 1.947 seconds.
        It takes 1.962 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.606 seconds.
Building the Label Vector...
        It takes 0.612 seconds.
Building the Label Vector...
        It takes 0.634 seconds.
Building the Label Vector...
        It takes 0.636 seconds.
Building the Label Vector...
        It takes 0.336 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
        It takes 0.334 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
        It takes 0.342 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
        It takes 0.348 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
WARNING: the current version only applies to linear GNN models!
*** Node 0, starting model training...
Number of operators: 80
0 2449029 0 21
0 2449029 21 41
0 2449029 41 61
0 2449029 61 80
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the partition [0, 21) x [0, 2449029)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_WEIGHT, output tensors: 1
    Op 2: type OPERATOR_MATMUL, output tensors: 2
    Op 3: type OPERATOR_AGGREGATION, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_AGGREGATION, output tensors: 6
    Op 7: type OPERATOR_WEIGHT, output tensors: 7
    Op 8: type OPERATOR_MATMUL, output tensors: 8
    Op 9: type OPERATOR_RELU, output tensors: 9
    Op 10: type OPERATOR_DROPOUT, output tensors: 10
    Op 11: type OPERATOR_AGGREGATION, output tensors: 11
    Op 12: type OPERATOR_WEIGHT, output tensors: 12
    Op 13: type OPERATOR_MATMUL, output tensors: 13
    Op 14: type OPERATOR_RELU, output tensors: 14
    Op 15: type OPERATOR_DROPOUT, output tensors: 15
    Op 16: type OPERATOR_AGGREGATION, output tensors: 16
    Op 17: type OPERATOR_WEIGHT, output tensors: 17
    Op 18: type OPERATOR_MATMUL, output tensors: 18
    Op 19: type OPERATOR_RELU, output tensors: 19
    Op 20: type OPERATOR_DROPOUT, output tensors: 20
    Op 21: type OPERATOR_AGGREGATION, output tensors: 21
    Op 22: type OPERATOR_WEIGHT, output tensors: 22
    Op 23: type OPERATOR_MATMUL, output tensors: 23
    Op 24: type OPERATOR_RELU, output tensors: 24
    Op 25: type OPERATOR_DROPOUT, output tensors: 25
    Op 26: type OPERATOR_AGGREGATION, output tensors: 26
    Op 27: type OPERATOR_WEIGHT, output tensors: 27
    Op 28: type OPERATOR_MATMUL, output tensors: 28
    Op 29: type OPERATOR_RELU, output tensors: 29
    Op 30: type OPERATOR_DROPOUT, output tensors: 30
    Op 31: type OPERATOR_AGGREGATION, output tensors: 31
    Op 32: type OPERATOR_WEIGHT, output tensors: 32
    Op 33: type OPERATOR_MATMUL, output tensors: 33
    Op 34: type OPERATOR_RELU, output tensors: 34
    Op 35: type OPERATOR_DROPOUT, output tensors: 35
    Op 36: type OPERATOR_AGGREGATION, output tensors: 36
    Op 37: type OPERATOR_WEIGHT, output tensors: 37
    Op 38: type OPERATOR_MATMUL, output tensors: 38
    Op 39: type OPERATOR_RELU, output tensors: 39
    Op 40: type OPERATOR_DROPOUT, output tensors: 40
    Op 41: type OPERATOR_AGGREGATION, output tensors: 41
    Op 42: type OPERATOR_WEIGHT, output tensors: 42
    Op 43: type OPERATOR_MATMUL, output tensors: 43
    Op 44: type OPERATOR_RELU, output tensors: 44
    Op 45: type OPERATOR_DROPOUT, output tensors: 45
    Op 46: type OPERATOR_AGGREGATION, output tensors: 46
    Op 47: type OPERATOR_WEIGHT, output tensors: 47
    Op 48: type OPERATOR_MATMUL, output tensors: 48
    Op 49: type OPERATOR_RELU, output tensors: 49
    Op 50: type OPERATOR_DROPOUT, output tensors: 50
    Op 51: type OPERATOR_AGGREGATION, output tensors: 51
    Op 52: type OPERATOR_WEIGHT, output tensors: 52
    Op 53: type OPERATOR_MATMUL, output tensors: 53
    Op 54: type OPERATOR_RELU, output tensors: 54
    Op 55: type OPERATOR_DROPOUT, output tensors: 55
    Op 56: type OPERATOR_AGGREGATION, output tensors: 56
    Op 57: type OPERATOR_WEIGHT, output tensors: 57
    Op 58: type OPERATOR_MATMUL, output tensors: 58
    Op 59: type OPERATOR_RELU, output tensors: 59
    Op 60: type OPERATOR_DROPOUT, output tensors: 60
    Op 61: type OPERATOR_AGGREGATION, output tensors: 61
    Op 62: type OPERATOR_WEIGHT, output tensors: 62
    Op 63: type OPERATOR_MATMUL, output tensors: 63
    Op 64: type OPERATOR_RELU, output tensors: 64
    Op 65: type OPERATOR_DROPOUT, output tensors: 65
    Op 66: type OPERATOR_AGGREGATION, output tensors: 66
    Op 67: type OPERATOR_WEIGHT, output tensors: 67
    Op 68: type OPERATOR_MATMUL, output tensors: 68
    Op 69: type OPERATOR_RELU, output tensors: 69
    Op 70: type OPERATOR_DROPOUT, output tensors: 70
    Op 71: type OPERATOR_AGGREGATION, output tensors: 71
    Op 72: type OPERATOR_WEIGHT, output tensors: 72
    Op 73: type OPERATOR_MATMUL, output tensors: 73
    Op 74: type OPERATOR_RELU, output tensors: 74
    Op 75: type OPERATOR_DROPOUT, output tensors: 75
    Op 76: type OPERATOR_AGGREGATION, output tensors: 76
    Op 77: type OPERATOR_WEIGHT, output tensors: 77
    Op 78: type OPERATOR_MATMUL, output tensors: 78
    Op 79: type OPERATOR_SOFTMAX, output tensors: 79
Boundaries: 0 0 0 0 2449029 2449029 2449029 2449029
Fragments: [0, 2449029)
Chunks (number of global chunks: 12): 0-[0, 204086) 1-[204086, 408172) 2-[408172, 612258) 3-[612258, 816344) 4-[816344, 1020430) 5-[1020430, 1224516) 6-[1224516, 1428602) 7-[1428602, 1632688) 8-[1632688, 1836774) ... 11-[2244946, 2449029)
(Forwarding) Node 0 (fragment 0) depends on nodes:
(Backwarding) Node 0 (fragment 0) depends on nodes: 1 (Tensor: 20)
(I-link dependencies): node 0 should send activation to nodes: 1 (tensor: 20)
(I-link dependencies): node 0 should receive activation from nodes:
(I-link dependencies): node 0 should send gradient to nodes:
(I-link dependencies): node 0 should receive gradient from nodes:
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
WARNING: the current version only applies to linear GNN models!
*** Node 2, starting model training...
Number of operators: 80
0 2449029 0 21
0 2449029 21 41
0 2449029 41 61
0 2449029 61 80
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the partition [41, 61) x [0, 2449029)
*** Node 2, constructing the helper classes...
(Forwarding) Node 2 (fragment 0) depends on nodes: 1 (Tensor: 40)
(Backwarding) Node 2 (fragment 0) depends on nodes: 3 (Tensor: 60)
(I-link dependencies): node 2 should send activation to nodes: 3 (tensor: 60)
(I-link dependencies): node 2 should receive activation from nodes: 1 (tensor: 40)
(I-link dependencies): node 2 should send gradient to nodes:
(I-link dependencies): node 2 should receive gradient from nodes:
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
WARNING: the current version only applies to linear GNN models!
*** Node 1, starting model training...
Number of operators: 80
0 2449029 0 21
0 2449029 21 41
0 2449029 41 61
0 2449029 61 80
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the partition [21, 41) x [0, 2449029)
*** Node 1, constructing the helper classes...
(Forwarding) Node 1 (fragment 0) depends on nodes: 0 (Tensor: 20)
(Backwarding) Node 1 (fragment 0) depends on nodes: 2 (Tensor: 40)
(I-link dependencies): node 1 should send activation to nodes: 2 (tensor: 40)
(I-link dependencies): node 1 should receive activation from nodes: 0 (tensor: 20)
(I-link dependencies): node 1 should send gradient to nodes:
(I-link dependencies): node 1 should receive gradient from nodes:
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
WARNING: the current version only applies to linear GNN models!
*** Node 3, starting model training...
Number of operators: 80
0 2449029 0 21
0 2449029 21 41
0 2449029 41 61
0 2449029 61 80
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the partition [61, 80) x [0, 2449029)
*** Node 3, constructing the helper classes...
(Forwarding) Node 3 (fragment 0) depends on nodes: 2 (Tensor: 60)
(Backwarding) Node 3 (fragment 0) depends on nodes:
(I-link dependencies): node 3 should send activation to nodes:
(I-link dependencies): node 3 should receive activation from nodes: 2 (tensor: 60)
(I-link dependencies): node 3 should send gradient to nodes:
(I-link dependencies): node 3 should receive gradient from nodes:
2449029, 126167053, 126167053
Number of vertices per chunk: 204086
2449029, 126167053, 126167053
Number of vertices per chunk: 204086
2449029, 126167053, 126167053
Number of vertices per chunk: 204086
2449029, 126167053, 126167053
Number of vertices per chunk: 204086
csr in-out ready !*** Node 0, setting up some other necessary information...
csr in-out ready !*** Node 1, setting up some other necessary information...
csr in-out ready !*** Node 3, setting up some other necessary information...
csr in-out ready !*** Node 2, setting up some other necessary information...
*** Node 0, starting the helper threads...
*** Node 1, starting the helper threads...
*** Node 2, starting the helper threads...
*** Node 3, starting the helper threads...
+++++++++ Node 1 initializing the weights for op[21, 41)...
+++++++++ Node 1, mapping weight op 22
+++++++++ Node 1, mapping weight op 27
+++++++++ Node 1, mapping weight op 32
+++++++++ Node 2 initializing the weights for op[41, 61)...
+++++++++ Node 2, mapping weight op 42
+++++++++ Node 1, mapping weight op 37
+++++++++ Node 2, mapping weight op 47
+++++++++ Node 2, mapping weight op 52
+++++++++ Node 2, mapping weight op 57
+++++++++ Node 0 initializing the weights for op[0, 21)...
+++++++++ Node 0, mapping weight op 1
+++++++++ Node 3 initializing the weights for op[61, 80)...
+++++++++ Node 3, mapping weight op 62
+++++++++ Node 3, mapping weight op 67
+++++++++ Node 0, mapping weight op 7
+++++++++ Node 3, mapping weight op 72
+++++++++ Node 0, mapping weight op 12
+++++++++ Node 3, mapping weight op 77
+++++++++ Node 0, mapping weight op 17
RANDOMLY DISPATCH THE CHUNKS...
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...



*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.003000000
The learning rate specified by the user: 0.003000000
The learning rate specified by the user: 0.003000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.003000000
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
The dropout reserved state is 0.0312 of the activation space.
    Epoch 9:	Loss 3.10764	TrainAcc 0.2928	ValidAcc 0.2754	TestAcc 0.2081
    Epoch 19:	Loss 2.56585	TrainAcc 0.2970	ValidAcc 0.2753	TestAcc 0.2109
    Epoch 29:	Loss 2.44530	TrainAcc 0.3068	ValidAcc 0.3017	TestAcc 0.2411
    Epoch 39:	Loss 2.37585	TrainAcc 0.3054	ValidAcc 0.2987	TestAcc 0.2365
    Epoch 49:	Loss 2.28184	TrainAcc 0.3074	ValidAcc 0.3041	TestAcc 0.2508
    Epoch 59:	Loss 2.22979	TrainAcc 0.3070	ValidAcc 0.3032	TestAcc 0.2476
    Epoch 69:	Loss 2.19020	TrainAcc 0.3051	ValidAcc 0.2971	TestAcc 0.2354
    Epoch 79:	Loss 2.17231	TrainAcc 0.3060	ValidAcc 0.2992	TestAcc 0.2366
    Epoch 89:	Loss 2.14680	TrainAcc 0.3062	ValidAcc 0.2985	TestAcc 0.2389
Node 0, Layer-level comm throughput (act): -nan GBps
Node 1, Layer-level comm throughput (act): 7.352 GBps
Node 2, Layer-level comm throughput (act): 10.538 GBps
Node 3, Layer-level comm throughput (act): 9.730 GBps
Node 3, Layer-level comm throughput (grad): -nan GBps
Node 2, Layer-level comm throughput (grad): 7.644 GBps
Node 1, Layer-level comm throughput (grad): 9.366 GBps
Node 0, Layer-level comm throughput (grad): 10.566 GBps
    Epoch 99:	Loss 2.13482	TrainAcc 0.3054	ValidAcc 0.2983	TestAcc 0.2372
Node 0, compression time: 0.232s, compression size: 58.389GB, throughput: 251.614GBps
Node 0, decompression time: 0.344s, compression size: 58.389GB, throughput: 169.637GBps
Node 0, pure compute time: 16.756 s, total compute time: 17.332 s
Node 0, wait_for_task_time: 13.255 s, wait_for_other_gpus_time: 0.001 s
------------------------node id 0,  per-epoch time: 0.316254 s---------------
Node 2, compression time: 0.594s, compression size: 116.779GB, throughput: 196.618GBps
Node 2, decompression time: 3.634s, compression size: 116.779GB, throughput: 32.132GBps
Node 2, pure compute time: 17.239 s, total compute time: 21.468 s
Node 2, wait_for_task_time: 5.912 s, wait_for_other_gpus_time: 0.010 s
------------------------node id 2,  per-epoch time: 0.316254 s---------------
Node 1, compression time: 0.569s, compression size: 116.779GB, throughput: 205.193GBps
Node 1, decompression time: 1.310s, compression size: 116.779GB, throughput: 89.160GBps
Node 1, pure compute time: 15.928 s, total compute time: 17.807 s
Node 1, wait_for_task_time: 11.779 s, wait_for_other_gpus_time: 0.012 s
------------------------node id 1,  per-epoch time: 0.316254 s---------------
Node 3, compression time: 0.354s, compression size: 58.389GB, throughput: 164.810GBps
Node 3, decompression time: 0.362s, compression size: 58.389GB, throughput: 161.371GBps
Node 3, pure compute time: 16.839 s, total compute time: 17.556 s
Node 3, wait_for_task_time: 6.104 s, wait_for_other_gpus_time: 0.011 s
------------------------node id 3,  per-epoch time: 0.316254 s---------------
************ Profiling Results ************
	Bubble: 9.005447 (s) (27.98 percentage)
	Compute: 19.717236 (s) (61.27 percentage)
	GradSync: 0.380159 (s) (1.18 percentage)
	GraphComm: 0.078299 (s) (0.24 percentage)
	Imbalance: 2.563958 (s) (7.97 percentage)
	LayerComm: 0.435713 (s) (1.35 percentage)
	Layer-level communication (cluster-wide, per epoch): 0.454 GB
Highest valid_acc: 0.3041
Target test_acc: 0.2508
Epoch to reach the target acc: 50
[MPI Rank 0] Success 
[MPI Rank 3] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 16
The number of hidden units: 64
The number of training epoches: 0
Learning rate: 0.000000
gcn_inference: /home/x-jchen5/baseline/Mithril_MultiGPU/core/src/cuda/cuda_resource.cc:26: virtual void TensorResourceGPU::map(): Assertion `gpu_grad_ != nullptr' failed.
[g000:3304193] *** Process received signal ***
[g000:3304193] Signal: Aborted (6)
[g000:3304193] Signal code:  (-6)
[g000:3304193] [ 0] /lib64/libc.so.6(+0x4eb80)[0x146a18e1cb80]
[g000:3304193] [ 1] /lib64/libc.so.6(gsignal+0x10f)[0x146a18e1caff]
[g000:3304193] [ 2] /lib64/libc.so.6(abort+0x127)[0x146a18defea5]
[g000:3304193] [ 3] /lib64/libc.so.6(+0x21d79)[0x146a18defd79]
[g000:3304193] [ 4] /lib64/libc.so.6(+0x47456)[0x146a18e15456]
[g000:3304193] [ 5] /home/x-jchen5/baseline/Mithril_MultiGPU/build/applications/single_gpu/gcn_inference(_ZN17TensorResourceGPU3mapEv+0x443)[0x458983]
[g000:3304193] [ 6] /home/x-jchen5/baseline/Mithril_MultiGPU/build/applications/single_gpu/gcn_inference(_ZN28SingleNodeExecutionEngineGPU15model_inferenceEP19AbstractApplicationNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE+0x16b)[0x45519b]
[g000:3304193] [ 7] /home/x-jchen5/baseline/Mithril_MultiGPU/build/applications/single_gpu/gcn_inference(main+0x118e)[0x42278e]
[g000:3304193] [ 8] /lib64/libc.so.6(__libc_start_main+0xe5)[0x146a18e08d85]
[g000:3304193] [ 9] /home/x-jchen5/baseline/Mithril_MultiGPU/build/applications/single_gpu/gcn_inference(_start+0x2e)[0x422d0e]
[g000:3304193] *** End of error message ***
/var/spool/slurm/job1314020/slurm_script: line 35: 3304193 Aborted                 $HOME/baseline/Mithril_MultiGPU/build/applications/single_gpu/gcn_inference --graph $PROJECT/gnn_datasets/reordered/$graph --layers $num_layers --hunits $hunits --weight_file $PROJECT/saved_weights_pipe
