g000.anvil.rcac.purdue.edu
Thu Jan 26 01:06:58 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:01:00.0 Off |                    0 |
| N/A   33C    P0    53W / 400W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2    9) numactl/2.0.14
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0    10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0   11) openmpi/4.0.6
  4) gmp/6.2.1              8) zlib/1.2.11  12) boost/1.74.0

 

[  4%] Built target context
[ 17%] Built target parallel
[ 19%] Built target core
[ 39%] Built target cudahelp
[ 41%] Built target test_mpi_gpu_hybrid
[ 43%] Built target test_cuda_data_compression
[ 45%] Built target test_mpi_gpu_model_parallel
[ 48%] Built target test_cuda_graph
[ 53%] Built target test_cuda_pipeline_parallel
[ 53%] Built target test_mpi_gpu_pipelined_model_parallel
[ 55%] Built target test_cuda
[ 56%] Built target test_trivial
[ 63%] Built target test_mpi_combined
[ 63%] Built target test_cuda_model_parallel
[ 77%] Built target test_nccl_thread
[ 77%] Built target test_hello_world
[ 89%] Built target test_full_structual_graph
[ 89%] Built target test_single_node_training
[ 89%] Built target test_mpi_pipelined_model_parallel
[ 89%] Built target test_nccl_mpi
[ 89%] Built target test_single_node_fullgpu_training
[ 89%] Built target test_mpi_structual_graph
[ 91%] Built target test_graph
[ 89%] Built target test_mpi_non_structual_graph
[ 89%] Built target test_mpi_loader
[ 91%] Built target estimate_comm_volume
[ 91%] Built target test_single_node_gpu_training
[ 89%] Built target test_full_non_structual_graph
[ 91%] Built target test_mpi_model_parallel
[100%] Built target OSDI2023_SINGLE_NODE_gcn_inference
[100%] Built target OSDI2023_MULTI_NODES_gcn
[100%] Built target test_two_layer_hybrid_parallelism_designer
[100%] Built target OSDI2023_SINGLE_NODE_gcn
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_mag
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: saved_weights_pipe
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_mag
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: saved_weights_pipe
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_mag
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: saved_weights_pipe
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_mag
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: saved_weights_pipe
Initialized node 0 on machine g000.anvil.rcac.purdue.edu
Initialized node 1 on machine g002.anvil.rcac.purdue.edu
Initialized node 2 on machine g007.anvil.rcac.purdue.edu
Initialized node 3 on machine g008.anvil.rcac.purdue.edu
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.173 seconds.
Building the CSC structure...
        It takes 0.176 seconds.
Building the CSC structure...
        It takes 0.188 seconds.
Building the CSC structure...
        It takes 0.178 seconds.
Building the CSC structure...
        It takes 0.173 seconds.
        It takes 0.173 seconds.
        It takes 0.175 seconds.
        It takes 0.173 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.216 seconds.
Building the Label Vector...
        It takes 0.218 seconds.
Building the Label Vector...
        It takes 0.232 seconds.
Building the Label Vector...
        It takes 0.233 seconds.
Building the Label Vector...
        It takes 0.530 seconds.
Number of classes: 349
Number of feature dimensions: 128
Number of vertices: 736389
        It takes 0.532 seconds.
Number of classes: 349
Number of feature dimensions: 128
Number of vertices: 736389
        It takes 0.568 seconds.
Number of classes: 349
Number of feature dimensions: 128
Number of vertices: 736389
        It takes 0.578 seconds.
Number of classes: 349
Number of feature dimensions: 128
Number of vertices: 736389
train nodes 629571, valid nodes 64879, test nodes 41939
Number of GPUs: 4
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
*** Node 0, starting model training...
Number of operators: 20
0 736389 0 6
0 736389 6 11
0 736389 11 16
0 736389 16 20
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the partition [0, 6) x [0, 736389)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_WEIGHT, output tensors: 1
    Op 2: type OPERATOR_MATMUL, output tensors: 2
    Op 3: type OPERATOR_AGGREGATION, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_WEIGHT, output tensors: 6
    Op 7: type OPERATOR_MATMUL, output tensors: 7
    Op 8: type OPERATOR_AGGREGATION, output tensors: 8
    Op 9: type OPERATOR_RELU, output tensors: 9
    Op 10: type OPERATOR_DROPOUT, output tensors: 10
    Op 11: type OPERATOR_WEIGHT, output tensors: 11
    Op 12: type OPERATOR_MATMUL, output tensors: 12
    Op 13: type OPERATOR_AGGREGATION, output tensors: 13
    Op 14: type OPERATOR_RELU, output tensors: 14
    Op 15: type OPERATOR_DROPOUT, output tensors: 15
    Op 16: type OPERATOR_WEIGHT, output tensors: 16
    Op 17: type OPERATOR_MATMUL, output tensors: 17
    Op 18: type OPERATOR_AGGREGATION, output tensors: 18
    Op 19: type OPERATOR_SOFTMAX, output tensors: 19
Boundaries: 0 0 0 0 736389 736389 736389 736389
Fragments: [0, 736389)
Chunks (number of global chunks: 32): 0-[0, 23013) 1-[23013, 46026) 2-[46026, 69039) 3-[69039, 92052) 4-[92052, 115065) 5-[115065, 138078) 6-[138078, 161091) 7-[161091, 184104) 8-[184104, 207117) ... 31-[713403, 736389)
(Forwarding) Node 0 (fragment 0) depends on nodes:
(Backwarding) Node 0 (fragment 0) depends on nodes: 1 (Tensor: 5)
(I-link dependencies): node 0 should send activation to nodes:
(I-link dependencies): node 0 should receive activation from nodes:
(I-link dependencies): node 0 should send gradient to nodes:
(I-link dependencies): node 0 should receive gradient from nodes:
736389, 11568931, 11568931
train nodes 629571, valid nodes 64879, test nodes 41939
Number of GPUs: 4
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
*** Node 1, starting model training...
Number of operators: 20
0 736389 0 6
0 736389 6 11
0 736389 11 16
0 736389 16 20
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the partition [6, 11) x [0, 736389)
*** Node 1, constructing the helper classes...
(Forwarding) Node 1 (fragment 0) depends on nodes: 0 (Tensor: 5)
(Backwarding) Node 1 (fragment 0) depends on nodes: 2 (Tensor: 10)
(I-link dependencies): node 1 should send activation to nodes:
(I-link dependencies): node 1 should receive activation from nodes:
(I-link dependencies): node 1 should send gradient to nodes:
(I-link dependencies): node 1 should receive gradient from nodes:
train nodes 629571, valid nodes 64879, test nodes 41939
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
*** Node 3, starting model training...
Number of operators: 20
0 736389 0 6
0 736389 6 11
0 736389 11 16
0 736389 16 20
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the partition [16, 20) x [0, 736389)
*** Node 3, constructing the helper classes...
train nodes 629571, valid nodes 64879, test nodes 41939
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
*** Node 2, starting model training...
Number of operators: 20
0 736389 0 6
0 736389 6 11
0 736389 11 16
0 736389 16 20
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the partition [11, 16) x [0, 736389)
*** Node 2, constructing the helper classes...
WARNING: the current version only applies to linear GNN models!
(Forwarding) Node 3 (fragment 0) depends on nodes: 2 (Tensor: 15)
(Backwarding) Node 3 (fragment 0) depends on nodes:
(I-link dependencies): node 3 should send activation to nodes:
(I-link dependencies): node 3 should receive activation from nodes:
(I-link dependencies): node 3 should send gradient to nodes:
(I-link dependencies): node 3 should receive gradient from nodes:
(Forwarding) Node 2 (fragment 0) depends on nodes: 1 (Tensor: 10)
(Backwarding) Node 2 (fragment 0) depends on nodes: 3 (Tensor: 15)
(I-link dependencies): node 2 should send activation to nodes:
(I-link dependencies): node 2 should receive activation from nodes:
(I-link dependencies): node 2 should send gradient to nodes:
(I-link dependencies): node 2 should receive gradient from nodes:
736389, 11568931, 11568931
736389, 11568931, 11568931
736389, 11568931, 11568931
csr in-out ready !*** Node 0, setting up some other necessary information...
csr in-out ready !*** Node 1, setting up some other necessary information...
csr in-out ready !*** Node 3, setting up some other necessary information...
csr in-out ready !*** Node 2, setting up some other necessary information...
*** Node 0, starting the helper threads...
*** Node 3, starting the helper threads...
*** Node 2, starting the helper threads...
*** Node 1, starting the helper threads...
+++++++++ Node 0 initializing the weights for op[0, 6)...
+++++++++ Node 0, mapping weight op 1
+++++++++ Node 3 initializing the weights for op[16, 20)...
+++++++++ Node 3, mapping weight op 16
+++++++++ Node 2 initializing the weights for op[11, 16)...
+++++++++ Node 2, mapping weight op 11
+++++++++ Node 1 initializing the weights for op[6, 11)...
+++++++++ Node 1, mapping weight op 6
RANDOMLY DISPATCH THE CHUNKS...
*** Node 0, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 1, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
    Epoch 9:	Loss 5.70059	TrainAcc 0.0184	ValidAcc 0.0232	TestAcc 0.0182
    Epoch 19:	Loss 5.18474	TrainAcc 0.0711	ValidAcc 0.0705	TestAcc 0.0631
    Epoch 29:	Loss 5.00017	TrainAcc 0.0952	ValidAcc 0.0945	TestAcc 0.0802
    Epoch 39:	Loss 4.73070	TrainAcc 0.1141	ValidAcc 0.1081	TestAcc 0.1106
    Epoch 49:	Loss 4.48925	TrainAcc 0.1315	ValidAcc 0.1087	TestAcc 0.0925
    Epoch 59:	Loss 4.27908	TrainAcc 0.1443	ValidAcc 0.1352	TestAcc 0.2223
    Epoch 69:	Loss 4.08381	TrainAcc 0.1574	ValidAcc 0.1669	TestAcc 0.2092
    Epoch 79:	Loss 3.94167	TrainAcc 0.1630	ValidAcc 0.1557	TestAcc 0.1795
    Epoch 89:	Loss 3.80482	TrainAcc 0.1781	ValidAcc 0.1948	TestAcc 0.2240
Node 0, Layer-level comm throughput (act): -nan GBps
Node 1, Layer-level comm throughput (act): 10.272 GBps
Node 2, Layer-level comm throughput (act): 10.139 GBps
Node 3, Layer-level comm throughput (act): 10.658 GBps
Node 3, Layer-level comm throughput (grad): -nan GBps
Node 2, Layer-level comm throughput (grad): 10.709 GBps
Node 1, Layer-level comm throughput (grad): 10.350 GBps
Node 0, Layer-level comm throughput (grad): 10.064 GBps
    Epoch 99:	Loss 3.72131	TrainAcc 0.1746	ValidAcc 0.2020	TestAcc 0.2606
Node 0, compression time: 0.271s, compression size: 35.114GB, throughput: 129.567GBps
Node 0, decompression time: 0.653s, compression size: 35.114GB, throughput: 53.812GBps
Node 0, pure compute time: 2.696 s, total compute time: 3.619 s
Node 0, wait_for_task_time: 8.356 s, wait_for_other_gpus_time: 0.001 s
------------------------node id 0,  per-epoch time: 0.126942 s---------------
Node 3, compression time: 0.321s, compression size: 35.114GB, throughput: 109.557GBps
Node 3, decompression time: 0.652s, compression size: 35.114GB, throughput: 53.866GBps
Node 3, pure compute time: 9.561 s, total compute time: 10.533 s
Node 3, wait_for_task_time: 0.394 s, wait_for_other_gpus_time: 0.002 s
------------------------node id 3,  per-epoch time: 0.126942 s---------------
Node 2, compression time: 0.564s, compression size: 70.228GB, throughput: 124.460GBps
Node 2, decompression time: 1.386s, compression size: 70.228GB, throughput: 50.664GBps
Node 2, pure compute time: 2.006 s, total compute time: 3.957 s
Node 2, wait_for_task_time: 7.591 s, wait_for_other_gpus_time: 0.003 s
------------------------node id 2,  per-epoch time: 0.126942 s---------------
Node 1, compression time: 0.565s, compression size: 70.228GB, throughput: 124.226GBps
Node 1, decompression time: 1.636s, compression size: 70.228GB, throughput: 42.923GBps
Node 1, pure compute time: 2.062 s, total compute time: 4.263 s
Node 1, wait_for_task_time: 7.502 s, wait_for_other_gpus_time: 0.003 s
------------------------node id 1,  per-epoch time: 0.126941 s---------------
ERROR: undercount the overhead: breakdown sum / all time: 0.453
************ Profiling Results ************
	Bubble: 0.437311 (s) (7.36 percentage)
	Compute: 4.247429 (s) (71.48 percentage)
	GradSync: 0.059530 (s) (1.00 percentage)
	GraphComm: 0.004583 (s) (0.08 percentage)
	Imbalance: 0.802362 (s) (13.50 percentage)
	LayerComm: 0.390965 (s) (6.58 percentage)
ERROR: undercount the overhead: breakdown sum / all time: 0.453
	Layer-level communication (cluster-wide, per epoch): 0.792 GB
Highest valid_acc: 0.2020
Target test_acc: 0.2606
Epoch to reach the target acc: 100
ERROR: undercount the overhead: breakdown sum / all time: 0.453
ERROR: undercount the overhead: breakdown sum / all time: 0.453
[MPI Rank 0] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
[MPI Rank 1] Success 
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_mag
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 0
Learning rate: 0.000000
Initialized node g000.anvil.rcac.purdue.edu
Building the CSR structure...
        It takes 0.181 seconds.
Building the CSC structure...
        It takes 0.175 seconds.
Building the Feature Vector...
        It takes 0.223 seconds.
Building the Label Vector...
        It takes 0.543 seconds.
Number of classes: 349
Number of feature dimensions: 128
Dropout: 0.000 
train nodes 629571, valid nodes 64879, test nodes 41939
*** Allocating resources for all tensors...
    OP_TYPE: OPERATOR_INPUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_SOFTMAX
*** Done allocating resource.
*** Preparing the input tensor...
*** Done preparing the input tensor.
*** Preparing the STD tensor...
    Number of labels: 349
    Number of vertices: 736389
*** Done preparing the STD tensor.
Version 0	TrainAcc 0.0742	ValidAcc 0.0884	TestAcc 0.0882
Version 1	TrainAcc 0.0879	ValidAcc 0.0811	TestAcc 0.0785
Version 2	TrainAcc 0.1037	ValidAcc 0.0992	TestAcc 0.0808
Version 3	TrainAcc 0.1312	ValidAcc 0.1087	TestAcc 0.0951
Version 4	TrainAcc 0.1409	ValidAcc 0.1112	TestAcc 0.0867
Version 5	TrainAcc 0.1544	ValidAcc 0.1409	TestAcc 0.2238
Version 6	TrainAcc 0.1614	ValidAcc 0.1792	TestAcc 0.2260
Version 7	TrainAcc 0.1812	ValidAcc 0.1668	TestAcc 0.1959
Version 8	TrainAcc 0.1902	ValidAcc 0.1803	TestAcc 0.2156
Version 9	TrainAcc 0.1879	ValidAcc 0.2234	TestAcc 0.2790
Version 9 achieved the highest validation accuracy 0.2234 (test accuracy: 0.2790)
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/metis_32_chunks/ogbn_mag
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: saved_weights_pipe
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/metis_32_chunks/ogbn_mag
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: saved_weights_pipe
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/metis_32_chunks/ogbn_mag
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: saved_weights_pipe
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/metis_32_chunks/ogbn_mag
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: saved_weights_pipe
Initialized node 3 on machine g008.anvil.rcac.purdue.edu
Initialized node 2 on machine g007.anvil.rcac.purdue.edu
Initialized node 0 on machine g000.anvil.rcac.purdue.edu
Initialized node 1 on machine g002.anvil.rcac.purdue.edu
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.187 seconds.
Building the CSC structure...
        It takes 0.190 seconds.
Building the CSC structure...
        It takes 0.185 seconds.
Building the CSC structure...
        It takes 0.193 seconds.
Building the CSC structure...
        It takes 0.166 seconds.
        It takes 0.170 seconds.
        It takes 0.173 seconds.
        It takes 0.175 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.219 seconds.
Building the Label Vector...
        It takes 0.217 seconds.
Building the Label Vector...
        It takes 0.230 seconds.
Building the Label Vector...
        It takes 0.237 seconds.
Building the Label Vector...
        It takes 0.534 seconds.
Number of classes: 349
Number of feature dimensions: 128
Number of vertices: 736389
        It takes 0.537 seconds.
Number of classes: 349
Number of feature dimensions: 128
Number of vertices: 736389
        It takes 0.565 seconds.
Number of classes: 349
Number of feature dimensions: 128
Number of vertices: 736389
        It takes 0.579 seconds.
Number of classes: 349
Number of feature dimensions: 128
Number of vertices: 736389
train nodes 629571, valid nodes 64879, test nodes 41939
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
*** Node 0, starting model training...
Number of operators: 20
0 736389 0 6
0 736389 6 11
0 736389 11 16
0 736389 16 20
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the partition [0, 6) x [0, 736389)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_WEIGHT, output tensors: 1
    Op 2: type OPERATOR_MATMUL, output tensors: 2
    Op 3: type OPERATOR_AGGREGATION, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_WEIGHT, output tensors: 6
    Op 7: type OPERATOR_MATMUL, output tensors: 7
    Op 8: type OPERATOR_AGGREGATION, output tensors: 8
    Op 9: type OPERATOR_RELU, output tensors: 9
    Op 10: type OPERATOR_DROPOUT, output tensors: 10
    Op 11: type OPERATOR_WEIGHT, output tensors: 11
    Op 12: type OPERATOR_MATMUL, output tensors: 12
    Op 13: type OPERATOR_AGGREGATION, output tensors: 13
    Op 14: type OPERATOR_RELU, output tensors: 14
    Op 15: type OPERATOR_DROPOUT, output tensors: 15
    Op 16: type OPERATOR_WEIGHT, output tensors: 16
    Op 17: type OPERATOR_MATMUL, output tensors: 17
    Op 18: type OPERATOR_AGGREGATION, output tensors: 18
    Op 19: type OPERATOR_SOFTMAX, output tensors: 19
Boundaries: 0 0 0 0 736389 736389 736389 736389
Fragments: [0, 736389)
Chunks (number of global chunks: 32): 0-[0, 23013) 1-[23013, 46026) 2-[46026, 69039) 3-[69039, 92052) 4-[92052, 115065) 5-[115065, 138078) 6-[138078, 161091) 7-[161091, 184104) 8-[184104, 207117) ... 31-[713403, 736389)
(Forwarding) Node 0 (fragment 0) depends on nodes:
(Backwarding) Node 0 (fragment 0) depends on nodes: 1 (Tensor: 5)
(I-link dependencies): node 0 should send activation to nodes:
(I-link dependencies): node 0 should receive activation from nodes:
(I-link dependencies): node 0 should send gradient to nodes:
(I-link dependencies): node 0 should receive gradient from nodes:
736389, 11568931, 11568931
train nodes 629571, valid nodes 64879, test nodes 41939
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
*** Node 3, starting model training...
Number of operators: 20
0 736389 0 6
0 736389 6 11
0 736389 11 16
0 736389 16 20
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the partition [16, 20) x [0, 736389)
*** Node 3, constructing the helper classes...
WARNING: the current version only applies to linear GNN models!
(Forwarding) Node 3 (fragment 0) depends on nodes: 2 (Tensor: 15)
(Backwarding) Node 3 (fragment 0) depends on nodes:
(I-link dependencies): node 3 should send activation to nodes:
(I-link dependencies): node 3 should receive activation from nodes:
(I-link dependencies): node 3 should send gradient to nodes:
(I-link dependencies): node 3 should receive gradient from nodes:
train nodes 629571, valid nodes 64879, test nodes 41939
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
*** Node 1, starting model training...
Number of operators: 20
0 736389 0 6
0 736389 6 11
0 736389 11 16
0 736389 16 20
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the partition [6, 11) x [0, 736389)
*** Node 1, constructing the helper classes...
(Forwarding) Node 1 (fragment 0) depends on nodes: 0 (Tensor: 5)
(Backwarding) Node 1 (fragment 0) depends on nodes: 2 (Tensor: 10)
(I-link dependencies): node 1 should send activation to nodes:
(I-link dependencies): node 1 should receive activation from nodes:
(I-link dependencies): node 1 should send gradient to nodes:
(I-link dependencies): node 1 should receive gradient from nodes:
736389, 11568931, 11568931
train nodes 629571, valid nodes 64879, test nodes 41939
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
*** Node 2, starting model training...
Number of operators: 20
0 736389 0 6
0 736389 6 11
0 736389 11 16
0 736389 16 20
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the partition [11, 16) x [0, 736389)
*** Node 2, constructing the helper classes...
(Forwarding) Node 2 (fragment 0) depends on nodes: 1 (Tensor: 10)
(Backwarding) Node 2 (fragment 0) depends on nodes: 3 (Tensor: 15)
(I-link dependencies): node 2 should send activation to nodes:
(I-link dependencies): node 2 should receive activation from nodes:
(I-link dependencies): node 2 should send gradient to nodes:
(I-link dependencies): node 2 should receive gradient from nodes:
736389, 11568931, 11568931
736389, 11568931, 11568931
csr in-out ready !*** Node 0, setting up some other necessary information...
csr in-out ready !*** Node 3, setting up some other necessary information...
csr in-out ready !*** Node 1, setting up some other necessary information...
csr in-out ready !*** Node 2, setting up some other necessary information...
*** Node 0, starting the helper threads...
*** Node 3, starting the helper threads...
*** Node 1, starting the helper threads...
*** Node 2, starting the helper threads...
+++++++++ Node 0 initializing the weights for op[0, 6)...
+++++++++ Node 0, mapping weight op 1
+++++++++ Node 2 initializing the weights for op[11, 16)...
+++++++++ Node 2, mapping weight op 11
+++++++++ Node 3 initializing the weights for op[16, 20)...
+++++++++ Node 3, mapping weight op 16
+++++++++ Node 1 initializing the weights for op[6, 11)...
+++++++++ Node 1, mapping weight op 6
RANDOMLY DISPATCH THE CHUNKS...
*** Node 0, starting task scheduling...



*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
    Epoch 9:	Loss 5.69740	TrainAcc 0.0479	ValidAcc 0.0354	TestAcc 0.0345
    Epoch 19:	Loss 5.16818	TrainAcc 0.0864	ValidAcc 0.0449	TestAcc 0.0621
    Epoch 29:	Loss 4.97050	TrainAcc 0.0997	ValidAcc 0.0521	TestAcc 0.0664
    Epoch 39:	Loss 4.68398	TrainAcc 0.1221	ValidAcc 0.0844	TestAcc 0.1084
    Epoch 49:	Loss 4.43643	TrainAcc 0.1318	ValidAcc 0.0836	TestAcc 0.1033
    Epoch 59:	Loss 4.22478	TrainAcc 0.1495	ValidAcc 0.1176	TestAcc 0.1434
    Epoch 69:	Loss 4.04435	TrainAcc 0.1590	ValidAcc 0.1405	TestAcc 0.1655
    Epoch 79:	Loss 3.93498	TrainAcc 0.1601	ValidAcc 0.1500	TestAcc 0.1689
    Epoch 89:	Loss 3.81590	TrainAcc 0.1734	ValidAcc 0.2050	TestAcc 0.2341
Node 0, Layer-level comm throughput (act): -nan GBps
Node 1, Layer-level comm throughput (act): 10.323 GBps
Node 2, Layer-level comm throughput (act): 10.089 GBps
Node 3, Layer-level comm throughput (act): 10.689 GBps
Node 3, Layer-level comm throughput (grad): -nan GBps
Node 2, Layer-level comm throughput (grad): 10.798 GBps
Node 1, Layer-level comm throughput (grad): 10.386 GBps
Node 0, Layer-level comm throughput (grad): 10.211 GBps
    Epoch 99:	Loss 3.72705	TrainAcc 0.1799	ValidAcc 0.1809	TestAcc 0.2012
Node 0, compression time: 0.269s, compression size: 35.114GB, throughput: 130.497GBps
Node 0, decompression time: 0.664s, compression size: 35.114GB, throughput: 52.914GBps
Node 0, pure compute time: 2.686 s, total compute time: 3.619 s
Node 0, wait_for_task_time: 8.422 s, wait_for_other_gpus_time: 0.001 s
------------------------node id 0,  per-epoch time: 0.129694 s---------------
Node 1, compression time: 0.551s, compression size: 70.228GB, throughput: 127.400GBps
Node 1, decompression time: 1.586s, compression size: 70.228GB, throughput: 44.273GBps
Node 1, pure compute time: 1.988 s, total compute time: 4.125 s
Node 1, wait_for_task_time: 7.769 s, wait_for_other_gpus_time: 0.002 s
------------------------node id 1,  per-epoch time: 0.129693 s---------------
Node 2, compression time: 0.563s, compression size: 70.228GB, throughput: 124.634GBps
Node 2, decompression time: 1.418s, compression size: 70.228GB, throughput: 49.520GBps
Node 2, pure compute time: 1.975 s, total compute time: 3.957 s
Node 2, wait_for_task_time: 7.750 s, wait_for_other_gpus_time: 0.002 s
------------------------node id 2,  per-epoch time: 0.129694 s---------------
Node 3, compression time: 0.310s, compression size: 35.114GB, throughput: 113.323GBps
Node 3, decompression time: 0.651s, compression size: 35.114GB, throughput: 53.916GBps
Node 3, pure compute time: 9.760 s, total compute time: 10.722 s
Node 3, wait_for_task_time: 0.351 s, wait_for_other_gpus_time: 0.002 s
------------------------node id 3,  per-epoch time: 0.129694 s---------------
ERROR: undercount the overhead: breakdown sum / all time: 0.445
************ Profiling Results ************
	Bubble: 0.418035 (s) (7.14 percentage)
	Compute: 4.266588 (s) (72.92 percentage)
	GradSync: 0.051724 (s) (0.88 percentage)
	GraphComm: 0.003965 (s) (0.07 percentage)
	Imbalance: 0.737329 (s) (12.60 percentage)
	LayerComm: 0.373523 (s) (6.38 percentage)
ERROR: undercount the overhead: breakdown sum / all time: 0.445
	Layer-level communication (cluster-wide, per epoch): 0.797 GB
Highest valid_acc: 0.2050
Target test_acc: 0.2341
Epoch to reach the target acc: 90
ERROR: undercount the overhead: breakdown sum / all time: 0.445
ERROR: undercount the overhead: breakdown sum / all time: 0.445
[MPI Rank 0] Success 
[MPI Rank 2] Success 
[MPI Rank 1] Success 
[MPI Rank 3] Success 
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_mag
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 0
Learning rate: 0.000000
Initialized node g000.anvil.rcac.purdue.edu
Building the CSR structure...
        It takes 0.172 seconds.
Building the CSC structure...
        It takes 0.169 seconds.
Building the Feature Vector...
        It takes 0.216 seconds.
Building the Label Vector...
        It takes 0.538 seconds.
Number of classes: 349
Number of feature dimensions: 128
Dropout: 0.000 
train nodes 629571, valid nodes 64879, test nodes 41939
*** Allocating resources for all tensors...
    OP_TYPE: OPERATOR_INPUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_SOFTMAX
*** Done allocating resource.
*** Preparing the input tensor...
*** Done preparing the input tensor.
*** Preparing the STD tensor...
    Number of labels: 349
    Number of vertices: 736389
*** Done preparing the STD tensor.
Version 0	TrainAcc 0.0909	ValidAcc 0.0987	TestAcc 0.2004
Version 1	TrainAcc 0.0813	ValidAcc 0.0747	TestAcc 0.0778
Version 2	TrainAcc 0.1034	ValidAcc 0.0988	TestAcc 0.0808
Version 3	TrainAcc 0.1263	ValidAcc 0.1108	TestAcc 0.1571
Version 4	TrainAcc 0.1421	ValidAcc 0.1149	TestAcc 0.0989
Version 5	TrainAcc 0.1524	ValidAcc 0.1476	TestAcc 0.2258
Version 6	TrainAcc 0.1523	ValidAcc 0.1676	TestAcc 0.2297
Version 7	TrainAcc 0.1694	ValidAcc 0.1648	TestAcc 0.2233
Version 8	TrainAcc 0.1900	ValidAcc 0.2122	TestAcc 0.2856
Version 9	TrainAcc 0.1818	ValidAcc 0.2185	TestAcc 0.2717
Version 9 achieved the highest validation accuracy 0.2185 (test accuracy: 0.2717)
