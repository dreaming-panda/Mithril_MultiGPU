g010.anvil.rcac.purdue.edu
Sun Jun  4 00:17:13 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:81:00.0 Off |                    0 |
| N/A   36C    P0    51W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2       9) zlib/1.2.11
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0       10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0      11) openmpi/4.0.6
  4) gmp/6.2.1              8) numactl/2.0.14  12) boost/1.74.0

 

Consolidate compiler generated dependencies of target cudahelp
[ 11%] Built target context
[ 36%] Built target core
[ 77%] Built target cudahelp
Consolidate compiler generated dependencies of target OSDI2023_MULTI_NODES_gcnii
[ 83%] Built target estimate_comm_volume
[ 94%] Built target OSDI2023_MULTI_NODES_graphsage
[ 94%] Built target OSDI2023_MULTI_NODES_gcn
[ 97%] Building CXX object applications/async_multi_gpus/CMakeFiles/OSDI2023_MULTI_NODES_gcnii.dir/gcnii.cc.o
[100%] Linking CXX executable gcnii
[100%] Built target OSDI2023_MULTI_NODES_gcnii
Initialized node 1 on machine g011.anvil.rcac.purdue.edu
Initialized node 2 on machine g012.anvil.rcac.purdue.edu
Initialized node 0 on machine g010.anvil.rcac.purdue.edu
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.949 seconds.
Building the CSC structure...
        It takes 1.988 seconds.
Building the CSC structure...
        It takes 2.943 seconds.
Building the CSC structure...
        It takes 1.919 seconds.
        It takes 1.960 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 2.382 seconds.
        It takes 0.631 seconds.
Building the Label Vector...
        It takes 0.633 seconds.
Building the Label Vector...
        It takes 0.344 seconds.
        It takes 0.345 seconds.
Building the Feature Vector...
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
*** Node 1, starting model training...
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [13, 20)
*** Node 1, constructing the helper classes...
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
*** Node 2, starting model training...
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [20, 30)
*** Node 2, constructing the helper classes...
2449029, 126167053, 126167053
Number of vertices per chunk: 153065
2449029, 126167053, 126167053
Number of vertices per chunk: 153065
        It takes 3.386 seconds.
Building the Label Vector...
        It takes 0.977 seconds.
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/partitioned_graphs/ogbn_products/16_parts
The number of GCNII layers: 3
The number of hidden units: 128
The number of training epoches: 100
Learning rate: 0.003000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
Number of GPUs: 3
train nodes 196615, valid nodes 39323, test nodes 2213091
GPU 0, layer [0, 2)
GPU 1, layer [2, 3)
GPU 2, layer [3, 4)
*** Node 0, starting model training...
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 13)
*** Node 0, constructing the helper classes...
WARNING: the current version only applies to linear GNN models!
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_DROPOUT, output tensors: 1
    Op 2: type OPERATOR_WEIGHT, output tensors: 2
    Op 3: type OPERATOR_MATMUL, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_AGGREGATION, output tensors: 6
    Op 7: type OPERATOR_ADD, output tensors: 7
    Op 8: type OPERATOR_WEIGHT, output tensors: 8
    Op 9: type OPERATOR_MATMUL, output tensors: 9
    Op 10: type OPERATOR_ADD, output tensors: 10
    Op 11: type OPERATOR_RELU, output tensors: 11
    Op 12: type OPERATOR_DROPOUT, output tensors: 12
    Op 13: type OPERATOR_AGGREGATION, output tensors: 13
    Op 14: type OPERATOR_ADD, output tensors: 14
    Op 15: type OPERATOR_WEIGHT, output tensors: 15
    Op 16: type OPERATOR_MATMUL, output tensors: 16
    Op 17: type OPERATOR_ADD, output tensors: 17
    Op 18: type OPERATOR_RELU, output tensors: 18
    Op 19: type OPERATOR_DROPOUT, output tensors: 19
    Op 20: type OPERATOR_AGGREGATION, output tensors: 20
    Op 21: type OPERATOR_ADD, output tensors: 21
    Op 22: type OPERATOR_WEIGHT, output tensors: 22
    Op 23: type OPERATOR_MATMUL, output tensors: 23
    Op 24: type OPERATOR_ADD, output tensors: 24
    Op 25: type OPERATOR_RELU, output tensors: 25
    Op 26: type OPERATOR_DROPOUT, output tensors: 26
    Op 27: type OPERATOR_WEIGHT, output tensors: 27
    Op 28: type OPERATOR_MATMUL, output tensors: 28
    Op 29: type OPERATOR_SOFTMAX, output tensors: 29
Chunks (number of global chunks: 16): 0-[0, 157663) 1-[157663, 315326) 2-[315326, 466495) 3-[466495, 624158) 4-[624158, 781821) 5-[781821, 939484) 6-[939484, 1092356) 7-[1092356, 1247314) 8-[1247314, 1395913) ... 15-[2298008, 2449029)
2449029, 126167053, 126167053
Number of vertices per chunk: 153065
csr in-out ready !*** Node 0, setting up some other necessary information...
csr in-out ready !*** Node 1, setting up some other necessary information...
csr in-out ready !*** Node 2, setting up some other necessary information...
*** Node 2, starting the helper threads...
+++++++++ Node 2 initializing the weights for op[20, 30)...
+++++++++ Node 2, mapping weight op 22
+++++++++ Node 2, mapping weight op 27
*** Node 0, starting the helper threads...
+++++++++ Node 0 initializing the weights for op[0, 13)...
+++++++++ Node 0, mapping weight op 2
+++++++++ Node 0, mapping weight op 8
*** Node 1, starting the helper threads...
+++++++++ Node 1 initializing the weights for op[13, 20)...
+++++++++ Node 1, mapping weight op 15
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.003000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.003000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.003000000
	Epoch 10:	Loss 2.4281
	Epoch 20:	Loss 1.5543
	Epoch 30:	Loss 1.1472
	Epoch 40:	Loss 0.9305
	Epoch 50:	Loss 0.8114
	Epoch 60:	Loss 0.7387
	Epoch 70:	Loss 0.6916
	Epoch 80:	Loss 0.6584
	Epoch 90:	Loss 0.6321
	Epoch 100:	Loss 0.6116
Node 0, Layer-level comm throughput (act): -nan GBps
Node 1, Layer-level comm throughput (act): 11.337 GBps
Node 2, Layer-level comm throughput (act): 11.397 GBps
Node 2, Layer-level comm throughput (grad): -nan GBps
Node 1, Layer-level comm throughput (grad): 11.378 GBps
Node 0, Layer-level comm throughput (grad): 11.386 GBps
Node 0, GPU memory consumption: 11.708 GB
------------------------node id 0,  per-epoch time: 0.473951 s---------------
Node 1, GPU memory consumption: 9.937 GB
------------------------node id 1,  per-epoch time: 0.473950 s---------------
Node 2, GPU memory consumption: 11.304 GB
------------------------node id 2,  per-epoch time: 0.473950 s---------------
************ Profiling Results ************
	Bubble: 9.393957 (s) (19.67 percentage)
	Compression: 1.350770 (s) (2.83 percentage)
	Compute: 15.150788 (s) (31.72 percentage)
	GraphCommGPUCPU: 0.022089 (s) (0.05 percentage)
	GraphCommNetwork: 0.006593 (s) (0.01 percentage)
	LayerCommGPUCPU: 17.545774 (s) (36.73 percentage)
	LayerCommNetwork: 3.709193 (s) (7.76 percentage)
	Optimization: 0.043235 (s) (0.09 percentage)
	Other: 0.545788 (s) (1.14 percentage)
	Layer-level communication (cluster-wide, per-epoch): 6.153 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.001 GB
	Total communication (cluster-wide, per-epoch): 6.154 GB
Highest valid_acc: 0.0000
Target test_acc: 0.0455
Epoch to reach the target acc: 0
[MPI Rank 2] Success 
[MPI Rank 0] Success 
[MPI Rank 1] Success 

 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  7.58ms  2.53ms  2.33ms  3.25  8.64K  3.63M
 chk_1  2.34ms  2.04ms  1.89ms  1.24  4.01K  3.86M
 chk_2  2.60ms  2.25ms  2.06ms  1.26  8.41K  3.64M
 chk_3  2.26ms  2.02ms  1.91ms  1.18  3.83K  3.70M
 chk_4  2.35ms  2.06ms  1.90ms  1.24  7.19K  3.47M
 chk_5  2.51ms  2.10ms  1.89ms  1.33 10.58K  3.43M
 chk_6  2.40ms  2.03ms  1.83ms  1.31 10.12K  3.50M
 chk_7  2.30ms  2.10ms  1.94ms  1.18  5.51K  3.56M
 chk_8  2.24ms  2.04ms  1.90ms  1.18  5.00K  3.58M
 chk_9  2.23ms  2.06ms  1.95ms  1.14  4.38K  3.61M
chk_10  2.42ms  2.26ms  2.15ms  1.13  3.53K  3.89M
chk_11  2.42ms  2.36ms  2.09ms  1.16  5.27K  3.57M
chk_12  2.20ms  2.02ms  1.90ms  1.16  4.76K  3.62M
chk_13  2.43ms  2.23ms  2.09ms  1.16  5.58K  3.55M
chk_14  2.48ms  2.25ms  2.09ms  1.19  6.61K  3.50M
chk_15  2.20ms  2.06ms  1.94ms  1.13  3.63K  3.72M
chk_16  3.29ms  2.09ms  1.87ms  1.76 12.44K  3.44M
chk_17  2.52ms  2.00ms  1.69ms  1.49 20.07K  3.00M
chk_18  2.22ms  1.91ms  1.73ms  1.29  9.44K  3.36M
chk_19  2.30ms  2.05ms  1.92ms  1.20  5.16K  3.57M
chk_20  2.39ms  2.11ms  1.93ms  1.24  8.54K  3.64M
chk_21  2.30ms  2.08ms  1.91ms  1.21  6.57K  3.50M
chk_22  2.48ms  2.20ms  1.98ms  1.25  9.58K  3.59M
chk_23  2.53ms  2.28ms  2.09ms  1.21  7.82K  3.67M
chk_24  2.39ms  2.16ms  2.01ms  1.19  6.02K  3.76M
chk_25  2.39ms  2.16ms  1.97ms  1.21  7.34K  3.70M
chk_26  2.31ms  2.05ms  1.83ms  1.26  9.03K  3.61M
chk_27  2.35ms  2.08ms  1.86ms  1.26  9.53K  3.35M
chk_28  2.76ms  2.49ms  2.28ms  1.21  8.87K  3.62M
chk_29  2.35ms  2.15ms  2.01ms  1.17  5.26K  3.57M
chk_30  2.33ms  2.13ms  1.99ms  1.17  6.03K  3.76M
chk_31  2.19ms  2.02ms  1.89ms  1.16  4.23K  3.62M
 
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.75ms  2.45ms  2.33ms  1.18  8.64K  3.63M
 chk_1  2.19ms  2.00ms  1.93ms  1.14  4.01K  3.86M
 chk_2  2.45ms  2.18ms  2.05ms  1.19  8.41K  3.64M
 chk_3  2.05ms  1.94ms  1.88ms  1.09  3.83K  3.70M
 chk_4  2.15ms  1.97ms  1.87ms  1.15  7.19K  3.47M
 chk_5  2.27ms  2.00ms  1.86ms  1.22 10.58K  3.43M
 chk_6  2.18ms  1.93ms  1.80ms  1.21 10.12K  3.50M
 chk_7  2.13ms  1.99ms  1.92ms  1.11  5.51K  3.56M
 chk_8  2.08ms  1.94ms  1.88ms  1.11  5.00K  3.58M
 chk_9  2.09ms  1.97ms  1.90ms  1.10  4.38K  3.61M
chk_10  2.29ms  2.18ms  2.11ms  1.08  3.53K  3.89M
chk_11  2.27ms  2.13ms  2.05ms  1.10  5.27K  3.57M
chk_12  2.04ms  1.93ms  1.86ms  1.10  4.76K  3.62M
chk_13  2.28ms  2.14ms  2.06ms  1.11  5.58K  3.55M
chk_14  2.33ms  2.17ms  2.06ms  1.13  6.61K  3.50M
chk_15  2.00ms  1.97ms  1.90ms  1.05  3.63K  3.72M
chk_16  2.31ms  1.99ms  1.83ms  1.26 12.44K  3.44M
chk_17  2.40ms  1.90ms  1.65ms  1.45 20.07K  3.00M
chk_18  2.05ms  1.81ms  1.70ms  1.21  9.44K  3.36M
chk_19  2.10ms  1.95ms  1.87ms  1.12  5.16K  3.57M
chk_20  2.29ms  2.06ms  1.95ms  1.17  8.54K  3.64M
chk_21  2.18ms  2.05ms  1.92ms  1.13  6.57K  3.50M
chk_22  2.37ms  2.13ms  2.02ms  1.17  9.58K  3.59M
chk_23  2.43ms  2.32ms  2.12ms  1.15  7.82K  3.67M
chk_24  2.25ms  2.07ms  2.03ms  1.11  6.02K  3.76M
chk_25  2.25ms  2.04ms  1.94ms  1.16  7.34K  3.70M
chk_26  2.16ms  1.93ms  1.80ms  1.20  9.03K  3.61M
chk_27  2.21ms  1.98ms  1.83ms  1.21  9.53K  3.35M
chk_28  2.61ms  2.40ms  2.26ms  1.15  8.87K  3.62M
chk_29  2.20ms  2.06ms  1.97ms  1.12  5.26K  3.57M
chk_30  2.19ms  2.04ms  1.95ms  1.13  6.03K  3.76M
chk_31  2.04ms  1.92ms  1.84ms  1.11  4.23K  3.62M
 
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.75ms  2.47ms  2.33ms  1.18  8.64K  3.63M
 chk_1  2.19ms  2.03ms  1.95ms  1.12  4.01K  3.86M
 chk_2  2.46ms  2.21ms  2.07ms  1.19  8.41K  3.64M
 chk_3  2.11ms  1.98ms  1.91ms  1.11  3.83K  3.70M
 chk_4  2.18ms  2.01ms  1.89ms  1.15  7.19K  3.47M
 chk_5  2.31ms  2.04ms  1.89ms  1.22 10.58K  3.43M
 chk_6  2.22ms  1.97ms  1.83ms  1.21 10.12K  3.50M
 chk_7  2.15ms  2.03ms  1.94ms  1.11  5.51K  3.56M
 chk_8  2.12ms  1.98ms  1.90ms  1.12  5.00K  3.58M
 chk_9  2.13ms  2.00ms  1.93ms  1.10  4.38K  3.61M
chk_10  2.49ms  2.20ms  2.16ms  1.16  3.53K  3.89M
chk_11  2.28ms  2.17ms  2.09ms  1.09  5.27K  3.57M
chk_12  2.07ms  1.98ms  1.88ms  1.10  4.76K  3.62M
chk_13  2.30ms  2.17ms  2.08ms  1.10  5.58K  3.55M
chk_14  2.36ms  2.19ms  2.08ms  1.13  6.61K  3.50M
chk_15  2.08ms  1.98ms  1.94ms  1.08  3.63K  3.72M
chk_16  2.34ms  2.02ms  1.86ms  1.26 12.44K  3.44M
chk_17  2.43ms  1.93ms  1.68ms  1.44 20.07K  3.00M
chk_18  2.09ms  1.84ms  1.72ms  1.22  9.44K  3.36M
chk_19  2.15ms  1.98ms  1.90ms  1.13  5.16K  3.57M
chk_20  2.29ms  2.05ms  1.94ms  1.18  8.54K  3.64M
chk_21  2.21ms  2.04ms  1.93ms  1.14  6.57K  3.50M
chk_22  2.42ms  2.14ms  2.02ms  1.20  9.58K  3.59M
chk_23  2.45ms  2.22ms  2.13ms  1.15  7.82K  3.67M
chk_24  2.30ms  2.11ms  2.05ms  1.12  6.02K  3.76M
chk_25  2.26ms  2.08ms  1.98ms  1.14  7.34K  3.70M
chk_26  2.18ms  1.96ms  1.82ms  1.20  9.03K  3.61M
chk_27  2.26ms  1.99ms  1.85ms  1.22  9.53K  3.35M
chk_28  2.63ms  2.41ms  2.28ms  1.15  8.87K  3.62M
chk_29  2.23ms  2.09ms  2.00ms  1.11  5.26K  3.57M
chk_30  2.23ms  2.08ms  1.98ms  1.12  6.03K  3.76M
chk_31  2.07ms  1.95ms  1.88ms  1.10  4.23K  3.62M
 
