g005.anvil.rcac.purdue.edu
Thu Jan 19 18:59:54 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:81:00.0 Off |                    0 |
| N/A   30C    P0    50W / 400W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2    9) numactl/2.0.14
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0    10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0   11) openmpi/4.0.6
  4) gmp/6.2.1              8) zlib/1.2.11  12) boost/1.74.0

 

[  9%] Built target core
[ 17%] Built target parallel
[ 19%] Built target context
[ 39%] Built target cudahelp
[ 45%] Built target test_cuda_graph
[ 54%] Built target test_mpi_gpu_hybrid
[ 54%] Built target test_mpi_gpu_pipelined_model_parallel
[ 54%] Built target test_mpi_gpu_model_parallel
[ 70%] Built target test_graph
[ 70%] Built target test_cuda
[ 70%] Built target test_cuda_model_parallel
[ 70%] Built target test_mpi_combined
[ 70%] Built target test_single_node_fullgpu_training
[ 84%] Built target test_full_non_structual_graph
[ 84%] Built target test_nccl_mpi
[ 84%] Built target test_full_structual_graph
[ 84%] Built target test_single_node_training
[ 84%] Built target test_mpi_loader
[ 84%] Built target test_cuda_pipeline_parallel
[ 84%] Built target test_trivial
[ 84%] Built target test_single_node_gpu_training
[ 84%] Built target test_cuda_data_compression
[ 84%] Built target test_mpi_model_parallel
[ 90%] Built target estimate_comm_volume
[ 90%] Built target test_mpi_non_structual_graph
[ 90%] Built target test_nccl_thread
[ 91%] Built target test_hello_world
[ 91%] Built target test_mpi_structual_graph
[ 91%] Built target test_mpi_pipelined_model_parallel
[ 97%] Built target OSDI2023_MULTI_NODES_gcn
[ 97%] Built target test_two_layer_hybrid_parallelism_designer
[ 97%] Built target OSDI2023_SINGLE_NODE_gcn_inference
[100%] Built target OSDI2023_SINGLE_NODE_gcn
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_arxiv
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 1000
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_arxiv
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 1000
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_arxiv
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 1000
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_arxiv
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 1000
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
Initialized node 3 on machine g008.anvil.rcac.purdue.edu
Initialized node 2 on machine g007.anvil.rcac.purdue.edu
Initialized node 1 on machine g006.anvil.rcac.purdue.edu
Initialized node 0 on machine g005.anvil.rcac.purdue.edu
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.037 seconds.
Building the CSC structure...
        It takes 0.044 seconds.
Building the CSC structure...
        It takes 0.045 seconds.
Building the CSC structure...
        It takes 0.043 seconds.
Building the CSC structure...
        It takes 0.038 seconds.
        It takes 0.038 seconds.
        It takes 0.038 seconds.
        It takes 0.038 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.050 seconds.
Building the Label Vector...
        It takes 0.020 seconds.
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
        It takes 0.060 seconds.
Building the Label Vector...
        It takes 0.065 seconds.
        It takes 0.065 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.027 seconds.
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
        It takes 0.027 seconds.
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
        It takes 0.028 seconds.
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
train nodes 90941, valid nodes 29799, test nodes 48603
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
*** Node 0, starting model training...
Number of operators: 20
0 169343 0 6
0 169343 6 11
0 169343 11 16
0 169343 16 20
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the partition [0, 6) x [0, 169343)
*** Node 0, constructing the helper classes...
WARNING: the current version only applies to linear GNN models!
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_WEIGHT, output tensors: 1
    Op 2: type OPERATOR_MATMUL, output tensors: 2
    Op 3: type OPERATOR_AGGREGATION, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_WEIGHT, output tensors: 6
    Op 7: type OPERATOR_MATMUL, output tensors: 7
    Op 8: type OPERATOR_AGGREGATION, output tensors: 8
    Op 9: type OPERATOR_RELU, output tensors: 9
    Op 10: type OPERATOR_DROPOUT, output tensors: 10
    Op 11: type OPERATOR_WEIGHT, output tensors: 11
    Op 12: type OPERATOR_MATMUL, output tensors: 12
    Op 13: type OPERATOR_AGGREGATION, output tensors: 13
    Op 14: type OPERATOR_RELU, output tensors: 14
    Op 15: type OPERATOR_DROPOUT, output tensors: 15
    Op 16: type OPERATOR_WEIGHT, output tensors: 16
    Op 17: type OPERATOR_MATMUL, output tensors: 17
    Op 18: type OPERATOR_AGGREGATION, output tensors: 18
    Op 19: type OPERATOR_SOFTMAX, output tensors: 19
Boundaries: 0 0 0 0 169343 169343 169343 169343
Fragments: [0, 169343)
Chunks (number of global chunks: 32): 0-[0, 5292) 1-[5292, 10584) 2-[10584, 15876) 3-[15876, 21168) 4-[21168, 26460) 5-[26460, 31752) 6-[31752, 37044) 7-[37044, 42336) 8-[42336, 47628) ... 31-[164052, 169343)
(Forwarding) Node 0 (fragment 0) depends on nodes:
(Backwarding) Node 0 (fragment 0) depends on nodes: 1 (Tensor: 5)
(I-link dependencies): node 0 should send activation to nodes:
(I-link dependencies): node 0 should receive activation from nodes:
(I-link dependencies): node 0 should send gradient to nodes:
(I-link dependencies): node 0 should receive gradient from nodes:
169343, 2484941, 2484941
train nodes 90941, valid nodes 29799, test nodes 48603
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
*** Node 1, starting model training...
Number of operators: 20
0 169343 0 6
0 169343 6 11
0 169343 11 16
0 169343 16 20
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the partition [6, 11) x [0, 169343)
*** Node 1, constructing the helper classes...
(Forwarding) Node 1 (fragment 0) depends on nodes: 0 (Tensor: 5)
(Backwarding) Node 1 (fragment 0) depends on nodes: 2 (Tensor: 10)
(I-link dependencies): node 1 should send activation to nodes:
(I-link dependencies): node 1 should receive activation from nodes:
(I-link dependencies): node 1 should send gradient to nodes:
(I-link dependencies): node 1 should receive gradient from nodes:
169343, 2484941, 2484941
train nodes 90941, valid nodes 29799, test nodes 48603
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
*** Node 2, starting model training...
Number of operators: 20
0 169343 0 6
0 169343 6 11
0 169343 11 16
0 169343 16 20
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the partition [11, 16) x [0, 169343)
*** Node 2, constructing the helper classes...
(Forwarding) Node 2 (fragment 0) depends on nodes: 1 (Tensor: 10)
(Backwarding) Node 2 (fragment 0) depends on nodes: 3 (Tensor: 15)
(I-link dependencies): node 2 should send activation to nodes:
(I-link dependencies): node 2 should receive activation from nodes:
(I-link dependencies): node 2 should send gradient to nodes:
(I-link dependencies): node 2 should receive gradient from nodes:
169343, 2484941, 2484941
train nodes 90941, valid nodes 29799, test nodes 48603
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
*** Node 3, starting model training...
Number of operators: 20
0 169343 0 6
0 169343 6 11
0 169343 11 16
0 169343 16 20
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the partition [16, 20) x [0, 169343)
*** Node 3, constructing the helper classes...
WARNING: the current version only applies to linear GNN models!
(Forwarding) Node 3 (fragment 0) depends on nodes: 2 (Tensor: 15)
(Backwarding) Node 3 (fragment 0) depends on nodes:
(I-link dependencies): node 3 should send activation to nodes:
(I-link dependencies): node 3 should receive activation from nodes:
(I-link dependencies): node 3 should send gradient to nodes:
(I-link dependencies): node 3 should receive gradient from nodes:
169343, 2484941, 2484941
csr in-out ready !*** Node 0, setting up some other necessary information...
csr in-out ready !*** Node 3, setting up some other necessary information...
csr in-out ready !*** Node 1, setting up some other necessary information...
csr in-out ready !*** Node 2, setting up some other necessary information...
*** Node 0, starting the helper threads...
*** Node 1, starting the helper threads...
*** Node 2, starting the helper threads...
*** Node 3, starting the helper threads...
+++++++++ Node 0 initializing the weights for op[0, 6)...
+++++++++ Node 0, mapping weight op 1
+++++++++ Node 2 initializing the weights for op[11, 16)...
+++++++++ Node 2, mapping weight op 11
+++++++++ Node 1 initializing the weights for op[6, 11)...
+++++++++ Node 1, mapping weight op 6
+++++++++ Node 3 initializing the weights for op[16, 20)...
+++++++++ Node 3, mapping weight op 16
RANDOMLY DISPATCH THE CHUNKS...
*** Node 0, starting task scheduling...
*** Node 2, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
    Epoch 9:	Loss 34.62736	TrainAcc 0.1791	ValidAcc 0.0764	TestAcc 0.0586
    Epoch 19:	Loss 31.33066	TrainAcc 0.2575	ValidAcc 0.2563	TestAcc 0.2377
    Epoch 29:	Loss 29.46007	TrainAcc 0.2948	ValidAcc 0.3148	TestAcc 0.2831
    Epoch 39:	Loss 26.92012	TrainAcc 0.3669	ValidAcc 0.3667	TestAcc 0.3380
    Epoch 49:	Loss 24.78033	TrainAcc 0.3208	ValidAcc 0.3355	TestAcc 0.3123
    Epoch 59:	Loss 22.83799	TrainAcc 0.4142	ValidAcc 0.4126	TestAcc 0.3827
    Epoch 69:	Loss 21.82201	TrainAcc 0.4130	ValidAcc 0.4552	TestAcc 0.4481
    Epoch 79:	Loss 21.19739	TrainAcc 0.3925	ValidAcc 0.3888	TestAcc 0.4068
    Epoch 89:	Loss 20.35915	TrainAcc 0.4462	ValidAcc 0.4458	TestAcc 0.4192
    Epoch 99:	Loss 19.67336	TrainAcc 0.4572	ValidAcc 0.4891	TestAcc 0.5060
    Epoch 109:	Loss 19.09973	TrainAcc 0.4824	ValidAcc 0.4906	TestAcc 0.4638
    Epoch 119:	Loss 18.52771	TrainAcc 0.4867	ValidAcc 0.5140	TestAcc 0.5261
    Epoch 129:	Loss 18.26625	TrainAcc 0.4862	ValidAcc 0.4836	TestAcc 0.4569
    Epoch 139:	Loss 17.87299	TrainAcc 0.5126	ValidAcc 0.5419	TestAcc 0.5509
    Epoch 149:	Loss 17.60707	TrainAcc 0.5211	ValidAcc 0.5299	TestAcc 0.5052
    Epoch 159:	Loss 17.43148	TrainAcc 0.5095	ValidAcc 0.5118	TestAcc 0.5039
    Epoch 169:	Loss 17.12394	TrainAcc 0.5200	ValidAcc 0.5331	TestAcc 0.5281
    Epoch 179:	Loss 16.85126	TrainAcc 0.5327	ValidAcc 0.5272	TestAcc 0.5048
    Epoch 189:	Loss 16.58740	TrainAcc 0.5510	ValidAcc 0.5652	TestAcc 0.5649
    Epoch 199:	Loss 16.35677	TrainAcc 0.5575	ValidAcc 0.5519	TestAcc 0.5247
    Epoch 209:	Loss 16.13896	TrainAcc 0.5641	ValidAcc 0.5840	TestAcc 0.5841
    Epoch 219:	Loss 15.89467	TrainAcc 0.5672	ValidAcc 0.5658	TestAcc 0.5437
    Epoch 229:	Loss 15.69395	TrainAcc 0.5695	ValidAcc 0.5916	TestAcc 0.5854
    Epoch 239:	Loss 15.47134	TrainAcc 0.5729	ValidAcc 0.5688	TestAcc 0.5488
    Epoch 249:	Loss 15.35082	TrainAcc 0.5767	ValidAcc 0.5986	TestAcc 0.5930
    Epoch 259:	Loss 15.16673	TrainAcc 0.5801	ValidAcc 0.5862	TestAcc 0.5695
    Epoch 269:	Loss 15.04991	TrainAcc 0.5839	ValidAcc 0.6044	TestAcc 0.5994
    Epoch 279:	Loss 14.90616	TrainAcc 0.5870	ValidAcc 0.5899	TestAcc 0.5779
    Epoch 289:	Loss 14.79475	TrainAcc 0.5909	ValidAcc 0.6038	TestAcc 0.6001
    Epoch 299:	Loss 14.71032	TrainAcc 0.5918	ValidAcc 0.6025	TestAcc 0.5926
    Epoch 309:	Loss 14.62061	TrainAcc 0.5978	ValidAcc 0.6070	TestAcc 0.5941
    Epoch 319:	Loss 14.51396	TrainAcc 0.5972	ValidAcc 0.6083	TestAcc 0.6032
    Epoch 329:	Loss 14.42536	TrainAcc 0.6017	ValidAcc 0.6089	TestAcc 0.5915
    Epoch 339:	Loss 14.32374	TrainAcc 0.6021	ValidAcc 0.6148	TestAcc 0.6073
    Epoch 349:	Loss 14.28711	TrainAcc 0.5983	ValidAcc 0.6080	TestAcc 0.6014
    Epoch 359:	Loss 14.20680	TrainAcc 0.6024	ValidAcc 0.6124	TestAcc 0.6026
    Epoch 369:	Loss 14.12320	TrainAcc 0.6049	ValidAcc 0.6142	TestAcc 0.6071
    Epoch 379:	Loss 14.07575	TrainAcc 0.6067	ValidAcc 0.6225	TestAcc 0.6181
    Epoch 389:	Loss 14.01078	TrainAcc 0.6089	ValidAcc 0.6131	TestAcc 0.5996
    Epoch 399:	Loss 13.94553	TrainAcc 0.6080	ValidAcc 0.6234	TestAcc 0.6213
    Epoch 409:	Loss 13.88331	TrainAcc 0.6125	ValidAcc 0.6200	TestAcc 0.6105
    Epoch 419:	Loss 13.84086	TrainAcc 0.6132	ValidAcc 0.6240	TestAcc 0.6137
    Epoch 429:	Loss 13.83545	TrainAcc 0.6102	ValidAcc 0.6184	TestAcc 0.6225
    Epoch 439:	Loss 13.84015	TrainAcc 0.6121	ValidAcc 0.6158	TestAcc 0.5955
    Epoch 449:	Loss 13.81674	TrainAcc 0.6124	ValidAcc 0.6235	TestAcc 0.6215
    Epoch 459:	Loss 13.78684	TrainAcc 0.6160	ValidAcc 0.6214	TestAcc 0.6038
    Epoch 469:	Loss 13.73497	TrainAcc 0.6195	ValidAcc 0.6330	TestAcc 0.6282
    Epoch 479:	Loss 13.69617	TrainAcc 0.6185	ValidAcc 0.6245	TestAcc 0.6155
    Epoch 489:	Loss 13.64491	TrainAcc 0.6246	ValidAcc 0.6337	TestAcc 0.6250
    Epoch 499:	Loss 13.58040	TrainAcc 0.6217	ValidAcc 0.6346	TestAcc 0.6258
    Epoch 509:	Loss 13.53128	TrainAcc 0.6265	ValidAcc 0.6371	TestAcc 0.6273
    Epoch 519:	Loss 13.49051	TrainAcc 0.6230	ValidAcc 0.6351	TestAcc 0.6285
    Epoch 529:	Loss 13.44669	TrainAcc 0.6255	ValidAcc 0.6317	TestAcc 0.6209
    Epoch 539:	Loss 13.42017	TrainAcc 0.6261	ValidAcc 0.6352	TestAcc 0.6315
    Epoch 549:	Loss 13.34841	TrainAcc 0.6255	ValidAcc 0.6327	TestAcc 0.6163
    Epoch 559:	Loss 13.30686	TrainAcc 0.6251	ValidAcc 0.6368	TestAcc 0.6308
    Epoch 569:	Loss 13.28009	TrainAcc 0.6267	ValidAcc 0.6349	TestAcc 0.6214
    Epoch 579:	Loss 13.23413	TrainAcc 0.6269	ValidAcc 0.6395	TestAcc 0.6332
    Epoch 589:	Loss 13.19581	TrainAcc 0.6272	ValidAcc 0.6357	TestAcc 0.6223
    Epoch 599:	Loss 13.15275	TrainAcc 0.6294	ValidAcc 0.6383	TestAcc 0.6380
    Epoch 609:	Loss 13.10585	TrainAcc 0.6317	ValidAcc 0.6386	TestAcc 0.6276
    Epoch 619:	Loss 13.06140	TrainAcc 0.6310	ValidAcc 0.6443	TestAcc 0.6361
    Epoch 629:	Loss 13.04553	TrainAcc 0.6299	ValidAcc 0.6367	TestAcc 0.6252
    Epoch 639:	Loss 13.01240	TrainAcc 0.6312	ValidAcc 0.6420	TestAcc 0.6351
    Epoch 649:	Loss 12.98543	TrainAcc 0.6353	ValidAcc 0.6434	TestAcc 0.6354
    Epoch 659:	Loss 12.97633	TrainAcc 0.6342	ValidAcc 0.6435	TestAcc 0.6397
    Epoch 669:	Loss 12.93665	TrainAcc 0.6381	ValidAcc 0.6415	TestAcc 0.6315
    Epoch 679:	Loss 12.90707	TrainAcc 0.6380	ValidAcc 0.6464	TestAcc 0.6468
    Epoch 689:	Loss 12.87999	TrainAcc 0.6366	ValidAcc 0.6399	TestAcc 0.6292
    Epoch 699:	Loss 12.87429	TrainAcc 0.6395	ValidAcc 0.6487	TestAcc 0.6456
    Epoch 709:	Loss 12.85068	TrainAcc 0.6388	ValidAcc 0.6480	TestAcc 0.6315
    Epoch 719:	Loss 12.84411	TrainAcc 0.6408	ValidAcc 0.6476	TestAcc 0.6429
    Epoch 729:	Loss 12.82910	TrainAcc 0.6415	ValidAcc 0.6490	TestAcc 0.6406
    Epoch 739:	Loss 12.79394	TrainAcc 0.6420	ValidAcc 0.6484	TestAcc 0.6436
    Epoch 749:	Loss 12.78758	TrainAcc 0.6399	ValidAcc 0.6471	TestAcc 0.6403
    Epoch 759:	Loss 12.75979	TrainAcc 0.6436	ValidAcc 0.6518	TestAcc 0.6424
    Epoch 769:	Loss 12.72510	TrainAcc 0.6423	ValidAcc 0.6525	TestAcc 0.6447
    Epoch 779:	Loss 12.73579	TrainAcc 0.6424	ValidAcc 0.6509	TestAcc 0.6434
    Epoch 789:	Loss 12.68976	TrainAcc 0.6449	ValidAcc 0.6490	TestAcc 0.6431
    Epoch 799:	Loss 12.67063	TrainAcc 0.6450	ValidAcc 0.6535	TestAcc 0.6460
    Epoch 809:	Loss 12.63093	TrainAcc 0.6429	ValidAcc 0.6531	TestAcc 0.6433
    Epoch 819:	Loss 12.60868	TrainAcc 0.6460	ValidAcc 0.6535	TestAcc 0.6472
    Epoch 829:	Loss 12.58972	TrainAcc 0.6456	ValidAcc 0.6514	TestAcc 0.6436
    Epoch 839:	Loss 12.56189	TrainAcc 0.6463	ValidAcc 0.6540	TestAcc 0.6481
    Epoch 849:	Loss 12.59070	TrainAcc 0.6425	ValidAcc 0.6522	TestAcc 0.6422
    Epoch 859:	Loss 12.57123	TrainAcc 0.6469	ValidAcc 0.6521	TestAcc 0.6453
    Epoch 869:	Loss 12.52765	TrainAcc 0.6462	ValidAcc 0.6559	TestAcc 0.6526
    Epoch 879:	Loss 12.50112	TrainAcc 0.6480	ValidAcc 0.6533	TestAcc 0.6422
    Epoch 889:	Loss 12.46477	TrainAcc 0.6477	ValidAcc 0.6586	TestAcc 0.6468
    Epoch 899:	Loss 12.42552	TrainAcc 0.6511	ValidAcc 0.6561	TestAcc 0.6454
    Epoch 909:	Loss 12.39782	TrainAcc 0.6509	ValidAcc 0.6565	TestAcc 0.6479
    Epoch 919:	Loss 12.38263	TrainAcc 0.6508	ValidAcc 0.6540	TestAcc 0.6492
    Epoch 929:	Loss 12.39897	TrainAcc 0.6500	ValidAcc 0.6569	TestAcc 0.6466
    Epoch 939:	Loss 12.40400	TrainAcc 0.6507	ValidAcc 0.6511	TestAcc 0.6468
    Epoch 949:	Loss 12.39019	TrainAcc 0.6492	ValidAcc 0.6564	TestAcc 0.6460
    Epoch 959:	Loss 12.35315	TrainAcc 0.6511	ValidAcc 0.6585	TestAcc 0.6519
    Epoch 969:	Loss 12.32648	TrainAcc 0.6519	ValidAcc 0.6578	TestAcc 0.6513
    Epoch 979:	Loss 12.31713	TrainAcc 0.6523	ValidAcc 0.6601	TestAcc 0.6512
    Epoch 989:	Loss 12.28168	TrainAcc 0.6517	ValidAcc 0.6576	TestAcc 0.6480
Node 0, Layer-level comm throughput (act): -nan GBps
Node 1, Layer-level comm throughput (act): 10.141 GBps
Node 2, Layer-level comm throughput (act): 10.209 GBps
Node 3, Layer-level comm throughput (act): 9.992 GBps
Node 3, Layer-level comm throughput (grad): -nan GBps
Node 2, Layer-level comm throughput (grad): 10.184 GBps
Node 1, Layer-level comm throughput (grad): 10.318 GBps
Node 0, Layer-level comm throughput (grad): 10.373 GBps
    Epoch 999:	Loss 12.24274	TrainAcc 0.6543	ValidAcc 0.6576	TestAcc 0.6493
Node 0, compression time: 1.854s, compression size: 80.749GB, throughput: 43.557GBps
Node 0, decompression time: 2.628s, compression size: 80.749GB, throughput: 30.726GBps
Node 0, pure compute time: 9.933 s, total compute time: 14.415 s
Node 0, wait_for_task_time: 9.918 s, wait_for_other_gpus_time: 0.003 s
------------------------node id 0,  per-epoch time: 0.026500 s---------------
Node 1, compression time: 5.579s, compression size: 161.498GB, throughput: 28.949GBps
Node 1, decompression time: 3.486s, compression size: 161.498GB, throughput: 46.329GBps
Node 3, compression time: 3.390s, compression size: 80.749GB, throughput: 23.823GBps
Node 3, decompression time: 0.904s, compression size: 80.749GB, throughput: 89.300GBps
Node 3, pure compute time: 12.176 s, total compute time: 16.470 s
Node 0, wait_for_task_time: 1.296 s, wait_for_other_gpus_time: 0.007 s
------------------------node id 3,  per-epoch time: 0.026500 s---------------
Node 2, compression time: 5.599s, compression size: 161.498GB, throughput: 28.843GBps
Node 2, decompression time: 3.304s, compression size: 161.498GB, throughput: 48.881GBps
Node 2, pure compute time: 8.397 s, total compute time: 17.301 s
Node 0, wait_for_task_time: 4.060 s, wait_for_other_gpus_time: 0.006 s
------------------------node id 2,  per-epoch time: 0.026500 s---------------
Node 1, pure compute time: 8.437 s, total compute time: 17.502 s
Node 0, wait_for_task_time: 4.251 s, wait_for_other_gpus_time: 0.007 s
------------------------node id 1,  per-epoch time: 0.026500 s---------------
ERROR: undercount the overhead: breakdown sum / all time: 0.574
************ Profiling Results ************
	Bubble: 2.302384 (s) (14.96 percentage)
	Compute: 9.749196 (s) (63.33 percentage)
	GradSync: 0.347907 (s) (2.26 percentage)
	GraphComm: 0.024306 (s) (0.16 percentage)
	Imbalance: 1.876040 (s) (12.19 percentage)
	LayerComm: 1.093311 (s) (7.10 percentage)
ERROR: undercount the overhead: breakdown sum / all time: 0.574
	Layer-level communication (cluster-wide, per epoch): 0.184 GB
Highest valid_acc: 0.6601
Target test_acc: 0.6512
Epoch to reach the target acc: 980
ERROR: undercount the overhead: breakdown sum / all time: 0.574
ERROR: undercount the overhead: breakdown sum / all time: 0.574
[MPI Rank 3] Success 
[MPI Rank 1] Success 
[MPI Rank 0] Success 
[MPI Rank 2] Success 
