g014.anvil.rcac.purdue.edu
Fri Apr 21 11:28:38 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:01:00.0 Off |                    0 |
| N/A   39C    P0    52W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2       9) zlib/1.2.11
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0       10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0      11) openmpi/4.0.6
  4) gmp/6.2.1              8) numactl/2.0.14  12) boost/1.74.0

 

[  4%] Built target context
[ 17%] Built target core
[ 19%] Built target parallel
[ 20%] Building CXX object CMakeFiles/cudahelp.dir/core/src/cuda/cuda_single_cpu_engine.cc.o
[ 21%] Linking CXX static library libcudahelp.a
[ 39%] Built target cudahelp
[ 42%] Linking CXX executable test_mpi_loader
[ 45%] Linking CXX executable test_trivial
[ 47%] Linking CXX executable test_hello_world
[ 51%] Linking CXX executable estimate_comm_volume
[ 47%] Linking CXX executable test_cuda_model_parallel
[ 47%] Linking CXX executable test_mpi_gpu_hybrid
[ 47%] Linking CXX executable test_mpi_combined
[ 47%] Linking CXX executable test_mpi_gpu_model_parallel
[ 55%] Linking CXX executable test_cuda_pipeline_parallel
[ 58%] Linking CXX executable test_full_non_structual_graph
[ 55%] Linking CXX executable test_full_structual_graph
[ 55%] Linking CXX executable test_mpi_non_structual_graph
[ 55%] Linking CXX executable test_cuda_graph
[ 55%] Linking CXX executable test_nccl_thread
[ 61%] Linking CXX executable test_mpi_pipelined_model_parallel
[ 61%] Linking CXX executable test_mpi_structual_graph
[ 61%] Linking CXX executable test_single_node_gpu_training
[ 61%] Linking CXX executable test_single_node_training
[ 63%] Linking CXX executable test_mpi_model_parallel
[ 61%] Linking CXX executable test_mpi_gpu_pipelined_model_parallel
[ 63%] Linking CXX executable test_nccl_mpi
[ 64%] Linking CXX executable test_graph
[ 63%] Linking CXX executable test_cuda
[ 64%] Linking CXX executable test_single_node_fullgpu_training
[ 69%] Linking CXX executable test_two_layer_hybrid_parallelism_designer
[ 69%] Linking CXX executable gcn
[ 69%] Linking CXX executable gcn
[ 69%] Linking CXX executable gcnii
[ 69%] Linking CXX executable gcn_inference
[ 71%] Built target test_mpi_non_structual_graph
[ 71%] Built target test_mpi_structual_graph
[ 72%] Built target test_mpi_loader
[ 73%] Built target test_two_layer_hybrid_parallelism_designer
[ 77%] Built target test_full_structual_graph
[ 77%] Built target test_hello_world
[ 77%] Built target test_mpi_combined
[ 78%] Built target estimate_comm_volume
[ 79%] Built target test_cuda
[ 82%] Built target test_single_node_training
[ 82%] Built target test_graph
[ 82%] Built target test_trivial
[ 83%] Built target test_cuda_graph
[ 84%] Built target test_full_non_structual_graph
[ 85%] Built target test_mpi_gpu_model_parallel
[ 87%] Built target test_mpi_gpu_pipelined_model_parallel
[ 88%] Built target test_mpi_model_parallel
[ 88%] Built target test_mpi_pipelined_model_parallel
[ 89%] Built target test_mpi_gpu_hybrid
[ 90%] Built target test_single_node_gpu_training
[ 91%] Built target OSDI2023_SINGLE_NODE_gcn
[ 93%] Built target OSDI2023_SINGLE_NODE_gcn_inference
[ 93%] Built target test_single_node_fullgpu_training
[ 94%] Built target test_cuda_pipeline_parallel
[ 95%] Built target OSDI2023_MULTI_NODES_gcn
[ 96%] Built target OSDI2023_MULTI_NODES_gcnii
[ 97%] Built target test_nccl_thread
[ 98%] Built target test_nccl_mpi
[100%] Built target test_cuda_model_parallel
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/cora
The number of GCNII layers: 16
The number of hidden units: 64
The number of training epoches: 250
The number of startup epoches: 0
Learning rate: 0.010000
The partition strategy: model
The dropout rate: 0.000
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 13
The scaling down factor of out-of-chunk gradients: 0.100000
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Initialized node 0 on machine g014.anvil.rcac.purdue.edu
Building the CSR structure...
        It takes 0.001 seconds.
Building the CSC structure...
        It takes 0.000 seconds.
Building the Feature Vector...
        It takes 0.009 seconds.
Building the Label Vector...
        It takes 0.001 seconds.
Number of classes: 7
Number of feature dimensions: 1433
Number of vertices: 2708
train nodes 140, valid nodes 500, test nodes 1000
Number of GPUs: 1
GPU 0, layer [0, 18)
WARNING: the current version only applies to linear GNN models!
*** Node 0, starting model training...
Number of operators: 121
0 2708 0 121
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the partition [0, 121) x [0, 2708)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_DROPOUT, output tensors: 1
    Op 2: type OPERATOR_WEIGHT, output tensors: 2
    Op 3: type OPERATOR_MATMUL, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_AGGREGATION, output tensors: 6
    Op 7: type OPERATOR_ADD, output tensors: 7
    Op 8: type OPERATOR_WEIGHT, output tensors: 8
    Op 9: type OPERATOR_MATMUL, output tensors: 9
    Op 10: type OPERATOR_ADD, output tensors: 10
    Op 11: type OPERATOR_RELU, output tensors: 11
    Op 12: type OPERATOR_DROPOUT, output tensors: 12
    Op 13: type OPERATOR_AGGREGATION, output tensors: 13
    Op 14: type OPERATOR_ADD, output tensors: 14
    Op 15: type OPERATOR_WEIGHT, output tensors: 15
    Op 16: type OPERATOR_MATMUL, output tensors: 16
    Op 17: type OPERATOR_ADD, output tensors: 17
    Op 18: type OPERATOR_RELU, output tensors: 18
    Op 19: type OPERATOR_DROPOUT, output tensors: 19
    Op 20: type OPERATOR_AGGREGATION, output tensors: 20
    Op 21: type OPERATOR_ADD, output tensors: 21
    Op 22: type OPERATOR_WEIGHT, output tensors: 22
    Op 23: type OPERATOR_MATMUL, output tensors: 23
    Op 24: type OPERATOR_ADD, output tensors: 24
    Op 25: type OPERATOR_RELU, output tensors: 25
    Op 26: type OPERATOR_DROPOUT, output tensors: 26
    Op 27: type OPERATOR_AGGREGATION, output tensors: 27
    Op 28: type OPERATOR_ADD, output tensors: 28
    Op 29: type OPERATOR_WEIGHT, output tensors: 29
    Op 30: type OPERATOR_MATMUL, output tensors: 30
    Op 31: type OPERATOR_ADD, output tensors: 31
    Op 32: type OPERATOR_RELU, output tensors: 32
    Op 33: type OPERATOR_DROPOUT, output tensors: 33
    Op 34: type OPERATOR_AGGREGATION, output tensors: 34
    Op 35: type OPERATOR_ADD, output tensors: 35
    Op 36: type OPERATOR_WEIGHT, output tensors: 36
    Op 37: type OPERATOR_MATMUL, output tensors: 37
    Op 38: type OPERATOR_ADD, output tensors: 38
    Op 39: type OPERATOR_RELU, output tensors: 39
    Op 40: type OPERATOR_DROPOUT, output tensors: 40
    Op 41: type OPERATOR_AGGREGATION, output tensors: 41
    Op 42: type OPERATOR_ADD, output tensors: 42
    Op 43: type OPERATOR_WEIGHT, output tensors: 43
    Op 44: type OPERATOR_MATMUL, output tensors: 44
    Op 45: type OPERATOR_ADD, output tensors: 45
    Op 46: type OPERATOR_RELU, output tensors: 46
    Op 47: type OPERATOR_DROPOUT, output tensors: 47
    Op 48: type OPERATOR_AGGREGATION, output tensors: 48
    Op 49: type OPERATOR_ADD, output tensors: 49
    Op 50: type OPERATOR_WEIGHT, output tensors: 50
    Op 51: type OPERATOR_MATMUL, output tensors: 51
    Op 52: type OPERATOR_ADD, output tensors: 52
    Op 53: type OPERATOR_RELU, output tensors: 53
    Op 54: type OPERATOR_DROPOUT, output tensors: 54
    Op 55: type OPERATOR_AGGREGATION, output tensors: 55
    Op 56: type OPERATOR_ADD, output tensors: 56
    Op 57: type OPERATOR_WEIGHT, output tensors: 57
    Op 58: type OPERATOR_MATMUL, output tensors: 58
    Op 59: type OPERATOR_ADD, output tensors: 59
    Op 60: type OPERATOR_RELU, output tensors: 60
    Op 61: type OPERATOR_DROPOUT, output tensors: 61
    Op 62: type OPERATOR_AGGREGATION, output tensors: 62
    Op 63: type OPERATOR_ADD, output tensors: 63
    Op 64: type OPERATOR_WEIGHT, output tensors: 64
    Op 65: type OPERATOR_MATMUL, output tensors: 65
    Op 66: type OPERATOR_ADD, output tensors: 66
    Op 67: type OPERATOR_RELU, output tensors: 67
    Op 68: type OPERATOR_DROPOUT, output tensors: 68
    Op 69: type OPERATOR_AGGREGATION, output tensors: 69
    Op 70: type OPERATOR_ADD, output tensors: 70
    Op 71: type OPERATOR_WEIGHT, output tensors: 71
    Op 72: type OPERATOR_MATMUL, output tensors: 72
    Op 73: type OPERATOR_ADD, output tensors: 73
    Op 74: type OPERATOR_RELU, output tensors: 74
    Op 75: type OPERATOR_DROPOUT, output tensors: 75
    Op 76: type OPERATOR_AGGREGATION, output tensors: 76
    Op 77: type OPERATOR_ADD, output tensors: 77
    Op 78: type OPERATOR_WEIGHT, output tensors: 78
    Op 79: type OPERATOR_MATMUL, output tensors: 79
    Op 80: type OPERATOR_ADD, output tensors: 80
    Op 81: type OPERATOR_RELU, output tensors: 81
    Op 82: type OPERATOR_DROPOUT, output tensors: 82
    Op 83: type OPERATOR_AGGREGATION, output tensors: 83
    Op 84: type OPERATOR_ADD, output tensors: 84
    Op 85: type OPERATOR_WEIGHT, output tensors: 85
    Op 86: type OPERATOR_MATMUL, output tensors: 86
    Op 87: type OPERATOR_ADD, output tensors: 87
    Op 88: type OPERATOR_RELU, output tensors: 88
    Op 89: type OPERATOR_DROPOUT, output tensors: 89
    Op 90: type OPERATOR_AGGREGATION, output tensors: 90
    Op 91: type OPERATOR_ADD, output tensors: 91
    Op 92: type OPERATOR_WEIGHT, output tensors: 92
    Op 93: type OPERATOR_MATMUL, output tensors: 93
    Op 94: type OPERATOR_ADD, output tensors: 94
    Op 95: type OPERATOR_RELU, output tensors: 95
    Op 96: type OPERATOR_DROPOUT, output tensors: 96
    Op 97: type OPERATOR_AGGREGATION, output tensors: 97
    Op 98: type OPERATOR_ADD, output tensors: 98
    Op 99: type OPERATOR_WEIGHT, output tensors: 99
    Op 100: type OPERATOR_MATMUL, output tensors: 100
    Op 101: type OPERATOR_ADD, output tensors: 101
    Op 102: type OPERATOR_RELU, output tensors: 102
    Op 103: type OPERATOR_DROPOUT, output tensors: 103
    Op 104: type OPERATOR_AGGREGATION, output tensors: 104
    Op 105: type OPERATOR_ADD, output tensors: 105
    Op 106: type OPERATOR_WEIGHT, output tensors: 106
    Op 107: type OPERATOR_MATMUL, output tensors: 107
    Op 108: type OPERATOR_ADD, output tensors: 108
    Op 109: type OPERATOR_RELU, output tensors: 109
    Op 110: type OPERATOR_DROPOUT, output tensors: 110
    Op 111: type OPERATOR_AGGREGATION, output tensors: 111
    Op 112: type OPERATOR_ADD, output tensors: 112
    Op 113: type OPERATOR_WEIGHT, output tensors: 113
    Op 114: type OPERATOR_MATMUL, output tensors: 114
    Op 115: type OPERATOR_ADD, output tensors: 115
    Op 116: type OPERATOR_RELU, output tensors: 116
    Op 117: type OPERATOR_DROPOUT, output tensors: 117
    Op 118: type OPERATOR_WEIGHT, output tensors: 118
    Op 119: type OPERATOR_MATMUL, output tensors: 119
    Op 120: type OPERATOR_SOFTMAX, output tensors: 120
Boundaries: 0 2708
Fragments: [0, 2708)
Chunks (number of global chunks: 1): 0-[0, 2708)
(Forwarding) Node 0 (fragment 0) depends on nodes:
(Backwarding) Node 0 (fragment 0) depends on nodes:
(I-link dependencies): node 0 should send activation to nodes:
(I-link dependencies): node 0 should receive activation from nodes:
(I-link dependencies): node 0 should send gradient to nodes:
(I-link dependencies): node 0 should receive gradient from nodes:
2708, 13264, 13264
Number of vertices per chunk: 2708
csr in-out ready !*** Node 0, setting up some other necessary information...
*** Node 0, starting the helper threads...
+++++++++ Node 0 initializing the weights for op[0, 121)...
+++++++++ Node 0, mapping weight op 2
+++++++++ Node 0, mapping weight op 8
+++++++++ Node 0, mapping weight op 15
+++++++++ Node 0, mapping weight op 22
+++++++++ Node 0, mapping weight op 29
+++++++++ Node 0, mapping weight op 36
+++++++++ Node 0, mapping weight op 43
+++++++++ Node 0, mapping weight op 50
+++++++++ Node 0, mapping weight op 57
+++++++++ Node 0, mapping weight op 64
+++++++++ Node 0, mapping weight op 71
+++++++++ Node 0, mapping weight op 78
+++++++++ Node 0, mapping weight op 85
+++++++++ Node 0, mapping weight op 92
+++++++++ Node 0, mapping weight op 99
+++++++++ Node 0, mapping weight op 106
+++++++++ Node 0, mapping weight op 113
+++++++++ Node 0, mapping weight op 118
RANDOMLY DISPATCH THE CHUNKS...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.010000000
    Epoch 9:	Loss 1.92975	TrainAcc 0.4643	ValidAcc 0.3000	TestAcc 0.3000
    Epoch 19:	Loss 1.91101	TrainAcc 0.3857	ValidAcc 0.2360	TestAcc 0.2340
    Epoch 29:	Loss 1.55355	TrainAcc 0.4357	ValidAcc 0.3660	TestAcc 0.3850
    Epoch 39:	Loss 1.26567	TrainAcc 0.4000	ValidAcc 0.2540	TestAcc 0.2690
    Epoch 49:	Loss 1.03170	TrainAcc 0.4857	ValidAcc 0.3620	TestAcc 0.3900
    Epoch 59:	Loss 0.93088	TrainAcc 0.6500	ValidAcc 0.4780	TestAcc 0.4740
    Epoch 69:	Loss 0.81752	TrainAcc 0.7429	ValidAcc 0.5240	TestAcc 0.5620
    Epoch 79:	Loss 0.72223	TrainAcc 0.8143	ValidAcc 0.5740	TestAcc 0.6020
    Epoch 89:	Loss 0.57849	TrainAcc 0.8500	ValidAcc 0.6160	TestAcc 0.6240
    Epoch 99:	Loss 0.40769	TrainAcc 0.8929	ValidAcc 0.6320	TestAcc 0.6460
    Epoch 109:	Loss 0.26994	TrainAcc 0.9357	ValidAcc 0.6600	TestAcc 0.6890
    Epoch 119:	Loss 0.17240	TrainAcc 0.9714	ValidAcc 0.6880	TestAcc 0.7110
    Epoch 129:	Loss 0.13186	TrainAcc 0.9714	ValidAcc 0.7040	TestAcc 0.7170
    Epoch 139:	Loss 0.11434	TrainAcc 0.9786	ValidAcc 0.7080	TestAcc 0.7180
    Epoch 149:	Loss 0.44559	TrainAcc 0.8429	ValidAcc 0.6000	TestAcc 0.6090
    Epoch 159:	Loss 0.27345	TrainAcc 0.9143	ValidAcc 0.7020	TestAcc 0.7150
    Epoch 169:	Loss 0.24320	TrainAcc 0.9357	ValidAcc 0.7060	TestAcc 0.7180
    Epoch 179:	Loss 0.18324	TrainAcc 0.9500	ValidAcc 0.7260	TestAcc 0.7460
    Epoch 189:	Loss 0.14699	TrainAcc 0.9643	ValidAcc 0.7380	TestAcc 0.7490
    Epoch 199:	Loss 0.12653	TrainAcc 0.9786	ValidAcc 0.7460	TestAcc 0.7420
    Epoch 209:	Loss 0.11886	TrainAcc 0.9714	ValidAcc 0.7400	TestAcc 0.7390
    Epoch 219:	Loss 0.11201	TrainAcc 0.9786	ValidAcc 0.7400	TestAcc 0.7340
    Epoch 229:	Loss 0.10711	TrainAcc 0.9786	ValidAcc 0.7320	TestAcc 0.7330
    Epoch 239:	Loss 0.10311	TrainAcc 0.9786	ValidAcc 0.7340	TestAcc 0.7250
Node 0, Layer-level comm throughput (act): -nan GBps
Node 0, Layer-level comm throughput (grad): -nan GBps
    Epoch 249:	Loss 0.09963	TrainAcc 0.9786	ValidAcc 0.7260	TestAcc 0.7190
Node 0, GPU memory consumption: 1.451 GB
Node 0, compression time: 0.000s, compression size: 0.000GB, throughput: -nanGBps
Node 0, decompression time: 0.000s, compression size: 0.000GB, throughput: -nanGBps
Node 0, pure compute time: 1.121 s, total compute time: 1.121 s
Node 0, wait_for_task_time: 0.000 s, wait_for_other_gpus_time: 0.000 s
------------------------node id 0,  per-epoch time: 0.006343 s---------------
************ Profiling Results ************
	Bubble: 0.050471 (s) (3.06 percentage)
	Compute: 1.426396 (s) (86.37 percentage)
	GradSync: 0.169926 (s) (10.29 percentage)
	GraphComm: 0.004632 (s) (0.28 percentage)
	Imbalance: 0.000144 (s) (0.01 percentage)
	LayerComm: 0.000000 (s) (0.00 percentage)
	Layer-level communication (cluster-wide, per epoch): 0.000 GB
Highest valid_acc: 0.7460
Target test_acc: 0.7420
Epoch to reach the target acc: 200
[MPI Rank 0] Success 
