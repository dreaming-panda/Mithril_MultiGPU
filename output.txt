g000.anvil.rcac.purdue.edu
Wed Apr  5 21:03:58 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:81:00.0 Off |                    0 |
| N/A   40C    P0    52W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2       9) zlib/1.2.11
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0       10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0      11) openmpi/4.0.6
  4) gmp/6.2.1              8) numactl/2.0.14  12) boost/1.74.0

 

[  4%] Built target context
[ 17%] Built target parallel
[ 20%] Built target core
[ 21%] Building CUDA object CMakeFiles/cudahelp.dir/core/src/cuda/cuda_data_compressor.cu.o
[ 22%] Linking CXX static library libcudahelp.a
[ 40%] Built target cudahelp
[ 44%] Linking CXX executable test_mpi_combined
[ 50%] Linking CXX executable test_nccl_mpi
[ 53%] Linking CXX executable test_full_structual_graph
[ 55%] Linking CXX executable test_full_non_structual_graph
[ 56%] Linking CXX executable test_single_node_fullgpu_training
[ 58%] Linking CXX executable test_mpi_model_parallel
[ 60%] Linking CXX executable test_single_node_training
[ 55%] Linking CXX executable test_graph
[ 55%] Linking CXX executable estimate_comm_volume
[ 55%] Linking CXX executable test_cuda
[ 55%] Linking CXX executable test_mpi_structual_graph
[ 55%] Linking CXX executable test_mpi_loader
[ 55%] Linking CXX executable test_nccl_thread
[ 55%] Linking CXX executable test_hello_world
[ 55%] Linking CXX executable test_mpi_non_structual_graph
[ 62%] Linking CXX executable test_mpi_gpu_hybrid
[ 62%] Linking CXX executable test_cuda_model_parallel
[ 62%] Linking CXX executable test_trivial
[ 62%] Linking CXX executable test_cuda_graph
[ 65%] Linking CXX executable test_mpi_gpu_pipelined_model_parallel
[ 65%] Linking CXX executable test_cuda_pipeline_parallel
[ 65%] Linking CXX executable test_mpi_pipelined_model_parallel
[ 65%] Linking CXX executable test_mpi_gpu_model_parallel
[ 65%] Linking CXX executable test_single_node_gpu_training
[ 69%] Linking CXX executable gcn
[ 69%] Linking CXX executable test_two_layer_hybrid_parallelism_designer
[ 69%] Linking CXX executable gcn
[ 70%] Linking CXX executable gcn_inference
[ 71%] Built target test_cuda_graph
[ 72%] Built target test_mpi_pipelined_model_parallel
[ 73%] Built target test_mpi_structual_graph
[ 74%] Built target test_mpi_combined
[ 77%] Built target test_hello_world
[ 78%] Built target test_graph
[ 78%] Built target test_mpi_loader
[ 81%] Built target test_mpi_non_structual_graph
[ 81%] Built target test_full_non_structual_graph
[ 82%] Built target test_cuda
[ 84%] Built target test_trivial
[ 84%] Built target test_full_structual_graph
[ 84%] Built target estimate_comm_volume
[ 86%] Built target OSDI2023_SINGLE_NODE_gcn
[ 86%] Built target test_single_node_training
[ 87%] Built target test_mpi_model_parallel
[ 88%] Built target test_single_node_gpu_training
[ 89%] Built target test_two_layer_hybrid_parallelism_designer
[ 90%] Built target test_mpi_gpu_model_parallel
[ 92%] Built target test_mpi_gpu_pipelined_model_parallel
[ 92%] Built target test_single_node_fullgpu_training
[ 93%] Built target test_mpi_gpu_hybrid
[ 94%] Built target OSDI2023_SINGLE_NODE_gcn_inference
[ 95%] Built target test_cuda_pipeline_parallel
[ 96%] Built target OSDI2023_MULTI_NODES_gcn
[ 97%] Built target test_nccl_thread
[ 98%] Built target test_nccl_mpi
[100%] Built target test_cuda_model_parallel
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 3
The number of hidden units: 128
The number of training epoches: 1000
The number of startup epoches: 0
Learning rate: 0.003000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 3
The scaling down factor of out-of-chunk gradients: 0.100000
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 3
The number of hidden units: 128
The number of training epoches: 1000
The number of startup epoches: 0
Learning rate: 0.003000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 3
The scaling down factor of out-of-chunk gradients: 0.100000
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 3
The number of hidden units: 128
The number of training epoches: 1000
The number of startup epoches: 0
Learning rate: 0.003000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 3
The scaling down factor of out-of-chunk gradients: 0.100000
Initialized node 0 on machine g000.anvil.rcac.purdue.edu
Initialized node 2 on machine g002.anvil.rcac.purdue.edu
Initialized node 1 on machine g001.anvil.rcac.purdue.edu
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.999 seconds.
Building the CSC structure...
        It takes 2.042 seconds.
Building the CSC structure...
        It takes 2.360 seconds.
Building the CSC structure...
        It takes 1.975 seconds.
        It takes 1.956 seconds.
        It takes 1.945 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.634 seconds.
Building the Label Vector...
        It takes 0.700 seconds.
Building the Label Vector...
        It takes 0.659 seconds.
Building the Label Vector...
        It takes 0.348 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
        It takes 0.454 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
        It takes 0.401 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 3
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
WARNING: the current version only applies to linear GNN models!
*** Node 1, starting model training...
Number of operators: 15
0 2449029 0 6
0 2449029 6 11
0 2449029 11 15
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the partition [6, 11) x [0, 2449029)
*** Node 1, constructing the helper classes...
(Forwarding) Node 1 (fragment 0) depends on nodes: 0 (Tensor: 5)
(Backwarding) Node 1 (fragment 0) depends on nodes: 2 (Tensor: 10)
(I-link dependencies): node 1 should send activation to nodes: 2 (tensor: 10)
(I-link dependencies): node 1 should receive activation from nodes: 0 (tensor: 5)
(I-link dependencies): node 1 should send gradient to nodes:
(I-link dependencies): node 1 should receive gradient from nodes:
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 3
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
WARNING: the current version only applies to linear GNN models!
*** Node 2, starting model training...
Number of operators: 15
0 2449029 0 6
0 2449029 6 11
0 2449029 11 15
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the partition [11, 15) x [0, 2449029)
*** Node 2, constructing the helper classes...
(Forwarding) Node 2 (fragment 0) depends on nodes: 1 (Tensor: 10)
(Backwarding) Node 2 (fragment 0) depends on nodes:
(I-link dependencies): node 2 should send activation to nodes:
(I-link dependencies): node 2 should receive activation from nodes: 1 (tensor: 10)
(I-link dependencies): node 2 should send gradient to nodes:
(I-link dependencies): node 2 should receive gradient from nodes:
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 3
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
WARNING: the current version only applies to linear GNN models!
*** Node 0, starting model training...
Number of operators: 15
0 2449029 0 6
0 2449029 6 11
0 2449029 11 15
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the partition [0, 6) x [0, 2449029)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_WEIGHT, output tensors: 1
    Op 2: type OPERATOR_MATMUL, output tensors: 2
    Op 3: type OPERATOR_AGGREGATION, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_AGGREGATION, output tensors: 6
    Op 7: type OPERATOR_WEIGHT, output tensors: 7
    Op 8: type OPERATOR_MATMUL, output tensors: 8
    Op 9: type OPERATOR_RELU, output tensors: 9
    Op 10: type OPERATOR_DROPOUT, output tensors: 10
    Op 11: type OPERATOR_AGGREGATION, output tensors: 11
    Op 12: type OPERATOR_WEIGHT, output tensors: 12
    Op 13: type OPERATOR_MATMUL, output tensors: 13
    Op 14: type OPERATOR_SOFTMAX, output tensors: 14
Boundaries: 0 0 0 2449029 2449029 2449029
Fragments: [0, 2449029)
Chunks (number of global chunks: 12): 0-[0, 204086) 1-[204086, 408172) 2-[408172, 612258) 3-[612258, 816344) 4-[816344, 1020430) 5-[1020430, 1224516) 6-[1224516, 1428602) 7-[1428602, 1632688) 8-[1632688, 1836774) ... 11-[2244946, 2449029)
(Forwarding) Node 0 (fragment 0) depends on nodes:
(Backwarding) Node 0 (fragment 0) depends on nodes: 1 (Tensor: 5)
(I-link dependencies): node 0 should send activation to nodes: 1 (tensor: 5)
(I-link dependencies): node 0 should receive activation from nodes:
(I-link dependencies): node 0 should send gradient to nodes:
(I-link dependencies): node 0 should receive gradient from nodes:
2449029, 126167053, 126167053
Number of vertices per chunk: 204086
2449029, 126167053, 126167053
Number of vertices per chunk: 204086
2449029, 126167053, 126167053
Number of vertices per chunk: 204086
csr in-out ready !*** Node 0, setting up some other necessary information...
csr in-out ready !*** Node 2, setting up some other necessary information...
csr in-out ready !*** Node 1, setting up some other necessary information...
*** Node 2, starting the helper threads...
*** Node 0, starting the helper threads...
*** Node 1, starting the helper threads...
+++++++++ Node 1 initializing the weights for op[6, 11)...
+++++++++ Node 1, mapping weight op 7
+++++++++ Node 2 initializing the weights for op[11, 15)...
+++++++++ Node 2, mapping weight op 12
+++++++++ Node 0 initializing the weights for op[0, 6)...
+++++++++ Node 0, mapping weight op 1
RANDOMLY DISPATCH THE CHUNKS...
*** Node 0, starting task scheduling...
*** Node 2, starting task scheduling...



The learning rate specified by the user: 0.003000000
The learning rate specified by the user: 0.003000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.003000000
    Epoch 9:	Loss 1.76255	TrainAcc 0.5845	ValidAcc 0.5854	TestAcc 0.4589
    Epoch 19:	Loss 1.02713	TrainAcc 0.7999	ValidAcc 0.7952	TestAcc 0.6259
    Epoch 29:	Loss 0.73313	TrainAcc 0.8467	ValidAcc 0.8432	TestAcc 0.6704
    Epoch 39:	Loss 0.63793	TrainAcc 0.8631	ValidAcc 0.8596	TestAcc 0.6848
    Epoch 49:	Loss 0.56625	TrainAcc 0.8734	ValidAcc 0.8686	TestAcc 0.6957
    Epoch 59:	Loss 0.52890	TrainAcc 0.8788	ValidAcc 0.8736	TestAcc 0.6990
    Epoch 69:	Loss 0.49384	TrainAcc 0.8859	ValidAcc 0.8799	TestAcc 0.7045
    Epoch 79:	Loss 0.47611	TrainAcc 0.8873	ValidAcc 0.8813	TestAcc 0.7080
    Epoch 89:	Loss 0.45705	TrainAcc 0.8911	ValidAcc 0.8839	TestAcc 0.7133
    Epoch 99:	Loss 0.44761	TrainAcc 0.8930	ValidAcc 0.8856	TestAcc 0.7164
    Epoch 109:	Loss 0.43431	TrainAcc 0.8941	ValidAcc 0.8865	TestAcc 0.7191
    Epoch 119:	Loss 0.42638	TrainAcc 0.8957	ValidAcc 0.8869	TestAcc 0.7222
    Epoch 129:	Loss 0.41754	TrainAcc 0.8979	ValidAcc 0.8894	TestAcc 0.7247
    Epoch 139:	Loss 0.41241	TrainAcc 0.8989	ValidAcc 0.8911	TestAcc 0.7261
    Epoch 149:	Loss 0.40490	TrainAcc 0.9005	ValidAcc 0.8924	TestAcc 0.7287
    Epoch 159:	Loss 0.40180	TrainAcc 0.9009	ValidAcc 0.8934	TestAcc 0.7298
    Epoch 169:	Loss 0.39617	TrainAcc 0.9024	ValidAcc 0.8944	TestAcc 0.7318
    Epoch 179:	Loss 0.39133	TrainAcc 0.9033	ValidAcc 0.8949	TestAcc 0.7337
    Epoch 189:	Loss 0.38695	TrainAcc 0.9045	ValidAcc 0.8956	TestAcc 0.7342
    Epoch 199:	Loss 0.38386	TrainAcc 0.9051	ValidAcc 0.8967	TestAcc 0.7351
    Epoch 209:	Loss 0.37961	TrainAcc 0.9056	ValidAcc 0.8959	TestAcc 0.7353
    Epoch 219:	Loss 0.37621	TrainAcc 0.9064	ValidAcc 0.8979	TestAcc 0.7380
    Epoch 229:	Loss 0.37311	TrainAcc 0.9074	ValidAcc 0.8988	TestAcc 0.7378
    Epoch 239:	Loss 0.36916	TrainAcc 0.9092	ValidAcc 0.8984	TestAcc 0.7383
    Epoch 249:	Loss 0.36717	TrainAcc 0.9088	ValidAcc 0.8992	TestAcc 0.7376
    Epoch 259:	Loss 0.36399	TrainAcc 0.9097	ValidAcc 0.8997	TestAcc 0.7374
    Epoch 269:	Loss 0.36108	TrainAcc 0.9106	ValidAcc 0.8995	TestAcc 0.7385
    Epoch 279:	Loss 0.35852	TrainAcc 0.9109	ValidAcc 0.9005	TestAcc 0.7392
    Epoch 289:	Loss 0.35582	TrainAcc 0.9113	ValidAcc 0.9022	TestAcc 0.7392
    Epoch 299:	Loss 0.35349	TrainAcc 0.9118	ValidAcc 0.9014	TestAcc 0.7390
    Epoch 309:	Loss 0.35157	TrainAcc 0.9126	ValidAcc 0.9015	TestAcc 0.7394
    Epoch 319:	Loss 0.34923	TrainAcc 0.9128	ValidAcc 0.9013	TestAcc 0.7401
    Epoch 329:	Loss 0.34797	TrainAcc 0.9134	ValidAcc 0.9029	TestAcc 0.7408
    Epoch 339:	Loss 0.34614	TrainAcc 0.9136	ValidAcc 0.9039	TestAcc 0.7396
    Epoch 349:	Loss 0.34381	TrainAcc 0.9142	ValidAcc 0.9034	TestAcc 0.7407
    Epoch 359:	Loss 0.34274	TrainAcc 0.9144	ValidAcc 0.9025	TestAcc 0.7407
    Epoch 369:	Loss 0.34017	TrainAcc 0.9151	ValidAcc 0.9025	TestAcc 0.7412
    Epoch 379:	Loss 0.33990	TrainAcc 0.9150	ValidAcc 0.9049	TestAcc 0.7416
    Epoch 389:	Loss 0.33699	TrainAcc 0.9155	ValidAcc 0.9033	TestAcc 0.7398
    Epoch 399:	Loss 0.33558	TrainAcc 0.9153	ValidAcc 0.9037	TestAcc 0.7402
    Epoch 409:	Loss 0.33347	TrainAcc 0.9159	ValidAcc 0.9044	TestAcc 0.7408
    Epoch 419:	Loss 0.33255	TrainAcc 0.9163	ValidAcc 0.9051	TestAcc 0.7402
    Epoch 429:	Loss 0.33087	TrainAcc 0.9172	ValidAcc 0.9058	TestAcc 0.7408
    Epoch 439:	Loss 0.32937	TrainAcc 0.9170	ValidAcc 0.9050	TestAcc 0.7403
    Epoch 449:	Loss 0.32894	TrainAcc 0.9169	ValidAcc 0.9046	TestAcc 0.7402
    Epoch 459:	Loss 0.32727	TrainAcc 0.9172	ValidAcc 0.9046	TestAcc 0.7413
    Epoch 469:	Loss 0.32619	TrainAcc 0.9172	ValidAcc 0.9042	TestAcc 0.7408
    Epoch 479:	Loss 0.32531	TrainAcc 0.9180	ValidAcc 0.9060	TestAcc 0.7399
    Epoch 489:	Loss 0.32334	TrainAcc 0.9179	ValidAcc 0.9065	TestAcc 0.7408
    Epoch 499:	Loss 0.32234	TrainAcc 0.9181	ValidAcc 0.9046	TestAcc 0.7407
    Epoch 509:	Loss 0.32199	TrainAcc 0.9184	ValidAcc 0.9059	TestAcc 0.7406
    Epoch 519:	Loss 0.32069	TrainAcc 0.9191	ValidAcc 0.9061	TestAcc 0.7421
    Epoch 529:	Loss 0.31953	TrainAcc 0.9191	ValidAcc 0.9061	TestAcc 0.7398
    Epoch 539:	Loss 0.31879	TrainAcc 0.9188	ValidAcc 0.9057	TestAcc 0.7402
    Epoch 549:	Loss 0.31781	TrainAcc 0.9193	ValidAcc 0.9067	TestAcc 0.7401
    Epoch 559:	Loss 0.31648	TrainAcc 0.9195	ValidAcc 0.9067	TestAcc 0.7397
    Epoch 569:	Loss 0.31554	TrainAcc 0.9198	ValidAcc 0.9075	TestAcc 0.7405
    Epoch 579:	Loss 0.31398	TrainAcc 0.9205	ValidAcc 0.9074	TestAcc 0.7398
    Epoch 589:	Loss 0.31405	TrainAcc 0.9204	ValidAcc 0.9081	TestAcc 0.7392
    Epoch 599:	Loss 0.31186	TrainAcc 0.9206	ValidAcc 0.9076	TestAcc 0.7399
    Epoch 609:	Loss 0.31136	TrainAcc 0.9207	ValidAcc 0.9068	TestAcc 0.7401
    Epoch 619:	Loss 0.31101	TrainAcc 0.9202	ValidAcc 0.9073	TestAcc 0.7396
    Epoch 629:	Loss 0.30991	TrainAcc 0.9209	ValidAcc 0.9070	TestAcc 0.7395
    Epoch 639:	Loss 0.30927	TrainAcc 0.9208	ValidAcc 0.9064	TestAcc 0.7393
    Epoch 649:	Loss 0.30780	TrainAcc 0.9213	ValidAcc 0.9080	TestAcc 0.7400
    Epoch 659:	Loss 0.30806	TrainAcc 0.9210	ValidAcc 0.9077	TestAcc 0.7397
    Epoch 669:	Loss 0.30712	TrainAcc 0.9216	ValidAcc 0.9079	TestAcc 0.7396
    Epoch 679:	Loss 0.30528	TrainAcc 0.9218	ValidAcc 0.9078	TestAcc 0.7388
    Epoch 689:	Loss 0.30464	TrainAcc 0.9218	ValidAcc 0.9084	TestAcc 0.7397
    Epoch 699:	Loss 0.30466	TrainAcc 0.9219	ValidAcc 0.9076	TestAcc 0.7387
    Epoch 709:	Loss 0.30452	TrainAcc 0.9218	ValidAcc 0.9081	TestAcc 0.7385
    Epoch 719:	Loss 0.30288	TrainAcc 0.9222	ValidAcc 0.9087	TestAcc 0.7378
    Epoch 729:	Loss 0.30233	TrainAcc 0.9219	ValidAcc 0.9071	TestAcc 0.7381
    Epoch 739:	Loss 0.30189	TrainAcc 0.9223	ValidAcc 0.9089	TestAcc 0.7394
    Epoch 749:	Loss 0.30085	TrainAcc 0.9224	ValidAcc 0.9088	TestAcc 0.7385
    Epoch 759:	Loss 0.29982	TrainAcc 0.9222	ValidAcc 0.9089	TestAcc 0.7382
    Epoch 769:	Loss 0.30000	TrainAcc 0.9223	ValidAcc 0.9092	TestAcc 0.7393
    Epoch 779:	Loss 0.29926	TrainAcc 0.9226	ValidAcc 0.9076	TestAcc 0.7391
    Epoch 789:	Loss 0.29899	TrainAcc 0.9227	ValidAcc 0.9089	TestAcc 0.7378
    Epoch 799:	Loss 0.29709	TrainAcc 0.9233	ValidAcc 0.9085	TestAcc 0.7381
    Epoch 809:	Loss 0.29677	TrainAcc 0.9235	ValidAcc 0.9082	TestAcc 0.7381
    Epoch 819:	Loss 0.29595	TrainAcc 0.9233	ValidAcc 0.9091	TestAcc 0.7378
    Epoch 829:	Loss 0.29575	TrainAcc 0.9231	ValidAcc 0.9088	TestAcc 0.7373
    Epoch 839:	Loss 0.29607	TrainAcc 0.9231	ValidAcc 0.9086	TestAcc 0.7372
    Epoch 849:	Loss 0.29458	TrainAcc 0.9237	ValidAcc 0.9087	TestAcc 0.7369
    Epoch 859:	Loss 0.29381	TrainAcc 0.9238	ValidAcc 0.9080	TestAcc 0.7366
    Epoch 869:	Loss 0.29290	TrainAcc 0.9239	ValidAcc 0.9093	TestAcc 0.7362
    Epoch 879:	Loss 0.29155	TrainAcc 0.9242	ValidAcc 0.9094	TestAcc 0.7366
    Epoch 889:	Loss 0.29288	TrainAcc 0.9236	ValidAcc 0.9097	TestAcc 0.7355
    Epoch 899:	Loss 0.29092	TrainAcc 0.9242	ValidAcc 0.9071	TestAcc 0.7356
    Epoch 909:	Loss 0.29096	TrainAcc 0.9252	ValidAcc 0.9101	TestAcc 0.7363
    Epoch 919:	Loss 0.29047	TrainAcc 0.9244	ValidAcc 0.9099	TestAcc 0.7356
    Epoch 929:	Loss 0.28877	TrainAcc 0.9250	ValidAcc 0.9086	TestAcc 0.7362
    Epoch 939:	Loss 0.28992	TrainAcc 0.9247	ValidAcc 0.9102	TestAcc 0.7360
    Epoch 949:	Loss 0.28766	TrainAcc 0.9254	ValidAcc 0.9098	TestAcc 0.7364
    Epoch 959:	Loss 0.28880	TrainAcc 0.9248	ValidAcc 0.9097	TestAcc 0.7352
    Epoch 969:	Loss 0.28836	TrainAcc 0.9251	ValidAcc 0.9094	TestAcc 0.7348
    Epoch 979:	Loss 0.28716	TrainAcc 0.9255	ValidAcc 0.9088	TestAcc 0.7364
    Epoch 989:	Loss 0.28615	TrainAcc 0.9255	ValidAcc 0.9089	TestAcc 0.7356
Node 0, Layer-level comm throughput (act): -nan GBps
Node 1, Layer-level comm throughput (act): 10.939 GBps
Node 2, Layer-level comm throughput (act): 10.880 GBps
Node 2, Layer-level comm throughput (grad): -nan GBps
Node 1, Layer-level comm throughput (grad): 11.309 GBps
Node 0, Layer-level comm throughput (grad): 11.055 GBps
    Epoch 999:	Loss 0.28501	TrainAcc 0.9255	ValidAcc 0.9092	TestAcc 0.7364
Node 0, GPU memory consumption: 11.193 GB
Node 0, compression time: 3.759s, compression size: 1167.788GB, throughput: 310.660GBps
Node 0, decompression time: 14.749s, compression size: 1167.788GB, throughput: 79.179GBps
Node 0, pure compute time: 75.589 s, total compute time: 94.096 s
Node 0, wait_for_task_time: 81.848 s, wait_for_other_gpus_time: 0.008 s
------------------------node id 0,  per-epoch time: 0.184320 s---------------
Node 2, GPU memory consumption: 10.417 GB
Node 2, compression time: 6.523s, compression size: 1167.788GB, throughput: 179.029GBps
Node 1, GPU memory consumption: 11.476 GB
Node 1, compression time: 10.406s, compression size: 2335.576GB, throughput: 224.448GBps
Node 1, decompression time: 63.635s, compression size: 2335.576GB, throughput: 36.703GBps
Node 1, pure compute time: 64.835 s, total compute time: 138.876 s
Node 2, decompression time: 19.272s, compression size: 1167.788GB, throughput: 60.594GBps
Node 2, pure compute time: 60.940 s, total compute time: 86.735 s
Node 2, wait_for_task_time: 36.477 s, wait_for_other_gpus_time: 0.008 s
------------------------node id 2,  per-epoch time: 0.184320 s---------------
Node 1, wait_for_task_time: 26.638 s, wait_for_other_gpus_time: 0.007 s
------------------------node id 1,  per-epoch time: 0.184320 s---------------
************ Profiling Results ************
	Bubble: 33.320340 (s) (18.01 percentage)
	Compute: 117.143474 (s) (63.31 percentage)
	GradSync: 1.060912 (s) (0.57 percentage)
	GraphComm: 0.170187 (s) (0.09 percentage)
	Imbalance: 17.221456 (s) (9.31 percentage)
	LayerComm: 16.108819 (s) (8.71 percentage)
	Layer-level communication (cluster-wide, per epoch): 1.378 GB
Highest valid_acc: 0.9102
Target test_acc: 0.7360
Epoch to reach the target acc: 940
[MPI Rank 2] Success 
[MPI Rank 0] Success 
[MPI Rank 1] Success 
