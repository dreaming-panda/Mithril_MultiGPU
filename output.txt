g003.anvil.rcac.purdue.edu
Sun Feb  5 12:46:02 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:41:00.0 Off |                    0 |
| N/A   28C    P0    50W / 400W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2    9) numactl/2.0.14
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0    10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0   11) openmpi/4.0.6
  4) gmp/6.2.1              8) zlib/1.2.11  12) boost/1.74.0

 

[  4%] Built target context
[ 18%] Built target parallel
[ 19%] Built target core
[ 39%] Built target cudahelp
[ 45%] Built target test_cuda_data_compression
[ 47%] Built target test_mpi_gpu_model_parallel
[ 50%] Built target test_mpi_gpu_pipelined_model_parallel
[ 50%] Built target test_mpi_gpu_hybrid
[ 53%] Built target test_cuda
[ 54%] Built target test_cuda_pipeline_parallel
[ 54%] Built target test_cuda_model_parallel
[ 81%] Built target test_mpi_loader
[ 81%] Built target test_graph
[ 81%] Built target test_mpi_pipelined_model_parallel
[ 81%] Built target test_trivial
[ 75%] Built target test_mpi_non_structual_graph
[ 81%] Built target test_nccl_thread
[ 81%] Built target test_cuda_graph
[ 81%] Built target test_full_structual_graph
[ 81%] Built target test_single_node_fullgpu_training
[ 81%] Built target test_hello_world
[ 81%] Built target test_full_non_structual_graph
[ 81%] Built target test_mpi_structual_graph
[ 81%] Built target test_mpi_combined
[ 86%] Built target test_single_node_training
[ 89%] Built target test_nccl_mpi
[ 90%] Built target test_mpi_model_parallel
[ 91%] Built target test_single_node_gpu_training
[ 91%] Built target estimate_comm_volume
[ 95%] Built target OSDI2023_SINGLE_NODE_gcn
[ 97%] Built target OSDI2023_SINGLE_NODE_gcn_inference
[100%] Built target OSDI2023_MULTI_NODES_gcn
[100%] Built target test_two_layer_hybrid_parallelism_designer
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/reddit
The number of GCN layers: 8
The number of hidden units: 128
The number of training epoches: 300
The number of startup epoches: 0
Learning rate: 0.003000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: saved_weights_pipe
The random seed: 1234
The scaling down factor of out-of-chunk gradients: 1.000000
Initialized node 0 on machine g003.anvil.rcac.purdue.edu
Building the CSR structure...
        It takes 1.721 seconds.
Building the CSC structure...
        It takes 1.683 seconds.
Building the Feature Vector...
        It takes 0.317 seconds.
Building the Label Vector...
        It takes 0.094 seconds.
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
train nodes 153431, valid nodes 23831, test nodes 55703
Number of GPUs: 1
GPU 0, layer [0, 8)
WARNING: the current version only applies to linear GNN models!
*** Node 0, starting model training...
Number of operators: 40
0 232965 0 40
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the partition [0, 40) x [0, 232965)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_WEIGHT, output tensors: 1
    Op 2: type OPERATOR_MATMUL, output tensors: 2
    Op 3: type OPERATOR_AGGREGATION, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_WEIGHT, output tensors: 6
    Op 7: type OPERATOR_MATMUL, output tensors: 7
    Op 8: type OPERATOR_AGGREGATION, output tensors: 8
    Op 9: type OPERATOR_RELU, output tensors: 9
    Op 10: type OPERATOR_DROPOUT, output tensors: 10
    Op 11: type OPERATOR_WEIGHT, output tensors: 11
    Op 12: type OPERATOR_MATMUL, output tensors: 12
    Op 13: type OPERATOR_AGGREGATION, output tensors: 13
    Op 14: type OPERATOR_RELU, output tensors: 14
    Op 15: type OPERATOR_DROPOUT, output tensors: 15
    Op 16: type OPERATOR_WEIGHT, output tensors: 16
    Op 17: type OPERATOR_MATMUL, output tensors: 17
    Op 18: type OPERATOR_AGGREGATION, output tensors: 18
    Op 19: type OPERATOR_RELU, output tensors: 19
    Op 20: type OPERATOR_DROPOUT, output tensors: 20
    Op 21: type OPERATOR_WEIGHT, output tensors: 21
    Op 22: type OPERATOR_MATMUL, output tensors: 22
    Op 23: type OPERATOR_AGGREGATION, output tensors: 23
    Op 24: type OPERATOR_RELU, output tensors: 24
    Op 25: type OPERATOR_DROPOUT, output tensors: 25
    Op 26: type OPERATOR_WEIGHT, output tensors: 26
    Op 27: type OPERATOR_MATMUL, output tensors: 27
    Op 28: type OPERATOR_AGGREGATION, output tensors: 28
    Op 29: type OPERATOR_RELU, output tensors: 29
    Op 30: type OPERATOR_DROPOUT, output tensors: 30
    Op 31: type OPERATOR_WEIGHT, output tensors: 31
    Op 32: type OPERATOR_MATMUL, output tensors: 32
    Op 33: type OPERATOR_AGGREGATION, output tensors: 33
    Op 34: type OPERATOR_RELU, output tensors: 34
    Op 35: type OPERATOR_DROPOUT, output tensors: 35
    Op 36: type OPERATOR_WEIGHT, output tensors: 36
    Op 37: type OPERATOR_MATMUL, output tensors: 37
    Op 38: type OPERATOR_AGGREGATION, output tensors: 38
    Op 39: type OPERATOR_SOFTMAX, output tensors: 39
Boundaries: 0 232965
Fragments: [0, 232965)
Chunks (number of global chunks: 16): 0-[0, 14561) 1-[14561, 29122) 2-[29122, 43683) 3-[43683, 58244) 4-[58244, 72805) 5-[72805, 87366) 6-[87366, 101927) 7-[101927, 116488) 8-[116488, 131049) ... 15-[218415, 232965)
(Forwarding) Node 0 (fragment 0) depends on nodes:
(Backwarding) Node 0 (fragment 0) depends on nodes:
(I-link dependencies): node 0 should send activation to nodes:
(I-link dependencies): node 0 should receive activation from nodes:
(I-link dependencies): node 0 should send gradient to nodes:
(I-link dependencies): node 0 should receive gradient from nodes:
232965, 114848857, 114848857
Number of vertices per chunk: 14561
csr in-out ready !*** Node 0, setting up some other necessary information...
*** Node 0, starting the helper threads...
+++++++++ Node 0 initializing the weights for op[0, 40)...
+++++++++ Node 0, mapping weight op 1
+++++++++ Node 0, mapping weight op 6
+++++++++ Node 0, mapping weight op 11
+++++++++ Node 0, mapping weight op 16
+++++++++ Node 0, mapping weight op 21
+++++++++ Node 0, mapping weight op 26
+++++++++ Node 0, mapping weight op 31
+++++++++ Node 0, mapping weight op 36
RANDOMLY DISPATCH THE CHUNKS...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.003000000
    Epoch 9:	Loss 5.12921	TrainAcc 0.1695	ValidAcc 0.2151	TestAcc 0.2128
    Epoch 19:	Loss 2.97743	TrainAcc 0.2461	ValidAcc 0.2775	TestAcc 0.2745
    Epoch 29:	Loss 2.53266	TrainAcc 0.2956	ValidAcc 0.3227	TestAcc 0.3205
    Epoch 39:	Loss 2.59204	TrainAcc 0.3040	ValidAcc 0.3168	TestAcc 0.3177
    Epoch 49:	Loss 2.42786	TrainAcc 0.3341	ValidAcc 0.3642	TestAcc 0.3603
    Epoch 59:	Loss 2.19892	TrainAcc 0.4097	ValidAcc 0.4342	TestAcc 0.4305
    Epoch 69:	Loss 2.05236	TrainAcc 0.3393	ValidAcc 0.3607	TestAcc 0.3563
    Epoch 79:	Loss 1.98118	TrainAcc 0.4167	ValidAcc 0.4335	TestAcc 0.4309
    Epoch 89:	Loss 1.84385	TrainAcc 0.4786	ValidAcc 0.4968	TestAcc 0.4972
    Epoch 99:	Loss 1.61380	TrainAcc 0.5424	ValidAcc 0.5782	TestAcc 0.5741
    Epoch 109:	Loss 1.39109	TrainAcc 0.5923	ValidAcc 0.6274	TestAcc 0.6243
    Epoch 119:	Loss 1.47029	TrainAcc 0.5785	ValidAcc 0.6089	TestAcc 0.6093
    Epoch 129:	Loss 1.49852	TrainAcc 0.5871	ValidAcc 0.6121	TestAcc 0.6082
    Epoch 139:	Loss 1.58914	TrainAcc 0.5812	ValidAcc 0.6056	TestAcc 0.6094
    Epoch 149:	Loss 1.42109	TrainAcc 0.5799	ValidAcc 0.6117	TestAcc 0.6047
    Epoch 159:	Loss 1.28079	TrainAcc 0.6688	ValidAcc 0.6902	TestAcc 0.6881
    Epoch 169:	Loss 1.18327	TrainAcc 0.6837	ValidAcc 0.7092	TestAcc 0.7046
    Epoch 179:	Loss 1.06116	TrainAcc 0.7234	ValidAcc 0.7380	TestAcc 0.7343
    Epoch 189:	Loss 1.02963	TrainAcc 0.7137	ValidAcc 0.7306	TestAcc 0.7283
    Epoch 199:	Loss 1.00720	TrainAcc 0.7203	ValidAcc 0.7390	TestAcc 0.7375
    Epoch 209:	Loss 1.04894	TrainAcc 0.7241	ValidAcc 0.7423	TestAcc 0.7384
    Epoch 219:	Loss 1.07364	TrainAcc 0.7132	ValidAcc 0.7298	TestAcc 0.7253
    Epoch 229:	Loss 1.03171	TrainAcc 0.7068	ValidAcc 0.7311	TestAcc 0.7253
    Epoch 239:	Loss 1.10019	TrainAcc 0.7200	ValidAcc 0.7356	TestAcc 0.7362
    Epoch 249:	Loss 1.05009	TrainAcc 0.7061	ValidAcc 0.7285	TestAcc 0.7205
    Epoch 259:	Loss 1.09052	TrainAcc 0.7052	ValidAcc 0.7289	TestAcc 0.7269
    Epoch 269:	Loss 0.97572	TrainAcc 0.7462	ValidAcc 0.7653	TestAcc 0.7593
    Epoch 279:	Loss 0.89533	TrainAcc 0.7474	ValidAcc 0.7626	TestAcc 0.7633
    Epoch 289:	Loss 0.84465	TrainAcc 0.7751	ValidAcc 0.7886	TestAcc 0.7833
Node 0, Layer-level comm throughput (act): -nan GBps
Node 0, Layer-level comm throughput (grad): -nan GBps
    Epoch 299:	Loss 0.85756	TrainAcc 0.7638	ValidAcc 0.7818	TestAcc 0.7811
Node 0, compression time: 0.000s, compression size: 0.000GB, throughput: -nanGBps
Node 0, decompression time: 0.000s, compression size: 0.000GB, throughput: -nanGBps
Node 0, pure compute time: 75.193 s, total compute time: 75.193 s
Node 0, wait_for_task_time: 0.001 s, wait_for_other_gpus_time: 0.000 s
------------------------node id 0,  per-epoch time: 0.263329 s---------------
************ Profiling Results ************
	Bubble: 0.771859 (s) (0.97 percentage)
	Compute: 78.395058 (s) (98.85 percentage)
	GradSync: 0.128571 (s) (0.16 percentage)
	GraphComm: 0.013144 (s) (0.02 percentage)
	Imbalance: 0.000264 (s) (0.00 percentage)
	LayerComm: 0.000000 (s) (0.00 percentage)
	Layer-level communication (cluster-wide, per epoch): 0.000 GB
Highest valid_acc: 0.7886
Target test_acc: 0.7833
Epoch to reach the target acc: 290
[MPI Rank 0] Success 
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/reddit
The number of GCN layers: 8
The number of hidden units: 128
The number of training epoches: 0
Learning rate: 0.000000
Initialized node g003.anvil.rcac.purdue.edu
Building the CSR structure...
        It takes 1.801 seconds.
Building the CSC structure...
        It takes 1.744 seconds.
Building the Feature Vector...
        It takes 0.300 seconds.
Building the Label Vector...
        It takes 0.047 seconds.
Number of classes: 41
Number of feature dimensions: 602
Dropout: 0.000 
train nodes 153431, valid nodes 23831, test nodes 55703
*** Allocating resources for all tensors...
    OP_TYPE: OPERATOR_INPUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_SOFTMAX
*** Done allocating resource.
*** Preparing the input tensor...
*** Done preparing the input tensor.
*** Preparing the STD tensor...
    Number of labels: 41
    Number of vertices: 232965
*** Done preparing the STD tensor.
Version 0	TrainAcc 0.1247	ValidAcc 0.1275	TestAcc 0.1217
Version 1	TrainAcc 0.3030	ValidAcc 0.3380	TestAcc 0.3347
Version 2	TrainAcc 0.3551	ValidAcc 0.3936	TestAcc 0.3940
Version 3	TrainAcc 0.3504	ValidAcc 0.3781	TestAcc 0.3789
Version 4	TrainAcc 0.3188	ValidAcc 0.3517	TestAcc 0.3482
Version 5	TrainAcc 0.3122	ValidAcc 0.3326	TestAcc 0.3320
Version 6	TrainAcc 0.3227	ValidAcc 0.3457	TestAcc 0.3431
Version 7	TrainAcc 0.4163	ValidAcc 0.4302	TestAcc 0.4263
Version 8	TrainAcc 0.4897	ValidAcc 0.5094	TestAcc 0.5081
Version 9	TrainAcc 0.5512	ValidAcc 0.5883	TestAcc 0.5885
Version 10	TrainAcc 0.5807	ValidAcc 0.6150	TestAcc 0.6169
Version 11	TrainAcc 0.6020	ValidAcc 0.6303	TestAcc 0.6319
Version 12	TrainAcc 0.5755	ValidAcc 0.5964	TestAcc 0.5935
Version 13	TrainAcc 0.6054	ValidAcc 0.6276	TestAcc 0.6271
Version 14	TrainAcc 0.5909	ValidAcc 0.6120	TestAcc 0.6047
Version 15	TrainAcc 0.6591	ValidAcc 0.6799	TestAcc 0.6763
Version 16	TrainAcc 0.7017	ValidAcc 0.7248	TestAcc 0.7196
Version 17	TrainAcc 0.7148	ValidAcc 0.7329	TestAcc 0.7332
Version 18	TrainAcc 0.7161	ValidAcc 0.7303	TestAcc 0.7278
Version 19	TrainAcc 0.7136	ValidAcc 0.7292	TestAcc 0.7268
Version 20	TrainAcc 0.7179	ValidAcc 0.7337	TestAcc 0.7304
Version 21	TrainAcc 0.7031	ValidAcc 0.7178	TestAcc 0.7149
Version 22	TrainAcc 0.7438	ValidAcc 0.7648	TestAcc 0.7572
Version 23	TrainAcc 0.7533	ValidAcc 0.7674	TestAcc 0.7677
Version 24	TrainAcc 0.6920	ValidAcc 0.7215	TestAcc 0.7122
Version 25	TrainAcc 0.7131	ValidAcc 0.7342	TestAcc 0.7322
Version 26	TrainAcc 0.7615	ValidAcc 0.7782	TestAcc 0.7721
Version 27	TrainAcc 0.7642	ValidAcc 0.7781	TestAcc 0.7780
Version 28	TrainAcc 0.7439	ValidAcc 0.7617	TestAcc 0.7583
Version 29	TrainAcc 0.7561	ValidAcc 0.7738	TestAcc 0.7742
Version 26 achieved the highest validation accuracy 0.7782 (test accuracy: 0.7721)
