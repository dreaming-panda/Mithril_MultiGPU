amadeus-MS-7B86
Mon May  8 14:26:35 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  On   | 00000000:29:00.0 Off |                  N/A |
| 52%   52C    P8     6W / 120W |    357MiB /  6144MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1331      G   /usr/lib/xorg/Xorg                101MiB |
|    0   N/A  N/A      2412      G   /usr/lib/xorg/Xorg                200MiB |
|    0   N/A  N/A      2537      G   /usr/bin/gnome-shell               44MiB |
+-----------------------------------------------------------------------------+
[ 11%] Built target context
[ 36%] Built target core
[ 77%] Built target cudahelp
[ 83%] Built target estimate_comm_volume
[ 88%] Built target OSDI2023_MULTI_NODES_gcn
[ 94%] Built target OSDI2023_MULTI_NODES_graphsage
[100%] Built target OSDI2023_MULTI_NODES_gcnii
Initialized node 0 on machine amadeus-MS-7B86
Building the CSR structure...
        It takes 0.035 seconds.
Building the CSC structure...
        It takes 0.035 seconds.
Building the Feature Vector...
        It takes 0.034 seconds.
Building the Label Vector...
        It takes 0.015 seconds.
The graph dataset locates at /home/amadeus/ssd512/gnn_datasets/reordered/ogbn_arxiv
The number of GCNII layers: 4
The number of hidden units: 256
The number of training epoches: 1000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 5
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 1
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 4)
*** Node 0, starting model training...
Number of operators: 20
0 169343 0 20
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the partition [0, 20) x [0, 169343)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_WEIGHT, output tensors: 1
    Op 2: type OPERATOR_MATMUL, output tensors: 2
    Op 3: type OPERATOR_AGGREGATION, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_AGGREGATION, output tensors: 6
    Op 7: type OPERATOR_WEIGHT, output tensors: 7
    Op 8: type OPERATOR_MATMUL, output tensors: 8
    Op 9: type OPERATOR_RELU, output tensors: 9
    Op 10: type OPERATOR_DROPOUT, output tensors: 10
    Op 11: type OPERATOR_AGGREGATION, output tensors: 11
    Op 12: type OPERATOR_WEIGHT, output tensors: 12
    Op 13: type OPERATOR_MATMUL, output tensors: 13
    Op 14: type OPERATOR_RELU, output tensors: 14
    Op 15: type OPERATOR_DROPOUT, output tensors: 15
    Op 16: type OPERATOR_AGGREGATION, output tensors: 16
    Op 17: type OPERATOR_WEIGHT, output tensors: 17
    Op 18: type OPERATOR_MATMUL, output tensors: 18
    Op 19: type OPERATOR_SOFTMAX, output tensors: 19
Boundaries: 0 169343
Fragments: [0, 169343)
Chunks (number of global chunks: 16): 0-[0, 10584) 1-[10584, 21168) 2-[21168, 31752) 3-[31752, 42336) 4-[42336, 52920) 5-[52920, 63504) 6-[63504, 74088) 7-[74088, 84672) 8-[84672, 95256) ... 15-[158760, 169343)
169343, 2484941, 2484941
Number of vertices per chunk: 10584
csr in-out ready !*** Node 0, setting up some other necessary information...
*** Node 0, starting the helper threads...
+++++++++ Node 0 initializing the weights for op[0, 20)...
+++++++++ Node 0, mapping weight op 1
using the Pytorch initialization method.
+++++++++ Node 0, mapping weight op 7
using the Pytorch initialization method.
+++++++++ Node 0, mapping weight op 12
using the Pytorch initialization method.
+++++++++ Node 0, mapping weight op 17
using the Pytorch initialization method.
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
	Epoch 10:	Loss 3.26528
	Epoch 20:	Loss 2.76873
	Epoch 30:	Loss 2.24477
	Epoch 40:	Loss 1.92264
	Epoch 50:	Loss 1.79855
	Epoch 60:	Loss 1.67918
	Epoch 70:	Loss 1.58030
	Epoch 80:	Loss 1.48741
	Epoch 90:	Loss 1.42895
	Epoch 100:	Loss 1.39013
	Epoch 110:	Loss 1.36484
	Epoch 120:	Loss 1.33750
	Epoch 130:	Loss 1.31655
	Epoch 140:	Loss 1.29228
	Epoch 150:	Loss 1.27662
	Epoch 160:	Loss 1.26257
	Epoch 170:	Loss 1.24832
	Epoch 180:	Loss 1.23515
	Epoch 190:	Loss 1.22208
	Epoch 200:	Loss 1.21188
	Epoch 210:	Loss 1.20363
	Epoch 220:	Loss 1.18843
	Epoch 230:	Loss 1.18410
	Epoch 240:	Loss 1.17218
	Epoch 250:	Loss 1.16389
	Epoch 260:	Loss 1.15703
	Epoch 270:	Loss 1.15011
	Epoch 280:	Loss 1.14612
	Epoch 290:	Loss 1.13815
	Epoch 300:	Loss 1.13303
	Epoch 310:	Loss 1.12692
	Epoch 320:	Loss 1.12214
	Epoch 330:	Loss 1.11847
	Epoch 340:	Loss 1.11144
	Epoch 350:	Loss 1.11027
	Epoch 360:	Loss 1.10464
	Epoch 370:	Loss 1.10044
	Epoch 380:	Loss 1.10042
	Epoch 390:	Loss 1.09440
	Epoch 400:	Loss 1.08635
	Epoch 410:	Loss 1.08577
	Epoch 420:	Loss 1.08093
	Epoch 430:	Loss 1.08016
	Epoch 440:	Loss 1.07619
	Epoch 450:	Loss 1.07112
	Epoch 460:	Loss 1.07158
	Epoch 470:	Loss 1.06760
	Epoch 480:	Loss 1.06522
	Epoch 490:	Loss 1.06185
	Epoch 500:	Loss 1.06021
	Epoch 510:	Loss 1.06123
	Epoch 520:	Loss 1.05428
	Epoch 530:	Loss 1.05420
	Epoch 540:	Loss 1.05378
	Epoch 550:	Loss 1.04786
	Epoch 560:	Loss 1.04626
	Epoch 570:	Loss 1.04470
	Epoch 580:	Loss 1.04226
	Epoch 590:	Loss 1.04633
	Epoch 600:	Loss 1.04332
	Epoch 610:	Loss 1.03988
	Epoch 620:	Loss 1.03898
	Epoch 630:	Loss 1.03405
	Epoch 640:	Loss 1.03543
	Epoch 650:	Loss 1.03227
	Epoch 660:	Loss 1.02949
	Epoch 670:	Loss 1.02788
	Epoch 680:	Loss 1.02739
	Epoch 690:	Loss 1.02416
	Epoch 700:	Loss 1.02009
	Epoch 710:	Loss 1.01849
	Epoch 720:	Loss 1.02213
	Epoch 730:	Loss 1.01821
	Epoch 740:	Loss 1.01499
	Epoch 750:	Loss 1.01753
	Epoch 760:	Loss 1.01351
	Epoch 770:	Loss 1.01332
	Epoch 780:	Loss 1.00975
	Epoch 790:	Loss 1.01078
	Epoch 800:	Loss 1.00681
	Epoch 810:	Loss 1.00691
	Epoch 820:	Loss 1.00559
	Epoch 830:	Loss 1.00381
	Epoch 840:	Loss 1.00529
	Epoch 850:	Loss 1.00071
	Epoch 860:	Loss 1.00165
	Epoch 870:	Loss 0.99929
	Epoch 880:	Loss 0.99710
	Epoch 890:	Loss 0.99787
	Epoch 900:	Loss 0.99508
	Epoch 910:	Loss 0.99564
	Epoch 920:	Loss 0.99213
	Epoch 930:	Loss 0.99244
	Epoch 940:	Loss 0.99263
	Epoch 950:	Loss 0.99050
	Epoch 960:	Loss 0.99017
	Epoch 970:	Loss 0.98747
	Epoch 980:	Loss 0.98740
	Epoch 990:	Loss 0.98820
Node 0, Layer-level comm throughput (act): -nan GBps
Node 0, Layer-level comm throughput (grad): -nan GBps
	Epoch 1000:	Loss 0.98728
Node 0, GPU memory consumption: 4.216 GB
Node 0, compression time: 0.000s, compression size: 0.000GB, throughput: -nanGBps
Node 0, decompression time: 0.000s, compression size: 0.000GB, throughput: -nanGBps
Node 0, pure compute time: 316.170 s, total compute time: 316.170 s
Node 0, wait_for_task_time: 17.021 s, wait_for_other_gpus_time: 0.000 s
------------------------node id 0,  per-epoch time: 0.403426 s---------------
************ Profiling Results ************
	Bubble: 24.497314 (s) (6.07 percentage)
	Compute: 349.380737 (s) (86.56 percentage)
	GradSync: 0.589007 (s) (0.15 percentage)
	GraphComm: 29.139205 (s) (7.22 percentage)
	Imbalance: 0.000186 (s) (0.00 percentage)
	LayerComm: 0.000000 (s) (0.00 percentage)
	Layer-level communication (cluster-wide, per epoch): 0.000 GB
Highest valid_acc: 0.0000
Target test_acc: 0.0000
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
