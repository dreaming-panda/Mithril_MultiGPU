g011.anvil.rcac.purdue.edu
Tue Jan 10 14:14:14 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:01:00.0 Off |                    0 |
| N/A   29C    P0    51W / 400W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-SXM...  On   | 00000000:41:00.0 Off |                    0 |
| N/A   27C    P0    49W / 400W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA A100-SXM...  On   | 00000000:81:00.0 Off |                    0 |
| N/A   28C    P0    49W / 400W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA A100-SXM...  On   | 00000000:C1:00.0 Off |                    0 |
| N/A   28C    P0    50W / 400W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2    9) numactl/2.0.14
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0    10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0   11) openmpi/4.0.6
  4) gmp/6.2.1              8) zlib/1.2.11  12) boost/1.74.0

 

[  4%] Built target context
[ 13%] Built target parallel
[ 19%] Built target core
Scanning dependencies of target cudahelp
[ 20%] Building CXX object CMakeFiles/cudahelp.dir/core/src/cuda/cuda_hybrid_parallel.cc.o
[ 21%] Linking CXX static library libcudahelp.a
[ 39%] Built target cudahelp
[ 50%] Linking CXX executable test_trivial
[ 51%] Linking CXX executable test_graph
[ 51%] Linking CXX executable test_mpi_loader
[ 51%] Linking CXX executable test_mpi_structual_graph
[ 51%] Linking CXX executable test_cuda_data_compression
[ 51%] Linking CXX executable estimate_comm_volume
[ 51%] Linking CXX executable test_full_non_structual_graph
[ 51%] Linking CXX executable test_mpi_non_structual_graph
[ 51%] Linking CXX executable test_full_structual_graph
[ 51%] Linking CXX executable test_hello_world
[ 55%] Linking CXX executable test_cuda
[ 55%] Linking CXX executable test_nccl_thread
[ 55%] Linking CXX executable test_cuda_graph
[ 55%] Linking CXX executable test_nccl_mpi
[ 55%] Linking CXX executable test_mpi_combined
[ 56%] Linking CXX executable test_mpi_gpu_hybrid
[ 58%] Linking CXX executable test_mpi_gpu_pipelined_model_parallel
[ 65%] Linking CXX executable test_single_node_training
[ 65%] Linking CXX executable test_mpi_gpu_model_parallel
[ 65%] Linking CXX executable test_mpi_pipelined_model_parallel
[ 65%] Linking CXX executable test_cuda_pipeline_parallel
[ 65%] Linking CXX executable test_mpi_model_parallel
[ 65%] Linking CXX executable test_cuda_model_parallel
[ 65%] Linking CXX executable test_single_node_fullgpu_training
[ 65%] Linking CXX executable test_single_node_gpu_training
[ 69%] Linking CXX executable gcn_inference
[ 69%] Linking CXX executable test_two_layer_hybrid_parallelism_designer
[ 69%] Linking CXX executable gcn
[ 69%] Linking CXX executable gcn
[ 70%] Built target test_full_structual_graph
[ 73%] Built target test_mpi_structual_graph
[ 75%] Built target test_mpi_combined
[ 75%] Built target test_mpi_loader
[ 75%] Built target estimate_comm_volume
[ 76%] Built target test_mpi_non_structual_graph
[ 79%] Built target test_full_non_structual_graph
[ 79%] Built target test_hello_world
[ 79%] Built target test_cuda_data_compression
[ 80%] Built target test_graph
[ 82%] Built target test_cuda
[ 82%] Built target test_trivial
[ 83%] Built target test_single_node_training
[ 85%] Built target test_single_node_gpu_training
[ 85%] Built target test_cuda_graph
[ 87%] Built target test_mpi_model_parallel
[ 87%] Built target test_mpi_gpu_hybrid
[ 88%] Built target test_mpi_gpu_model_parallel
[ 91%] Built target test_mpi_gpu_pipelined_model_parallel
[ 92%] Built target test_mpi_pipelined_model_parallel
[ 91%] Built target test_two_layer_hybrid_parallelism_designer
[ 92%] Built target test_single_node_fullgpu_training
[ 93%] Built target OSDI2023_SINGLE_NODE_gcn_inference
[ 94%] Built target test_cuda_pipeline_parallel
[ 95%] Built target OSDI2023_SINGLE_NODE_gcn
[ 96%] Built target OSDI2023_MULTI_NODES_gcn
[ 97%] Built target test_nccl_thread
[ 98%] Built target test_nccl_mpi
[100%] Built target test_cuda_model_parallel
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/reddit
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/reddit
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/reddit
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/reddit
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
Initialized node g011.anvil.rcac.purdue.edu
Initialized node g011.anvil.rcac.purdue.edu
Initialized node g011.anvil.rcac.purdue.edu
Initialized node g011.anvil.rcac.purdue.edu
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.877 seconds.
Building the CSC structure...
        It takes 1.924 seconds.
Building the CSC structure...
        It takes 1.978 seconds.
Building the CSC structure...
        It takes 2.027 seconds.
Building the CSC structure...
        It takes 1.769 seconds.
        It takes 1.796 seconds.
        It takes 1.834 seconds.
        It takes 1.823 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.364 seconds.
Building the Label Vector...
        It takes 0.044 seconds.
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
        It takes 0.409 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
        It takes 0.398 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
        It takes 0.393 seconds.
Building the Label Vector...
        It takes 0.027 seconds.
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
train nodes 153431, valid nodes 23831, test nodes 55703
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
*** Node 3, starting model training...
Number of operators: 20
0 232965 0 6
0 232965 6 11
0 232965 11 16
0 232965 16 20
*** Node 3 owns the partition [16, 20) x [0, 232965)
*** Node 3, constructing the helper classes...
(Forwarding) Node 3 (fragment 0) depends on nodes: 2 (Tensor: 15)
(Backwarding) Node 3 (fragment 0) depends on nodes:
(I-link dependencies): node 3 should send activation to nodes:
(I-link dependencies): node 3 should receive activation from nodes:
(I-link dependencies): node 3 should send gradient to nodes:
(I-link dependencies): node 3 should receive gradient from nodes:
train nodes 153431, valid nodes 23831, test nodes 55703
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
*** Node 0, starting model training...
Number of operators: 20
0 232965 0 6
0 232965 6 11
0 232965 11 16
0 232965 16 20
*** Node 0 owns the partition [0, 6) x [0, 232965)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_WEIGHT, output tensors: 1
    Op 2: type OPERATOR_MATMUL, output tensors: 2
    Op 3: type OPERATOR_AGGREGATION, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_WEIGHT, output tensors: 6
    Op 7: type OPERATOR_MATMUL, output tensors: 7
    Op 8: type OPERATOR_AGGREGATION, output tensors: 8
    Op 9: type OPERATOR_RELU, output tensors: 9
    Op 10: type OPERATOR_DROPOUT, output tensors: 10
    Op 11: type OPERATOR_WEIGHT, output tensors: 11
    Op 12: type OPERATOR_MATMUL, output tensors: 12
    Op 13: type OPERATOR_AGGREGATION, output tensors: 13
    Op 14: type OPERATOR_RELU, output tensors: 14
    Op 15: type OPERATOR_DROPOUT, output tensors: 15
    Op 16: type OPERATOR_WEIGHT, output tensors: 16
    Op 17: type OPERATOR_MATMUL, output tensors: 17
    Op 18: type OPERATOR_AGGREGATION, output tensors: 18
    Op 19: type OPERATOR_SOFTMAX, output tensors: 19
train nodes 153431, valid nodes 23831, test nodes 55703
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
*** Node 1, starting model training...
Number of operators: 20
0 232965 0 6
0 232965 6 11
0 232965 11 16
0 232965 16 20
*** Node 1 owns the partition [6, 11) x [0, 232965)
*** Node 1, constructing the helper classes...
train nodes 153431, valid nodes 23831, test nodes 55703
Boundaries: 0 0 0 0 232965 232965 232965 232965
Fragments: [0, 232965)
Chunks (number of global chunks: 33): 0-[0, 7280) 1-[7280, 14560) 2-[14560, 21840) 3-[21840, 29120) 4-[29120, 36400) 5-[36400, 43680) 6-[43680, 50960) 7-[50960, 58240) 8-[58240, 65520) ... 32-[232960, 232965)
(Forwarding) Node 0 (fragment 0) depends on nodes:
(Backwarding) Node 0 (fragment 0) depends on nodes: 1 (Tensor: 5)
(I-link dependencies): node 0 should send activation to nodes:
(I-link dependencies): node 0 should receive activation from nodes:
(I-link dependencies): node 0 should send gradient to nodes:
(I-link dependencies): node 0 should receive gradient from nodes:
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
*** Node 2, starting model training...
Number of operators: 20
0 232965 0 6
0 232965 6 11
0 232965 11 16
0 232965 16 20
*** Node 2 owns the partition [11, 16) x [0, 232965)
*** Node 2, constructing the helper classes...
(Forwarding) Node 1 (fragment 0) depends on nodes: 0 (Tensor: 5)
(Backwarding) Node 1 (fragment 0) depends on nodes: 2 (Tensor: 10)
(I-link dependencies): node 1 should send activation to nodes:
(I-link dependencies): node 1 should receive activation from nodes:
(I-link dependencies): node 1 should send gradient to nodes:
(I-link dependencies): node 1 should receive gradient from nodes:
(Forwarding) Node 2 (fragment 0) depends on nodes: 1 (Tensor: 10)
(Backwarding) Node 2 (fragment 0) depends on nodes: 3 (Tensor: 15)
(I-link dependencies): node 2 should send activation to nodes:
(I-link dependencies): node 2 should receive activation from nodes:
(I-link dependencies): node 2 should send gradient to nodes:
(I-link dependencies): node 2 should receive gradient from nodes:
232965, 114848857, 114848857
232965, 114848857, 114848857
232965, 114848857, 114848857
232965, 114848857, 114848857
csr in-out ready !*** Node 1, setting up some other necessary information...
*** Node 1, starting the helper threads...
csr in-out ready !*** Node 0, setting up some other necessary information...
*** Node 0, starting the helper threads...
csr in-out ready !*** Node 3, setting up some other necessary information...
*** Node 3, starting the helper threads...
csr in-out ready !*** Node 2, setting up some other necessary information...
*** Node 2, starting the helper threads...
+++++++++ Node 2 initializing the weights for op[11, 16)...
+++++++++ Node 2, mapping weight op 11
+++++++++ Node 3 initializing the weights for op[16, 20)...
+++++++++ Node 3, mapping weight op 16
+++++++++ Node 1 initializing the weights for op[6, 11)...
+++++++++ Node 1, mapping weight op 6
+++++++++ Node 0 initializing the weights for op[0, 6)...
+++++++++ Node 0, mapping weight op 1
RANDOMLY DISPATCH THE CHUNKS...
*** Node 3, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
    Epoch 0:	Loss 3.71274	TrainAcc 0.0136	ValidAcc 0.0127	TestAcc 0.0129
    Epoch 1:	Loss 3.66738	TrainAcc 0.0594	ValidAcc 0.0778	TestAcc 0.0757
    Epoch 2:	Loss 3.54792	TrainAcc 0.1894	ValidAcc 0.2060	TestAcc 0.2000
    Epoch 3:	Loss 3.44217	TrainAcc 0.2300	ValidAcc 0.2401	TestAcc 0.2329
    Epoch 4:	Loss 3.35254	TrainAcc 0.3224	ValidAcc 0.3240	TestAcc 0.3196
    Epoch 5:	Loss 3.26484	TrainAcc 0.3347	ValidAcc 0.3378	TestAcc 0.3304
    Epoch 6:	Loss 3.17909	TrainAcc 0.3404	ValidAcc 0.3443	TestAcc 0.3375
    Epoch 7:	Loss 3.09174	TrainAcc 0.3770	ValidAcc 0.4016	TestAcc 0.3933
    Epoch 8:	Loss 2.99722	TrainAcc 0.4289	ValidAcc 0.4693	TestAcc 0.4634
    Epoch 9:	Loss 2.89886	TrainAcc 0.4424	ValidAcc 0.4813	TestAcc 0.4748
    Epoch 10:	Loss 2.79994	TrainAcc 0.4499	ValidAcc 0.4880	TestAcc 0.4817
    Epoch 11:	Loss 2.69810	TrainAcc 0.4546	ValidAcc 0.4918	TestAcc 0.4850
    Epoch 12:	Loss 2.59363	TrainAcc 0.4667	ValidAcc 0.5035	TestAcc 0.4962
    Epoch 13:	Loss 2.48364	TrainAcc 0.4846	ValidAcc 0.5193	TestAcc 0.5142
    Epoch 14:	Loss 2.36944	TrainAcc 0.5251	ValidAcc 0.5599	TestAcc 0.5556
    Epoch 15:	Loss 2.25987	TrainAcc 0.5585	ValidAcc 0.5947	TestAcc 0.5907
    Epoch 16:	Loss 2.18084	TrainAcc 0.5580	ValidAcc 0.5979	TestAcc 0.5920
    Epoch 17:	Loss 2.12369	TrainAcc 0.5281	ValidAcc 0.5717	TestAcc 0.5652
    Epoch 18:	Loss 2.06044	TrainAcc 0.4831	ValidAcc 0.5224	TestAcc 0.5166
    Epoch 19:	Loss 1.96558	TrainAcc 0.5071	ValidAcc 0.5512	TestAcc 0.5434
    Epoch 20:	Loss 1.86852	TrainAcc 0.5362	ValidAcc 0.5780	TestAcc 0.5696
    Epoch 21:	Loss 1.78102	TrainAcc 0.5949	ValidAcc 0.6249	TestAcc 0.6196
    Epoch 22:	Loss 1.71098	TrainAcc 0.6397	ValidAcc 0.6621	TestAcc 0.6591
    Epoch 23:	Loss 1.67663	TrainAcc 0.6495	ValidAcc 0.6703	TestAcc 0.6660
    Epoch 24:	Loss 1.66339	TrainAcc 0.6333	ValidAcc 0.6562	TestAcc 0.6529
    Epoch 25:	Loss 1.63725	TrainAcc 0.6432	ValidAcc 0.6654	TestAcc 0.6615
    Epoch 26:	Loss 1.57273	TrainAcc 0.6703	ValidAcc 0.6933	TestAcc 0.6903
    Epoch 27:	Loss 1.49736	TrainAcc 0.6749	ValidAcc 0.6997	TestAcc 0.6941
    Epoch 28:	Loss 1.43034	TrainAcc 0.6846	ValidAcc 0.7079	TestAcc 0.7020
    Epoch 29:	Loss 1.38939	TrainAcc 0.7058	ValidAcc 0.7301	TestAcc 0.7244
    Epoch 30:	Loss 1.36660	TrainAcc 0.6964	ValidAcc 0.7230	TestAcc 0.7178
    Epoch 31:	Loss 1.34735	TrainAcc 0.6739	ValidAcc 0.7057	TestAcc 0.6965
    Epoch 32:	Loss 1.33070	TrainAcc 0.6531	ValidAcc 0.6855	TestAcc 0.6760
    Epoch 33:	Loss 1.30889	TrainAcc 0.6649	ValidAcc 0.6982	TestAcc 0.6896
    Epoch 34:	Loss 1.27826	TrainAcc 0.6616	ValidAcc 0.6954	TestAcc 0.6888
    Epoch 35:	Loss 1.24118	TrainAcc 0.6902	ValidAcc 0.7155	TestAcc 0.7109
    Epoch 36:	Loss 1.19349	TrainAcc 0.7279	ValidAcc 0.7475	TestAcc 0.7451
    Epoch 37:	Loss 1.14722	TrainAcc 0.7343	ValidAcc 0.7553	TestAcc 0.7514
    Epoch 38:	Loss 1.11115	TrainAcc 0.7422	ValidAcc 0.7607	TestAcc 0.7559
    Epoch 39:	Loss 1.09034	TrainAcc 0.7579	ValidAcc 0.7744	TestAcc 0.7721
    Epoch 40:	Loss 1.05173	TrainAcc 0.7758	ValidAcc 0.7911	TestAcc 0.7878
    Epoch 41:	Loss 1.01851	TrainAcc 0.7821	ValidAcc 0.7958	TestAcc 0.7925
    Epoch 42:	Loss 0.99024	TrainAcc 0.7757	ValidAcc 0.7904	TestAcc 0.7868
    Epoch 43:	Loss 0.99628	TrainAcc 0.7706	ValidAcc 0.7844	TestAcc 0.7786
    Epoch 44:	Loss 1.02462	TrainAcc 0.7612	ValidAcc 0.7775	TestAcc 0.7712
    Epoch 45:	Loss 1.05023	TrainAcc 0.7517	ValidAcc 0.7682	TestAcc 0.7613
    Epoch 46:	Loss 1.03665	TrainAcc 0.7419	ValidAcc 0.7609	TestAcc 0.7516
    Epoch 47:	Loss 0.98729	TrainAcc 0.7538	ValidAcc 0.7741	TestAcc 0.7646
    Epoch 48:	Loss 0.93435	TrainAcc 0.7705	ValidAcc 0.7888	TestAcc 0.7820
    Epoch 49:	Loss 0.88930	TrainAcc 0.8012	ValidAcc 0.8174	TestAcc 0.8118
    Epoch 50:	Loss 0.88063	TrainAcc 0.7942	ValidAcc 0.8076	TestAcc 0.8029
    Epoch 51:	Loss 0.91320	TrainAcc 0.7776	ValidAcc 0.7943	TestAcc 0.7893
    Epoch 52:	Loss 0.95482	TrainAcc 0.7707	ValidAcc 0.7879	TestAcc 0.7825
    Epoch 53:	Loss 0.98377	TrainAcc 0.7670	ValidAcc 0.7839	TestAcc 0.7774
    Epoch 54:	Loss 0.98518	TrainAcc 0.7681	ValidAcc 0.7838	TestAcc 0.7791
    Epoch 55:	Loss 0.94892	TrainAcc 0.7801	ValidAcc 0.7992	TestAcc 0.7959
    Epoch 56:	Loss 0.91239	TrainAcc 0.8068	ValidAcc 0.8238	TestAcc 0.8184
    Epoch 57:	Loss 0.88953	TrainAcc 0.8176	ValidAcc 0.8339	TestAcc 0.8283
    Epoch 58:	Loss 0.87869	TrainAcc 0.8181	ValidAcc 0.8334	TestAcc 0.8292
    Epoch 59:	Loss 0.87621	TrainAcc 0.8138	ValidAcc 0.8290	TestAcc 0.8246
    Epoch 60:	Loss 0.87089	TrainAcc 0.8099	ValidAcc 0.8258	TestAcc 0.8205
    Epoch 61:	Loss 0.87173	TrainAcc 0.8027	ValidAcc 0.8191	TestAcc 0.8152
    Epoch 62:	Loss 0.86340	TrainAcc 0.7948	ValidAcc 0.8113	TestAcc 0.8054
    Epoch 63:	Loss 0.84616	TrainAcc 0.7982	ValidAcc 0.8153	TestAcc 0.8097
    Epoch 64:	Loss 0.83163	TrainAcc 0.7963	ValidAcc 0.8128	TestAcc 0.8079
    Epoch 65:	Loss 0.81164	TrainAcc 0.7992	ValidAcc 0.8166	TestAcc 0.8111
    Epoch 66:	Loss 0.78321	TrainAcc 0.8161	ValidAcc 0.8305	TestAcc 0.8260
    Epoch 67:	Loss 0.75294	TrainAcc 0.8337	ValidAcc 0.8487	TestAcc 0.8451
    Epoch 68:	Loss 0.74286	TrainAcc 0.8459	ValidAcc 0.8613	TestAcc 0.8569
    Epoch 69:	Loss 0.75533	TrainAcc 0.8385	ValidAcc 0.8530	TestAcc 0.8479
    Epoch 70:	Loss 0.77643	TrainAcc 0.8290	ValidAcc 0.8447	TestAcc 0.8398
    Epoch 71:	Loss 0.79714	TrainAcc 0.8237	ValidAcc 0.8395	TestAcc 0.8337
    Epoch 72:	Loss 0.79673	TrainAcc 0.8259	ValidAcc 0.8392	TestAcc 0.8350
    Epoch 73:	Loss 0.79790	TrainAcc 0.8259	ValidAcc 0.8402	TestAcc 0.8350
    Epoch 74:	Loss 0.79272	TrainAcc 0.8325	ValidAcc 0.8472	TestAcc 0.8410
    Epoch 75:	Loss 0.77569	TrainAcc 0.8410	ValidAcc 0.8541	TestAcc 0.8482
    Epoch 76:	Loss 0.75521	TrainAcc 0.8453	ValidAcc 0.8579	TestAcc 0.8532
    Epoch 77:	Loss 0.73218	TrainAcc 0.8471	ValidAcc 0.8598	TestAcc 0.8548
    Epoch 78:	Loss 0.71146	TrainAcc 0.8593	ValidAcc 0.8688	TestAcc 0.8668
    Epoch 79:	Loss 0.69860	TrainAcc 0.8628	ValidAcc 0.8748	TestAcc 0.8702
    Epoch 80:	Loss 0.69398	TrainAcc 0.8554	ValidAcc 0.8666	TestAcc 0.8633
    Epoch 81:	Loss 0.69289	TrainAcc 0.8526	ValidAcc 0.8634	TestAcc 0.8604
    Epoch 82:	Loss 0.69841	TrainAcc 0.8510	ValidAcc 0.8618	TestAcc 0.8584
    Epoch 83:	Loss 0.70059	TrainAcc 0.8477	ValidAcc 0.8605	TestAcc 0.8560
    Epoch 84:	Loss 0.69562	TrainAcc 0.8468	ValidAcc 0.8593	TestAcc 0.8546
    Epoch 85:	Loss 0.68983	TrainAcc 0.8473	ValidAcc 0.8601	TestAcc 0.8558
    Epoch 86:	Loss 0.67539	TrainAcc 0.8510	ValidAcc 0.8616	TestAcc 0.8602
    Epoch 87:	Loss 0.66343	TrainAcc 0.8548	ValidAcc 0.8652	TestAcc 0.8629
    Epoch 88:	Loss 0.65034	TrainAcc 0.8630	ValidAcc 0.8760	TestAcc 0.8719
    Epoch 89:	Loss 0.64556	TrainAcc 0.8688	ValidAcc 0.8805	TestAcc 0.8789
    Epoch 90:	Loss 0.65136	TrainAcc 0.8572	ValidAcc 0.8705	TestAcc 0.8689
    Epoch 91:	Loss 0.65657	TrainAcc 0.8545	ValidAcc 0.8662	TestAcc 0.8651
    Epoch 92:	Loss 0.66210	TrainAcc 0.8527	ValidAcc 0.8659	TestAcc 0.8636
    Epoch 93:	Loss 0.65734	TrainAcc 0.8549	ValidAcc 0.8659	TestAcc 0.8627
    Epoch 94:	Loss 0.65566	TrainAcc 0.8560	ValidAcc 0.8678	TestAcc 0.8641
    Epoch 95:	Loss 0.64586	TrainAcc 0.8592	ValidAcc 0.8712	TestAcc 0.8682
    Epoch 96:	Loss 0.63448	TrainAcc 0.8636	ValidAcc 0.8765	TestAcc 0.8713
    Epoch 97:	Loss 0.62374	TrainAcc 0.8696	ValidAcc 0.8813	TestAcc 0.8785
    Epoch 98:	Loss 0.61846	TrainAcc 0.8724	ValidAcc 0.8833	TestAcc 0.8809
------------------------node id 3,  total time 10.154269s (per-epoch: 0.101543s)---------------
------------------------node id 2,  total time 10.154265s (per-epoch: 0.101543s)---------------
    Epoch 99:	Loss 0.61252	TrainAcc 0.8695	ValidAcc 0.8827	TestAcc 0.8794
------------------------node id 0,  total time 10.154270s (per-epoch: 0.101543s)---------------
------------------------node id 1,  total time 10.154260s (per-epoch: 0.101543s)---------------
************ Profiling Results ************
	Bubble: 1.777237 (s) (17.51 percentage)
	Compute: 5.955633 (s) (58.66 percentage)
	GradSync: 0.056789 (s) (0.56 percentage)
	GraphComm: 0.003678 (s) (0.04 percentage)
	Imbalance: 0.971758 (s) (9.57 percentage)
	LayerComm: 1.387143 (s) (13.66 percentage)
	Graph-level communication (cluster-wide, per epoch): 0.000 GB
	Layer-level communication (cluster-wide, per epoch): 0.667 GB
	Graph+Layer-level communication (cluster-wide, per epoch): 0.667 GB
	Parameter-server communication (cluster-wide, per epoch): 0.002 GB
	Graph-level dev2host communication time: 0.000 s, throughput: -nan GBps
	Graph-level memcpy communication time: 0.000 s, throughput: -nan GBps
	Graph-level net Activation communication time: 0.000 s, throughput: -nan GBps
	Graph-level net Gradient communication time: 0.000 s, throughput: -nan GBps
	Graph-level network batch size: -nan Bytes
Highest valid_acc: 0.8833
Target test_acc: 0.8809
Epoch to reach the target acc: 99
[MPI Rank 1] Success 
[MPI Rank 0] Success 
[MPI Rank 3] Success 
[MPI Rank 2] Success 
