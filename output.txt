Fri Sep 15 23:48:48 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 36%   58C    P5    39W / 230W |     51MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 48%   65C    P3    48W / 230W |      3MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 43%   62C    P5    41W / 230W |      3MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 41%   62C    P5    44W / 230W |      3MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
gnerv7
gnerv7
gnerv7
gnerv7
[ 15%] Built target context
[ 34%] Built target core
[ 73%] Built target cudahelp
[ 81%] Built target OSDI2023_MULTI_NODES_resgcn
[ 94%] Built target OSDI2023_MULTI_NODES_gcnii
[ 94%] Built target OSDI2023_MULTI_NODES_graphsage
[ 94%] Built target estimate_comm_volume
[ 94%] Built target OSDI2023_MULTI_NODES_gcn
Running experiments...
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INITInitialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 1 on machine gnerv7

Initialized node 3 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 2.374 seconds.
Building the CSC structure...
        It takes 2.402 seconds.
Building the CSC structure...
        It takes 2.530 seconds.
Building the CSC structure...
        It takes 2.624 seconds.
Building the CSC structure...
        It takes 2.308 seconds.
        It takes 2.293 seconds.
        It takes 2.249 seconds.
        It takes 2.313 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.296 seconds.
Building the Label Vector...
        It takes 0.294 seconds.
Building the Label Vector...
        It takes 0.039 seconds.
        It takes 0.038 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.272 seconds.
Building the Label Vector...
        It takes 0.293 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/reddit/4_parts
The number of GCNII layers: 16
The number of hidden units: 128
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 4
        It takes 0.032 seconds.
GPU 0, layer [0, 16)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 16)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
232965, 114848857, 114848857
Number of vertices per chunk: 58242
232965, 114848857, 114848857
Number of vertices per chunk: 58242
GPU 0, layer [0, 16)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 16)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 4): 0-[0, 58241) 1-[58241, 116483) 2-[116483, 174724) 3-[174724, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 58242
232965, 114848857, 114848857
Number of vertices per chunk: 58242
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 116.215 Gbps (per GPU), 464.859 Gbps (aggregated)
The layer-level communication performance: 116.935 Gbps (per GPU), 467.741 Gbps (aggregated)
The layer-level communication performance: 115.921 Gbps (per GPU), 463.683 Gbps (aggregated)
The layer-level communication performance: 116.777 Gbps (per GPU), 467.108 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.204 Gbps (per GPU), 632.816 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.201 Gbps (per GPU), 632.804 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.204 Gbps (per GPU), 632.816 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.207 Gbps (per GPU), 632.828 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.364 Gbps (per GPU), 417.458 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.362 Gbps (per GPU), 417.448 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.364 Gbps (per GPU), 417.458 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.364 Gbps (per GPU), 417.454 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0 25.63ms 21.07ms 21.72ms  1.22 58.24K 20.79M
 chk_1 48.79ms 44.12ms 44.95ms  1.11 58.24K 47.63M
 chk_2 31.06ms 26.95ms 27.42ms  1.15 58.24K 28.22M
 chk_3 24.80ms 20.22ms 20.81ms  1.23 58.24K 17.97M
   Avg 32.57 28.09 28.73
   Max 48.79 44.12 44.95
   Min 24.80 20.22 20.81
 Ratio  1.97  2.18  2.16
   Var 93.48 92.37 94.20
Profiling takes 4.072 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 167)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 58241
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 167)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 58241, Num Local Vertices: 58242
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 167)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 116483, Num Local Vertices: 58241
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 167)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 174724, Num Local Vertices: 58241
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 167)...
+++++++++ Node 3 initializing the weights for op[0, 167)...
+++++++++ Node 2 initializing the weights for op[0, 167)...
+++++++++ Node 1 initializing the weights for op[0, 167)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 340512
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 11.7756	TrainAcc 0.1506	ValidAcc 0.1605	TestAcc 0.1576	BestValid 0.1605
	Epoch 50:	Loss 0.8035	TrainAcc 0.8950	ValidAcc 0.9053	TestAcc 0.9003	BestValid 0.9053
	Epoch 100:	Loss 0.4666	TrainAcc 0.9367	ValidAcc 0.9412	TestAcc 0.9405	BestValid 0.9412
	Epoch 150:	Loss 0.3806	TrainAcc 0.9440	ValidAcc 0.9474	TestAcc 0.9469	BestValid 0.9474
	Epoch 200:	Loss 0.3336	TrainAcc 0.9479	ValidAcc 0.9504	TestAcc 0.9499	BestValid 0.9504
	Epoch 250:	Loss 0.3028	TrainAcc 0.9499	ValidAcc 0.9522	TestAcc 0.9514	BestValid 0.9522
	Epoch 300:	Loss 0.2808	TrainAcc 0.9515	ValidAcc 0.9540	TestAcc 0.9523	BestValid 0.9540
	Epoch 350:	Loss 0.2658	TrainAcc 0.9528	ValidAcc 0.9545	TestAcc 0.9532	BestValid 0.9545
	Epoch 400:	Loss 0.2514	TrainAcc 0.9540	ValidAcc 0.9548	TestAcc 0.9539	BestValid 0.9548
	Epoch 450:	Loss 0.2433	TrainAcc 0.9545	ValidAcc 0.9548	TestAcc 0.9542	BestValid 0.9548
	Epoch 500:	Loss 0.2350	TrainAcc 0.9558	ValidAcc 0.9562	TestAcc 0.9550	BestValid 0.9562
	Epoch 550:	Loss 0.2277	TrainAcc 0.9558	ValidAcc 0.9561	TestAcc 0.9552	BestValid 0.9562
	Epoch 600:	Loss 0.2179	TrainAcc 0.9571	ValidAcc 0.9573	TestAcc 0.9558	BestValid 0.9573
	Epoch 650:	Loss 0.2164	TrainAcc 0.9572	ValidAcc 0.9575	TestAcc 0.9559	BestValid 0.9575
	Epoch 700:	Loss 0.2102	TrainAcc 0.9575	ValidAcc 0.9578	TestAcc 0.9560	BestValid 0.9578
	Epoch 750:	Loss 0.2048	TrainAcc 0.9581	ValidAcc 0.9579	TestAcc 0.9566	BestValid 0.9579
	Epoch 800:	Loss 0.1988	TrainAcc 0.9583	ValidAcc 0.9578	TestAcc 0.9566	BestValid 0.9579
	Epoch 850:	Loss 0.1987	TrainAcc 0.9587	ValidAcc 0.9582	TestAcc 0.9569	BestValid 0.9582
	Epoch 900:	Loss 0.1926	TrainAcc 0.9589	ValidAcc 0.9581	TestAcc 0.9568	BestValid 0.9582
	Epoch 950:	Loss 0.1897	TrainAcc 0.9594	ValidAcc 0.9587	TestAcc 0.9573	BestValid 0.9587
	Epoch 1000:	Loss 0.1843	TrainAcc 0.9600	ValidAcc 0.9590	TestAcc 0.9574	BestValid 0.9590
	Epoch 1050:	Loss 0.1826	TrainAcc 0.9602	ValidAcc 0.9589	TestAcc 0.9578	BestValid 0.9590
	Epoch 1100:	Loss 0.1779	TrainAcc 0.9602	ValidAcc 0.9593	TestAcc 0.9576	BestValid 0.9593
	Epoch 1150:	Loss 0.1764	TrainAcc 0.9608	ValidAcc 0.9589	TestAcc 0.9578	BestValid 0.9593
	Epoch 1200:	Loss 0.1756	TrainAcc 0.9611	ValidAcc 0.9593	TestAcc 0.9580	BestValid 0.9593
	Epoch 1250:	Loss 0.1735	TrainAcc 0.9615	ValidAcc 0.9593	TestAcc 0.9583	BestValid 0.9593
	Epoch 1300:	Loss 0.1714	TrainAcc 0.9617	ValidAcc 0.9599	TestAcc 0.9585	BestValid 0.9599
	Epoch 1350:	Loss 0.1666	TrainAcc 0.9619	ValidAcc 0.9596	TestAcc 0.9586	BestValid 0.9599
	Epoch 1400:	Loss 0.1668	TrainAcc 0.9620	ValidAcc 0.9598	TestAcc 0.9584	BestValid 0.9599
	Epoch 1450:	Loss 0.1640	TrainAcc 0.9622	ValidAcc 0.9598	TestAcc 0.9587	BestValid 0.9599
	Epoch 1500:	Loss 0.1606	TrainAcc 0.9624	ValidAcc 0.9600	TestAcc 0.9588	BestValid 0.9600
	Epoch 1550:	Loss 0.1600	TrainAcc 0.9624	ValidAcc 0.9595	TestAcc 0.9585	BestValid 0.9600
	Epoch 1600:	Loss 0.1589	TrainAcc 0.9625	ValidAcc 0.9599	TestAcc 0.9588	BestValid 0.9600
	Epoch 1650:	Loss 0.1579	TrainAcc 0.9629	ValidAcc 0.9600	TestAcc 0.9586	BestValid 0.9600
	Epoch 1700:	Loss 0.1554	TrainAcc 0.9633	ValidAcc 0.9601	TestAcc 0.9589	BestValid 0.9601
	Epoch 1750:	Loss 0.1533	TrainAcc 0.9632	ValidAcc 0.9601	TestAcc 0.9589	BestValid 0.9601
	Epoch 1800:	Loss 0.1517	TrainAcc 0.9637	ValidAcc 0.9600	TestAcc 0.9589	BestValid 0.9601
	Epoch 1850:	Loss 0.1498	TrainAcc 0.9636	ValidAcc 0.9604	TestAcc 0.9591	BestValid 0.9604
	Epoch 1900:	Loss 0.1503	TrainAcc 0.9638	ValidAcc 0.9602	TestAcc 0.9592	BestValid 0.9604
	Epoch 1950:	Loss 0.1465	TrainAcc 0.9647	ValidAcc 0.9610	TestAcc 0.9598	BestValid 0.9610
	Epoch 2000:	Loss 0.1459	TrainAcc 0.9646	ValidAcc 0.9607	TestAcc 0.9594	BestValid 0.9610
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd-gnerv7: error: *** JOB 3386 ON gnerv7 CANCELLED AT 2023-09-16T00:18:55 DUE TO TIME LIMIT ***
slurmstepd-gnerv7: error: *** STEP 3386.1 ON gnerv7 CANCELLED AT 2023-09-16T00:18:55 DUE TO TIME LIMIT ***
