g002.anvil.rcac.purdue.edu
Sun May 28 20:45:27 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:81:00.0 Off |                    0 |
| N/A   29C    P0    53W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2    9) numactl/2.0.14
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0    10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0   11) openmpi/4.0.6
  4) gmp/6.2.1              8) zlib/1.2.11  12) boost/1.74.0

 

[ 11%] Built target context
[ 36%] Built target core
[ 77%] Built target cudahelp
[ 83%] Built target estimate_comm_volume
[ 88%] Built target OSDI2023_MULTI_NODES_graphsage
[ 94%] Built target OSDI2023_MULTI_NODES_gcn
[100%] Built target OSDI2023_MULTI_NODES_gcnii
Initialized node 0 on machine g002.anvil.rcac.purdue.edu
Initialized node 1 on machine g004.anvil.rcac.purdue.edu
Initialized node 2 on machine g009.anvil.rcac.purdue.edu
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.830 seconds.
Building the CSC structure...
        It takes 1.835 seconds.
Building the CSC structure...
        It takes 1.839 seconds.
Building the CSC structure...
        It takes 1.769 seconds.
        It takes 1.776 seconds.
        It takes 1.785 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.437 seconds.
Building the Label Vector...
        It takes 0.483 seconds.
        It takes 0.454 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.075 seconds.
        It takes 0.074 seconds.
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/partitioned_graphs/reddit/16_parts
The number of GCNII layers: 6
The number of hidden units: 256
The number of training epoches: 100
Learning rate: 0.003000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 3
        It takes 0.073 seconds.
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
GPU 2, layer [4, 6)
*** Node 2, starting model training...
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [21, 30)
*** Node 2, constructing the helper classes...
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
GPU 2, layer [4, 6)
*** Node 1, starting model training...
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [11, 21)
*** Node 1, constructing the helper classes...
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
GPU 2, layer [4, 6)
WARNING: the current version only applies to linear GNN models!
*** Node 0, starting model training...
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 11)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_WEIGHT, output tensors: 1
    Op 2: type OPERATOR_MATMUL, output tensors: 2
    Op 3: type OPERATOR_AGGREGATION, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_AGGREGATION, output tensors: 6
    Op 7: type OPERATOR_WEIGHT, output tensors: 7
    Op 8: type OPERATOR_MATMUL, output tensors: 8
    Op 9: type OPERATOR_RELU, output tensors: 9
    Op 10: type OPERATOR_DROPOUT, output tensors: 10
    Op 11: type OPERATOR_AGGREGATION, output tensors: 11
    Op 12: type OPERATOR_WEIGHT, output tensors: 12
    Op 13: type OPERATOR_MATMUL, output tensors: 13
    Op 14: type OPERATOR_RELU, output tensors: 14
    Op 15: type OPERATOR_DROPOUT, output tensors: 15
    Op 16: type OPERATOR_AGGREGATION, output tensors: 16
    Op 17: type OPERATOR_WEIGHT, output tensors: 17
    Op 18: type OPERATOR_MATMUL, output tensors: 18
    Op 19: type OPERATOR_RELU, output tensors: 19
    Op 20: type OPERATOR_DROPOUT, output tensors: 20
    Op 21: type OPERATOR_AGGREGATION, output tensors: 21
    Op 22: type OPERATOR_WEIGHT, output tensors: 22
    Op 23: type OPERATOR_MATMUL, output tensors: 23
    Op 24: type OPERATOR_RELU, output tensors: 24
    Op 25: type OPERATOR_DROPOUT, output tensors: 25
    Op 26: type OPERATOR_AGGREGATION, output tensors: 26
    Op 27: type OPERATOR_WEIGHT, output tensors: 27
    Op 28: type OPERATOR_MATMUL, output tensors: 28
    Op 29: type OPERATOR_SOFTMAX, output tensors: 29
Chunks (number of global chunks: 16): 0-[0, 14136) 1-[14136, 29133) 2-[29133, 43268) 3-[43268, 57403) 4-[57403, 71538) 5-[71538, 86535) 6-[86535, 101510) 7-[101510, 115645) 8-[115645, 129779) ... 15-[218830, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 14561
232965, 114848857, 114848857
Number of vertices per chunk: 14561
232965, 114848857, 114848857
Number of vertices per chunk: 14561
csr in-out ready !*** Node 0, setting up some other necessary information...
csr in-out ready !*** Node 2, setting up some other necessary information...
csr in-out ready !*** Node 1, setting up some other necessary information...
*** Node 2, starting the helper threads...
*** Node 0, starting the helper threads...
+++++++++ Node 2 initializing the weights for op[21, 30)...
+++++++++ Node 2, mapping weight op 22
+++++++++ Node 2, mapping weight op 27
+++++++++ Node 0 initializing the weights for op[0, 11)...
+++++++++ Node 0, mapping weight op 1
+++++++++ Node 0, mapping weight op 7
*** Node 1, starting the helper threads...
+++++++++ Node 1 initializing the weights for op[11, 21)...
+++++++++ Node 1, mapping weight op 12
+++++++++ Node 1, mapping weight op 17
Node 0, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.003000000
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.003000000
The learning rate specified by the user: 0.003000000
	Epoch 10:	Loss 2.1985
	Epoch 20:	Loss 1.2592
	Epoch 30:	Loss 0.9416
	Epoch 40:	Loss 0.6938
	Epoch 50:	Loss 0.5561
	Epoch 60:	Loss 0.4902
	Epoch 70:	Loss 0.4465
	Epoch 80:	Loss 0.4146
	Epoch 90:	Loss 0.3912
Node 0, Layer-level comm throughput (act): -nan GBps
Node 1, Layer-level comm throughput (act): 10.010 GBps
Node 2, Layer-level comm throughput (act): 9.594 GBps
Node 2, Layer-level comm throughput (grad): -nan GBps
Node 1, Layer-level comm throughput (grad): 9.645 GBps
Node 0, Layer-level comm throughput (grad): 10.084 GBps
	Epoch 100:	Loss 0.3733
Node 0, GPU memory consumption: 6.415 GB
Node 0, compression time: 0.128s, compression size: 22.217GB, throughput: 173.606GBps
Node 0, decompression time: 0.241s, compression size: 22.217GB, throughput: 92.095GBps
Node 0, pure compute time: 14.393 s, total compute time: 14.762 s
Node 0, wait_for_task_time: 3.187 s, wait_for_other_gpus_time: 0.002 s
------------------------node id 0,  per-epoch time: 0.182489 s---------------
Node 2, GPU memory consumption: 5.537 GB
Node 2, compression time: 0.199s, compression size: 22.217GB, throughput: 111.691GBps
Node 2, decompression time: 0.265s, compression size: 22.217GB, throughput: 83.829GBps
Node 2, pure compute time: 12.753 s, total compute time: 13.217 s
Node 2, wait_for_task_time: 1.935 s, wait_for_other_gpus_time: 0.002 s
------------------------node id 2,  per-epoch time: 0.182489 s---------------
Node 1, GPU memory consumption: 5.880 GB
Node 1, compression time: 0.329s, compression size: 44.435GB, throughput: 135.182GBps
Node 1, decompression time: 0.656s, compression size: 44.435GB, throughput: 67.765GBps
Node 1, pure compute time: 13.898 s, total compute time: 14.883 s
Node 1, wait_for_task_time: 2.222 s, wait_for_other_gpus_time: 0.001 s
------------------------node id 1,  per-epoch time: 0.182490 s---------------
************ Profiling Results ************
	Bubble: 4.977792 (s) (23.58 percentage)
	Compute: 14.737357 (s) (69.82 percentage)
	GradSync: 0.144040 (s) (0.68 percentage)
	GraphComm: 0.003460 (s) (0.02 percentage)
	Imbalance: 1.114010 (s) (5.28 percentage)
	LayerComm: 0.131726 (s) (0.62 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.199 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.005 GB
	Total communication (cluster-wide, per-epoch): 0.204 GB
Highest valid_acc: 0.0000
Target test_acc: 0.0576
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 2] Success 
[MPI Rank 1] Success 
