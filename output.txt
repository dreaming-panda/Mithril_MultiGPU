g001.anvil.rcac.purdue.edu
Sat May 20 21:49:31 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:01:00.0 Off |                    0 |
| N/A   30C    P0    53W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2    9) numactl/2.0.14
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0    10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0   11) openmpi/4.0.6
  4) gmp/6.2.1              8) zlib/1.2.11  12) boost/1.74.0

 

[ 11%] Built target context
[ 36%] Built target core
[ 38%] Building CXX object CMakeFiles/cudahelp.dir/core/src/cuda/cuda_hybrid_parallel.cc.o
[ 41%] Building CUDA object CMakeFiles/cudahelp.dir/core/src/cuda/cuda_hybrid_parallel.cu.o
[ 44%] Linking CXX static library libcudahelp.a
[ 77%] Built target cudahelp
[ 80%] Linking CXX executable estimate_comm_volume
[ 83%] Building CXX object applications/async_multi_gpus/CMakeFiles/OSDI2023_MULTI_NODES_graphsage.dir/graphsage.cc.o
[ 88%] Building CXX object applications/async_multi_gpus/CMakeFiles/OSDI2023_MULTI_NODES_gcn.dir/gcn.cc.o
[ 88%] Building CXX object applications/async_multi_gpus/CMakeFiles/OSDI2023_MULTI_NODES_gcnii.dir/gcnii.cc.o
[ 91%] Built target estimate_comm_volume
[ 94%] Linking CXX executable gcn
[ 97%] Linking CXX executable graphsage
[100%] Linking CXX executable gcnii
[100%] Built target OSDI2023_MULTI_NODES_gcn
[100%] Built target OSDI2023_MULTI_NODES_graphsage
[100%] Built target OSDI2023_MULTI_NODES_gcnii
Initialized node 3 on machine g013.anvil.rcac.purdue.edu
Initialized node 1 on machine g007.anvil.rcac.purdue.edu
Initialized node 2 on machine g010.anvil.rcac.purdue.edu
Initialized node 0 on machine g001.anvil.rcac.purdue.edu
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.063 seconds.
Building the CSC structure...
        It takes 0.072 seconds.
Building the CSC structure...
        It takes 0.088 seconds.
Building the CSC structure...
        It takes 0.044 seconds.
Building the CSC structure...
        It takes 0.038 seconds.
        It takes 0.039 seconds.
        It takes 0.039 seconds.
        It takes 0.045 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.132 seconds.
        It takes 0.134 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.123 seconds.
Building the Label Vector...
        It takes 0.131 seconds.
Building the Label Vector...
        It takes 0.060 seconds.
        It takes 0.061 seconds.
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_arxiv
The number of GCNII layers: 8
The number of hidden units: 256
The number of training epoches: 3000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.000000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 4
        It takes 0.062 seconds.
        It takes 0.061 seconds.
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 3)
GPU 1, layer [3, 6)
GPU 2, layer [6, 8)
GPU 3, layer [8, 10)
WARNING: the current version only applies to linear GNN models!
*** Node 0, starting model training...
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 20)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_DROPOUT, output tensors: 1
    Op 2: type OPERATOR_WEIGHT, output tensors: 2
    Op 3: type OPERATOR_MATMUL, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_AGGREGATION, output tensors: 6
    Op 7: type OPERATOR_ADD, output tensors: 7
    Op 8: type OPERATOR_WEIGHT, output tensors: 8
    Op 9: type OPERATOR_MATMUL, output tensors: 9
    Op 10: type OPERATOR_ADD, output tensors: 10
    Op 11: type OPERATOR_RELU, output tensors: 11
    Op 12: type OPERATOR_DROPOUT, output tensors: 12
    Op 13: type OPERATOR_AGGREGATION, output tensors: 13
    Op 14: type OPERATOR_ADD, output tensors: 14
    Op 15: type OPERATOR_WEIGHT, output tensors: 15
    Op 16: type OPERATOR_MATMUL, output tensors: 16
    Op 17: type OPERATOR_ADD, output tensors: 17
    Op 18: type OPERATOR_RELU, output tensors: 18
    Op 19: type OPERATOR_DROPOUT, output tensors: 19
    Op 20: type OPERATOR_AGGREGATION, output tensors: 20
    Op 21: type OPERATOR_ADD, output tensors: 21
    Op 22: type OPERATOR_WEIGHT, output tensors: 22
    Op 23: type OPERATOR_MATMUL, output tensors: 23
    Op 24: type OPERATOR_ADD, output tensors: 24
    Op 25: type OPERATOR_RELU, output tensors: 25
    Op 26: type OPERATOR_DROPOUT, output tensors: 26
    Op 27: type OPERATOR_AGGREGATION, output tensors: 27
    Op 28: type OPERATOR_ADD, output tensors: 28
    Op 29: type OPERATOR_WEIGHT, output tensors: 29
    Op 30: type OPERATOR_MATMUL, output tensors: 30
    Op 31: type OPERATOR_ADD, output tensors: 31
    Op 32: type OPERATOR_RELU, output tensors: 32
    Op 33: type OPERATOR_DROPOUT, output tensors: 33
    Op 34: type OPERATOR_AGGREGATION, output tensors: 34
    Op 35: type OPERATOR_ADD, output tensors: 35
    Op 36: type OPERATOR_WEIGHT, output tensors: 36
    Op 37: type OPERATOR_MATMUL, output tensors: 37
    Op 38: type OPERATOR_ADD, output tensors: 38
    Op 39: type OPERATOR_RELU, output tensors: 39
    Op 40: type OPERATOR_DROPOUT, output tensors: 40
    Op 41: type OPERATOR_AGGREGATION, output tensors: 41
    Op 42: type OPERATOR_ADD, output tensors: 42
    Op 43: type OPERATOR_WEIGHT, output tensors: 43
    Op 44: type OPERATOR_MATMUL, output tensors: 44
    Op 45: type OPERATOR_ADD, output tensors: 45
    Op 46: type OPERATOR_RELU, output tensors: 46
    Op 47: type OPERATOR_DROPOUT, output tensors: 47
    Op 48: type OPERATOR_AGGREGATION, output tensors: 48
    Op 49: type OPERATOR_ADD, output tensors: 49
    Op 50: type OPERATOR_WEIGHT, output tensors: 50
    Op 51: type OPERATOR_MATMUL, output tensors: 51
    Op 52: type OPERATOR_ADD, output tensors: 52
    Op 53: type OPERATOR_RELU, output tensors: 53
    Op 54: type OPERATOR_DROPOUT, output tensors: 54
    Op 55: type OPERATOR_AGGREGATION, output tensors: 55
    Op 56: type OPERATOR_ADD, output tensors: 56
    Op 57: type OPERATOR_WEIGHT, output tensors: 57
    Op 58: type OPERATOR_MATMUL, output tensors: 58
    Op 59: type OPERATOR_ADD, output tensors: 59
    Op 60: type OPERATOR_RELU, output tensors: 60
    Op 61: type OPERATOR_DROPOUT, output tensors: 61
    Op 62: type OPERATOR_WEIGHT, output tensors: 62
    Op 63: type OPERATOR_MATMUL, output tensors: 63
    Op 64: type OPERATOR_SOFTMAX, output tensors: 64
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 3)
GPU 1, layer [3, 6)
GPU 2, layer [6, 8)
GPU 3, layer [8, 10)
*** Node 3, starting model training...
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [55, 65)
*** Node 3, constructing the helper classes...
Boundaries: 0 0 0 0 169343 169343 169343 169343
Fragments: [0, 169343)
Chunks (number of global chunks: 16): 0-[0, 10584) 1-[10584, 21168) 2-[21168, 31752) 3-[31752, 42336) 4-[42336, 52920) 5-[52920, 63504) 6-[63504, 74088) 7-[74088, 84672) 8-[84672, 95256) ... 15-[158760, 169343)
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 3)
GPU 1, layer [3, 6)
GPU 2, layer [6, 8)
GPU 3, layer [8, 10)
*** Node 2, starting model training...
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [41, 55)
*** Node 2, constructing the helper classes...
GPU 0, layer [0, 3)
GPU 1, layer [3, 6)
GPU 2, layer [6, 8)
GPU 3, layer [8, 10)
*** Node 1, starting model training...
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
WARNING: the current version only applies to linear GNN models!
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [20, 41)
*** Node 1, constructing the helper classes...
169343, 2484941, 2484941
Number of vertices per chunk: 10584
169343, 2484941, 2484941
Number of vertices per chunk: 10584
169343, 2484941, 2484941
Number of vertices per chunk: 10584
169343, 2484941, 2484941
Number of vertices per chunk: 10584
csr in-out ready !*** Node 0, setting up some other necessary information...
csr in-out ready !*** Node 3, setting up some other necessary information...
csr in-out ready !*** Node 1, setting up some other necessary information...
csr in-out ready !*** Node 2, setting up some other necessary information...
*** Node 3, starting the helper threads...
+++++++++ Node 3 initializing the weights for op[55, 65)...
+++++++++ Node 3, mapping weight op 57
+++++++++ Node 3, mapping weight op 62
*** Node 0, starting the helper threads...
+++++++++ Node 0 initializing the weights for op[0, 20)...
+++++++++ Node 0, mapping weight op 2
+++++++++ Node 0, mapping weight op 8
+++++++++ Node 0, mapping weight op 15
*** Node 1, starting the helper threads...
+++++++++ Node 1 initializing the weights for op[20, 41)...
+++++++++ Node 1, mapping weight op 22
+++++++++ Node 1, mapping weight op 29
+++++++++ Node 1, mapping weight op 36
*** Node 2, starting the helper threads...
+++++++++ Node 2 initializing the weights for op[41, 55)...
+++++++++ Node 2, mapping weight op 43
+++++++++ Node 2, mapping weight op 50
Node 0, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 0, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 10:	Loss 3.3250	TrainAcc 0.1791	ValidAcc 0.0763	BestValid 0.0763
	Epoch 20:	Loss 3.1052	TrainAcc 0.2651	ValidAcc 0.2597	BestValid 0.2597
	Epoch 30:	Loss 2.9028	TrainAcc 0.3020	ValidAcc 0.3014	BestValid 0.3014
	Epoch 40:	Loss 2.6756	TrainAcc 0.3649	ValidAcc 0.3485	BestValid 0.3485
	Epoch 50:	Loss 2.5435	TrainAcc 0.4288	ValidAcc 0.4087	BestValid 0.4087
	Epoch 60:	Loss 2.3902	TrainAcc 0.4678	ValidAcc 0.4511	BestValid 0.4511
	Epoch 70:	Loss 2.3054	TrainAcc 0.4976	ValidAcc 0.4846	BestValid 0.4846
	Epoch 80:	Loss 2.2157	TrainAcc 0.5100	ValidAcc 0.4970	BestValid 0.4970
	Epoch 90:	Loss 2.1618	TrainAcc 0.5326	ValidAcc 0.5260	BestValid 0.5260
	Epoch 100:	Loss 2.1010	TrainAcc 0.5419	ValidAcc 0.5265	BestValid 0.5265
	Epoch 110:	Loss 2.0617	TrainAcc 0.5553	ValidAcc 0.5488	BestValid 0.5488
	Epoch 120:	Loss 2.0152	TrainAcc 0.5597	ValidAcc 0.5465	BestValid 0.5488
	Epoch 130:	Loss 1.9951	TrainAcc 0.5698	ValidAcc 0.5651	BestValid 0.5651
	Epoch 140:	Loss 1.9493	TrainAcc 0.5735	ValidAcc 0.5666	BestValid 0.5666
	Epoch 150:	Loss 1.9312	TrainAcc 0.5799	ValidAcc 0.5781	BestValid 0.5781
	Epoch 160:	Loss 1.9040	TrainAcc 0.5827	ValidAcc 0.5787	BestValid 0.5787
	Epoch 170:	Loss 1.8818	TrainAcc 0.5875	ValidAcc 0.5857	BestValid 0.5857
	Epoch 180:	Loss 1.8602	TrainAcc 0.5905	ValidAcc 0.5888	BestValid 0.5888
	Epoch 190:	Loss 1.8397	TrainAcc 0.5931	ValidAcc 0.5900	BestValid 0.5900
	Epoch 200:	Loss 1.8237	TrainAcc 0.5959	ValidAcc 0.5941	BestValid 0.5941
	Epoch 210:	Loss 1.8180	TrainAcc 0.5989	ValidAcc 0.5981	BestValid 0.5981
	Epoch 220:	Loss 1.7969	TrainAcc 0.6000	ValidAcc 0.5987	BestValid 0.5987
	Epoch 230:	Loss 1.7823	TrainAcc 0.6028	ValidAcc 0.6044	BestValid 0.6044
	Epoch 240:	Loss 1.7734	TrainAcc 0.6040	ValidAcc 0.6035	BestValid 0.6044
	Epoch 250:	Loss 1.7668	TrainAcc 0.6075	ValidAcc 0.6115	BestValid 0.6115
	Epoch 260:	Loss 1.7468	TrainAcc 0.6081	ValidAcc 0.6087	BestValid 0.6115
	Epoch 270:	Loss 1.7466	TrainAcc 0.6108	ValidAcc 0.6128	BestValid 0.6128
	Epoch 280:	Loss 1.7277	TrainAcc 0.6120	ValidAcc 0.6132	BestValid 0.6132
	Epoch 290:	Loss 1.7275	TrainAcc 0.6131	ValidAcc 0.6135	BestValid 0.6135
	Epoch 300:	Loss 1.7161	TrainAcc 0.6144	ValidAcc 0.6149	BestValid 0.6149
	Epoch 310:	Loss 1.7126	TrainAcc 0.6159	ValidAcc 0.6179	BestValid 0.6179
	Epoch 320:	Loss 1.6974	TrainAcc 0.6171	ValidAcc 0.6169	BestValid 0.6179
	Epoch 330:	Loss 1.6977	TrainAcc 0.6190	ValidAcc 0.6219	BestValid 0.6219
	Epoch 340:	Loss 1.6915	TrainAcc 0.6199	ValidAcc 0.6214	BestValid 0.6219
	Epoch 350:	Loss 1.6819	TrainAcc 0.6216	ValidAcc 0.6229	BestValid 0.6229
	Epoch 360:	Loss 1.6805	TrainAcc 0.6226	ValidAcc 0.6237	BestValid 0.6237
	Epoch 370:	Loss 1.6694	TrainAcc 0.6240	ValidAcc 0.6254	BestValid 0.6254
	Epoch 380:	Loss 1.6640	TrainAcc 0.6246	ValidAcc 0.6246	BestValid 0.6254
	Epoch 390:	Loss 1.6644	TrainAcc 0.6255	ValidAcc 0.6284	BestValid 0.6284
	Epoch 400:	Loss 1.6583	TrainAcc 0.6259	ValidAcc 0.6270	BestValid 0.6284
	Epoch 410:	Loss 1.6521	TrainAcc 0.6274	ValidAcc 0.6310	BestValid 0.6310
	Epoch 420:	Loss 1.6461	TrainAcc 0.6276	ValidAcc 0.6295	BestValid 0.6310
	Epoch 430:	Loss 1.6396	TrainAcc 0.6287	ValidAcc 0.6316	BestValid 0.6316
	Epoch 440:	Loss 1.6412	TrainAcc 0.6294	ValidAcc 0.6308	BestValid 0.6316
	Epoch 450:	Loss 1.6348	TrainAcc 0.6300	ValidAcc 0.6318	BestValid 0.6318
	Epoch 460:	Loss 1.6333	TrainAcc 0.6309	ValidAcc 0.6326	BestValid 0.6326
	Epoch 470:	Loss 1.6243	TrainAcc 0.6327	ValidAcc 0.6352	BestValid 0.6352
	Epoch 480:	Loss 1.6201	TrainAcc 0.6331	ValidAcc 0.6353	BestValid 0.6353
	Epoch 490:	Loss 1.6181	TrainAcc 0.6336	ValidAcc 0.6354	BestValid 0.6354
	Epoch 500:	Loss 1.6114	TrainAcc 0.6339	ValidAcc 0.6347	BestValid 0.6354
	Epoch 510:	Loss 1.6092	TrainAcc 0.6351	ValidAcc 0.6363	BestValid 0.6363
	Epoch 520:	Loss 1.6064	TrainAcc 0.6354	ValidAcc 0.6363	BestValid 0.6363
	Epoch 530:	Loss 1.6046	TrainAcc 0.6362	ValidAcc 0.6363	BestValid 0.6363
	Epoch 540:	Loss 1.6022	TrainAcc 0.6369	ValidAcc 0.6373	BestValid 0.6373
	Epoch 550:	Loss 1.5956	TrainAcc 0.6378	ValidAcc 0.6377	BestValid 0.6377
	Epoch 560:	Loss 1.5887	TrainAcc 0.6381	ValidAcc 0.6379	BestValid 0.6379
	Epoch 570:	Loss 1.5982	TrainAcc 0.6398	ValidAcc 0.6393	BestValid 0.6393
	Epoch 580:	Loss 1.5892	TrainAcc 0.6387	ValidAcc 0.6379	BestValid 0.6393
	Epoch 590:	Loss 1.5903	TrainAcc 0.6407	ValidAcc 0.6412	BestValid 0.6412
	Epoch 600:	Loss 1.5808	TrainAcc 0.6407	ValidAcc 0.6392	BestValid 0.6412
	Epoch 610:	Loss 1.5765	TrainAcc 0.6417	ValidAcc 0.6424	BestValid 0.6424
	Epoch 620:	Loss 1.5742	TrainAcc 0.6418	ValidAcc 0.6411	BestValid 0.6424
	Epoch 630:	Loss 1.5765	TrainAcc 0.6424	ValidAcc 0.6423	BestValid 0.6424
	Epoch 640:	Loss 1.5691	TrainAcc 0.6432	ValidAcc 0.6413	BestValid 0.6424
	Epoch 650:	Loss 1.5690	TrainAcc 0.6441	ValidAcc 0.6440	BestValid 0.6440
	Epoch 660:	Loss 1.5690	TrainAcc 0.6443	ValidAcc 0.6443	BestValid 0.6443
	Epoch 670:	Loss 1.5678	TrainAcc 0.6450	ValidAcc 0.6431	BestValid 0.6443
	Epoch 680:	Loss 1.5660	TrainAcc 0.6454	ValidAcc 0.6443	BestValid 0.6443
	Epoch 690:	Loss 1.5662	TrainAcc 0.6455	ValidAcc 0.6439	BestValid 0.6443
	Epoch 700:	Loss 1.5657	TrainAcc 0.6452	ValidAcc 0.6430	BestValid 0.6443
	Epoch 710:	Loss 1.5571	TrainAcc 0.6464	ValidAcc 0.6461	BestValid 0.6461
	Epoch 720:	Loss 1.5522	TrainAcc 0.6468	ValidAcc 0.6450	BestValid 0.6461
	Epoch 730:	Loss 1.5542	TrainAcc 0.6473	ValidAcc 0.6467	BestValid 0.6467
	Epoch 740:	Loss 1.5520	TrainAcc 0.6469	ValidAcc 0.6456	BestValid 0.6467
	Epoch 750:	Loss 1.5424	TrainAcc 0.6483	ValidAcc 0.6476	BestValid 0.6476
	Epoch 760:	Loss 1.5491	TrainAcc 0.6474	ValidAcc 0.6453	BestValid 0.6476
	Epoch 770:	Loss 1.5444	TrainAcc 0.6489	ValidAcc 0.6481	BestValid 0.6481
	Epoch 780:	Loss 1.5454	TrainAcc 0.6489	ValidAcc 0.6466	BestValid 0.6481
	Epoch 790:	Loss 1.5427	TrainAcc 0.6495	ValidAcc 0.6491	BestValid 0.6491
	Epoch 800:	Loss 1.5435	TrainAcc 0.6498	ValidAcc 0.6484	BestValid 0.6491
	Epoch 810:	Loss 1.5380	TrainAcc 0.6495	ValidAcc 0.6482	BestValid 0.6491
	Epoch 820:	Loss 1.5377	TrainAcc 0.6503	ValidAcc 0.6483	BestValid 0.6491
	Epoch 830:	Loss 1.5328	TrainAcc 0.6502	ValidAcc 0.6496	BestValid 0.6496
	Epoch 840:	Loss 1.5361	TrainAcc 0.6505	ValidAcc 0.6487	BestValid 0.6496
	Epoch 850:	Loss 1.5340	TrainAcc 0.6502	ValidAcc 0.6487	BestValid 0.6496
	Epoch 860:	Loss 1.5356	TrainAcc 0.6512	ValidAcc 0.6499	BestValid 0.6499
	Epoch 870:	Loss 1.5321	TrainAcc 0.6514	ValidAcc 0.6498	BestValid 0.6499
	Epoch 880:	Loss 1.5291	TrainAcc 0.6518	ValidAcc 0.6497	BestValid 0.6499
	Epoch 890:	Loss 1.5279	TrainAcc 0.6520	ValidAcc 0.6500	BestValid 0.6500
	Epoch 900:	Loss 1.5299	TrainAcc 0.6523	ValidAcc 0.6496	BestValid 0.6500
	Epoch 910:	Loss 1.5276	TrainAcc 0.6528	ValidAcc 0.6510	BestValid 0.6510
	Epoch 920:	Loss 1.5294	TrainAcc 0.6530	ValidAcc 0.6509	BestValid 0.6510
	Epoch 930:	Loss 1.5242	TrainAcc 0.6525	ValidAcc 0.6502	BestValid 0.6510
	Epoch 940:	Loss 1.5234	TrainAcc 0.6533	ValidAcc 0.6512	BestValid 0.6512
	Epoch 950:	Loss 1.5235	TrainAcc 0.6535	ValidAcc 0.6511	BestValid 0.6512
	Epoch 960:	Loss 1.5197	TrainAcc 0.6536	ValidAcc 0.6514	BestValid 0.6514
	Epoch 970:	Loss 1.5215	TrainAcc 0.6546	ValidAcc 0.6520	BestValid 0.6520
	Epoch 980:	Loss 1.5252	TrainAcc 0.6543	ValidAcc 0.6518	BestValid 0.6520
	Epoch 990:	Loss 1.5175	TrainAcc 0.6551	ValidAcc 0.6525	BestValid 0.6525
	Epoch 1000:	Loss 1.5185	TrainAcc 0.6546	ValidAcc 0.6522	BestValid 0.6525
	Epoch 1010:	Loss 1.5144	TrainAcc 0.6547	ValidAcc 0.6522	BestValid 0.6525
	Epoch 1020:	Loss 1.5175	TrainAcc 0.6548	ValidAcc 0.6519	BestValid 0.6525
	Epoch 1030:	Loss 1.5210	TrainAcc 0.6553	ValidAcc 0.6528	BestValid 0.6528
	Epoch 1040:	Loss 1.5062	TrainAcc 0.6553	ValidAcc 0.6527	BestValid 0.6528
	Epoch 1050:	Loss 1.5174	TrainAcc 0.6560	ValidAcc 0.6538	BestValid 0.6538
	Epoch 1060:	Loss 1.5134	TrainAcc 0.6556	ValidAcc 0.6529	BestValid 0.6538
	Epoch 1070:	Loss 1.5099	TrainAcc 0.6565	ValidAcc 0.6551	BestValid 0.6551
	Epoch 1080:	Loss 1.5106	TrainAcc 0.6568	ValidAcc 0.6543	BestValid 0.6551
	Epoch 1090:	Loss 1.5088	TrainAcc 0.6567	ValidAcc 0.6536	BestValid 0.6551
	Epoch 1100:	Loss 1.5087	TrainAcc 0.6569	ValidAcc 0.6540	BestValid 0.6551
	Epoch 1110:	Loss 1.5062	TrainAcc 0.6563	ValidAcc 0.6535	BestValid 0.6551
	Epoch 1120:	Loss 1.5088	TrainAcc 0.6566	ValidAcc 0.6529	BestValid 0.6551
	Epoch 1130:	Loss 1.5049	TrainAcc 0.6576	ValidAcc 0.6554	BestValid 0.6554
	Epoch 1140:	Loss 1.5076	TrainAcc 0.6574	ValidAcc 0.6538	BestValid 0.6554
	Epoch 1150:	Loss 1.5048	TrainAcc 0.6579	ValidAcc 0.6540	BestValid 0.6554
	Epoch 1160:	Loss 1.5064	TrainAcc 0.6579	ValidAcc 0.6536	BestValid 0.6554
	Epoch 1170:	Loss 1.4985	TrainAcc 0.6583	ValidAcc 0.6551	BestValid 0.6554
	Epoch 1180:	Loss 1.4987	TrainAcc 0.6584	ValidAcc 0.6547	BestValid 0.6554
	Epoch 1190:	Loss 1.4990	TrainAcc 0.6583	ValidAcc 0.6540	BestValid 0.6554
	Epoch 1200:	Loss 1.5003	TrainAcc 0.6587	ValidAcc 0.6553	BestValid 0.6554
	Epoch 1210:	Loss 1.4987	TrainAcc 0.6592	ValidAcc 0.6567	BestValid 0.6567
	Epoch 1220:	Loss 1.4926	TrainAcc 0.6589	ValidAcc 0.6551	BestValid 0.6567
	Epoch 1230:	Loss 1.5007	TrainAcc 0.6595	ValidAcc 0.6566	BestValid 0.6567
	Epoch 1240:	Loss 1.4901	TrainAcc 0.6590	ValidAcc 0.6557	BestValid 0.6567
	Epoch 1250:	Loss 1.4871	TrainAcc 0.6599	ValidAcc 0.6569	BestValid 0.6569
	Epoch 1260:	Loss 1.5004	TrainAcc 0.6596	ValidAcc 0.6565	BestValid 0.6569
	Epoch 1270:	Loss 1.5011	TrainAcc 0.6596	ValidAcc 0.6552	BestValid 0.6569
	Epoch 1280:	Loss 1.4925	TrainAcc 0.6598	ValidAcc 0.6563	BestValid 0.6569
	Epoch 1290:	Loss 1.4922	TrainAcc 0.6605	ValidAcc 0.6577	BestValid 0.6577
	Epoch 1300:	Loss 1.4863	TrainAcc 0.6599	ValidAcc 0.6568	BestValid 0.6577
	Epoch 1310:	Loss 1.4907	TrainAcc 0.6604	ValidAcc 0.6577	BestValid 0.6577
	Epoch 1320:	Loss 1.4853	TrainAcc 0.6605	ValidAcc 0.6570	BestValid 0.6577
	Epoch 1330:	Loss 1.4918	TrainAcc 0.6600	ValidAcc 0.6572	BestValid 0.6577
	Epoch 1340:	Loss 1.4914	TrainAcc 0.6604	ValidAcc 0.6574	BestValid 0.6577
	Epoch 1350:	Loss 1.4853	TrainAcc 0.6612	ValidAcc 0.6579	BestValid 0.6579
	Epoch 1360:	Loss 1.4875	TrainAcc 0.6606	ValidAcc 0.6559	BestValid 0.6579
	Epoch 1370:	Loss 1.4847	TrainAcc 0.6613	ValidAcc 0.6585	BestValid 0.6585
	Epoch 1380:	Loss 1.4869	TrainAcc 0.6612	ValidAcc 0.6581	BestValid 0.6585
	Epoch 1390:	Loss 1.4898	TrainAcc 0.6613	ValidAcc 0.6594	BestValid 0.6594
	Epoch 1400:	Loss 1.4816	TrainAcc 0.6613	ValidAcc 0.6576	BestValid 0.6594
	Epoch 1410:	Loss 1.4841	TrainAcc 0.6621	ValidAcc 0.6601	BestValid 0.6601
	Epoch 1420:	Loss 1.4852	TrainAcc 0.6621	ValidAcc 0.6595	BestValid 0.6601
	Epoch 1430:	Loss 1.4856	TrainAcc 0.6616	ValidAcc 0.6574	BestValid 0.6601
	Epoch 1440:	Loss 1.4809	TrainAcc 0.6621	ValidAcc 0.6587	BestValid 0.6601
	Epoch 1450:	Loss 1.4806	TrainAcc 0.6619	ValidAcc 0.6587	BestValid 0.6601
	Epoch 1460:	Loss 1.4803	TrainAcc 0.6620	ValidAcc 0.6567	BestValid 0.6601
	Epoch 1470:	Loss 1.4838	TrainAcc 0.6623	ValidAcc 0.6598	BestValid 0.6601
	Epoch 1480:	Loss 1.4794	TrainAcc 0.6622	ValidAcc 0.6585	BestValid 0.6601
	Epoch 1490:	Loss 1.4802	TrainAcc 0.6623	ValidAcc 0.6580	BestValid 0.6601
	Epoch 1500:	Loss 1.4746	TrainAcc 0.6628	ValidAcc 0.6584	BestValid 0.6601
	Epoch 1510:	Loss 1.4824	TrainAcc 0.6625	ValidAcc 0.6589	BestValid 0.6601
	Epoch 1520:	Loss 1.4800	TrainAcc 0.6627	ValidAcc 0.6589	BestValid 0.6601
	Epoch 1530:	Loss 1.4778	TrainAcc 0.6628	ValidAcc 0.6599	BestValid 0.6601
	Epoch 1540:	Loss 1.4826	TrainAcc 0.6631	ValidAcc 0.6594	BestValid 0.6601
	Epoch 1550:	Loss 1.4816	TrainAcc 0.6632	ValidAcc 0.6599	BestValid 0.6601
	Epoch 1560:	Loss 1.4812	TrainAcc 0.6633	ValidAcc 0.6590	BestValid 0.6601
	Epoch 1570:	Loss 1.4750	TrainAcc 0.6636	ValidAcc 0.6622	BestValid 0.6622
	Epoch 1580:	Loss 1.4786	TrainAcc 0.6630	ValidAcc 0.6593	BestValid 0.6622
	Epoch 1590:	Loss 1.4752	TrainAcc 0.6635	ValidAcc 0.6595	BestValid 0.6622
	Epoch 1600:	Loss 1.4750	TrainAcc 0.6635	ValidAcc 0.6596	BestValid 0.6622
	Epoch 1610:	Loss 1.4705	TrainAcc 0.6639	ValidAcc 0.6597	BestValid 0.6622
	Epoch 1620:	Loss 1.4719	TrainAcc 0.6635	ValidAcc 0.6594	BestValid 0.6622
	Epoch 1630:	Loss 1.4746	TrainAcc 0.6635	ValidAcc 0.6599	BestValid 0.6622
	Epoch 1640:	Loss 1.4752	TrainAcc 0.6634	ValidAcc 0.6602	BestValid 0.6622
	Epoch 1650:	Loss 1.4752	TrainAcc 0.6641	ValidAcc 0.6613	BestValid 0.6622
	Epoch 1660:	Loss 1.4710	TrainAcc 0.6635	ValidAcc 0.6597	BestValid 0.6622
	Epoch 1670:	Loss 1.4680	TrainAcc 0.6644	ValidAcc 0.6618	BestValid 0.6622
	Epoch 1680:	Loss 1.4699	TrainAcc 0.6641	ValidAcc 0.6613	BestValid 0.6622
	Epoch 1690:	Loss 1.4725	TrainAcc 0.6643	ValidAcc 0.6613	BestValid 0.6622
	Epoch 1700:	Loss 1.4749	TrainAcc 0.6642	ValidAcc 0.6609	BestValid 0.6622
	Epoch 1710:	Loss 1.4615	TrainAcc 0.6650	ValidAcc 0.6621	BestValid 0.6622
	Epoch 1720:	Loss 1.4752	TrainAcc 0.6645	ValidAcc 0.6619	BestValid 0.6622
	Epoch 1730:	Loss 1.4590	TrainAcc 0.6646	ValidAcc 0.6611	BestValid 0.6622
	Epoch 1740:	Loss 1.4772	TrainAcc 0.6648	ValidAcc 0.6616	BestValid 0.6622
	Epoch 1750:	Loss 1.4665	TrainAcc 0.6647	ValidAcc 0.6615	BestValid 0.6622
	Epoch 1760:	Loss 1.4671	TrainAcc 0.6644	ValidAcc 0.6595	BestValid 0.6622
	Epoch 1770:	Loss 1.4624	TrainAcc 0.6652	ValidAcc 0.6624	BestValid 0.6624
	Epoch 1780:	Loss 1.4692	TrainAcc 0.6650	ValidAcc 0.6613	BestValid 0.6624
	Epoch 1790:	Loss 1.4683	TrainAcc 0.6657	ValidAcc 0.6620	BestValid 0.6624
	Epoch 1800:	Loss 1.4624	TrainAcc 0.6653	ValidAcc 0.6622	BestValid 0.6624
	Epoch 1810:	Loss 1.4571	TrainAcc 0.6654	ValidAcc 0.6626	BestValid 0.6626
	Epoch 1820:	Loss 1.4667	TrainAcc 0.6657	ValidAcc 0.6624	BestValid 0.6626
	Epoch 1830:	Loss 1.4581	TrainAcc 0.6653	ValidAcc 0.6616	BestValid 0.6626
	Epoch 1840:	Loss 1.4659	TrainAcc 0.6651	ValidAcc 0.6614	BestValid 0.6626
	Epoch 1850:	Loss 1.4668	TrainAcc 0.6657	ValidAcc 0.6626	BestValid 0.6626
	Epoch 1860:	Loss 1.4618	TrainAcc 0.6652	ValidAcc 0.6608	BestValid 0.6626
	Epoch 1870:	Loss 1.4606	TrainAcc 0.6661	ValidAcc 0.6629	BestValid 0.6629
	Epoch 1880:	Loss 1.4588	TrainAcc 0.6657	ValidAcc 0.6616	BestValid 0.6629
	Epoch 1890:	Loss 1.4658	TrainAcc 0.6654	ValidAcc 0.6617	BestValid 0.6629
	Epoch 1900:	Loss 1.4569	TrainAcc 0.6661	ValidAcc 0.6619	BestValid 0.6629
	Epoch 1910:	Loss 1.4649	TrainAcc 0.6661	ValidAcc 0.6626	BestValid 0.6629
	Epoch 1920:	Loss 1.4637	TrainAcc 0.6663	ValidAcc 0.6628	BestValid 0.6629
	Epoch 1930:	Loss 1.4593	TrainAcc 0.6669	ValidAcc 0.6632	BestValid 0.6632
	Epoch 1940:	Loss 1.4650	TrainAcc 0.6664	ValidAcc 0.6632	BestValid 0.6632
	Epoch 1950:	Loss 1.4640	TrainAcc 0.6667	ValidAcc 0.6637	BestValid 0.6637
	Epoch 1960:	Loss 1.4527	TrainAcc 0.6668	ValidAcc 0.6636	BestValid 0.6637
	Epoch 1970:	Loss 1.4632	TrainAcc 0.6670	ValidAcc 0.6639	BestValid 0.6639
	Epoch 1980:	Loss 1.4585	TrainAcc 0.6668	ValidAcc 0.6631	BestValid 0.6639
	Epoch 1990:	Loss 1.4580	TrainAcc 0.6668	ValidAcc 0.6631	BestValid 0.6639
	Epoch 2000:	Loss 1.4643	TrainAcc 0.6668	ValidAcc 0.6631	BestValid 0.6639
	Epoch 2010:	Loss 1.4594	TrainAcc 0.6668	ValidAcc 0.6637	BestValid 0.6639
	Epoch 2020:	Loss 1.4625	TrainAcc 0.6670	ValidAcc 0.6635	BestValid 0.6639
	Epoch 2030:	Loss 1.4619	TrainAcc 0.6671	ValidAcc 0.6633	BestValid 0.6639
	Epoch 2040:	Loss 1.4598	TrainAcc 0.6668	ValidAcc 0.6627	BestValid 0.6639
	Epoch 2050:	Loss 1.4586	TrainAcc 0.6669	ValidAcc 0.6622	BestValid 0.6639
	Epoch 2060:	Loss 1.4524	TrainAcc 0.6671	ValidAcc 0.6630	BestValid 0.6639
	Epoch 2070:	Loss 1.4519	TrainAcc 0.6672	ValidAcc 0.6636	BestValid 0.6639
	Epoch 2080:	Loss 1.4587	TrainAcc 0.6670	ValidAcc 0.6624	BestValid 0.6639
	Epoch 2090:	Loss 1.4636	TrainAcc 0.6678	ValidAcc 0.6637	BestValid 0.6639
	Epoch 2100:	Loss 1.4552	TrainAcc 0.6673	ValidAcc 0.6627	BestValid 0.6639
	Epoch 2110:	Loss 1.4548	TrainAcc 0.6679	ValidAcc 0.6633	BestValid 0.6639
	Epoch 2120:	Loss 1.4537	TrainAcc 0.6673	ValidAcc 0.6633	BestValid 0.6639
	Epoch 2130:	Loss 1.4479	TrainAcc 0.6677	ValidAcc 0.6633	BestValid 0.6639
	Epoch 2140:	Loss 1.4608	TrainAcc 0.6679	ValidAcc 0.6640	BestValid 0.6640
	Epoch 2150:	Loss 1.4528	TrainAcc 0.6684	ValidAcc 0.6644	BestValid 0.6644
	Epoch 2160:	Loss 1.4535	TrainAcc 0.6679	ValidAcc 0.6642	BestValid 0.6644
	Epoch 2170:	Loss 1.4506	TrainAcc 0.6684	ValidAcc 0.6641	BestValid 0.6644
	Epoch 2180:	Loss 1.4471	TrainAcc 0.6680	ValidAcc 0.6636	BestValid 0.6644
	Epoch 2190:	Loss 1.4535	TrainAcc 0.6685	ValidAcc 0.6645	BestValid 0.6645
	Epoch 2200:	Loss 1.4510	TrainAcc 0.6680	ValidAcc 0.6627	BestValid 0.6645
	Epoch 2210:	Loss 1.4591	TrainAcc 0.6684	ValidAcc 0.6643	BestValid 0.6645
	Epoch 2220:	Loss 1.4540	TrainAcc 0.6681	ValidAcc 0.6634	BestValid 0.6645
	Epoch 2230:	Loss 1.4532	TrainAcc 0.6688	ValidAcc 0.6636	BestValid 0.6645
	Epoch 2240:	Loss 1.4506	TrainAcc 0.6687	ValidAcc 0.6642	BestValid 0.6645
	Epoch 2250:	Loss 1.4511	TrainAcc 0.6686	ValidAcc 0.6635	BestValid 0.6645
	Epoch 2260:	Loss 1.4546	TrainAcc 0.6681	ValidAcc 0.6631	BestValid 0.6645
	Epoch 2270:	Loss 1.4500	TrainAcc 0.6691	ValidAcc 0.6638	BestValid 0.6645
	Epoch 2280:	Loss 1.4461	TrainAcc 0.6687	ValidAcc 0.6638	BestValid 0.6645
	Epoch 2290:	Loss 1.4512	TrainAcc 0.6690	ValidAcc 0.6652	BestValid 0.6652
	Epoch 2300:	Loss 1.4463	TrainAcc 0.6688	ValidAcc 0.6636	BestValid 0.6652
	Epoch 2310:	Loss 1.4472	TrainAcc 0.6696	ValidAcc 0.6655	BestValid 0.6655
	Epoch 2320:	Loss 1.4478	TrainAcc 0.6687	ValidAcc 0.6643	BestValid 0.6655
	Epoch 2330:	Loss 1.4512	TrainAcc 0.6695	ValidAcc 0.6650	BestValid 0.6655
	Epoch 2340:	Loss 1.4477	TrainAcc 0.6696	ValidAcc 0.6644	BestValid 0.6655
	Epoch 2350:	Loss 1.4521	TrainAcc 0.6693	ValidAcc 0.6646	BestValid 0.6655
	Epoch 2360:	Loss 1.4502	TrainAcc 0.6692	ValidAcc 0.6637	BestValid 0.6655
	Epoch 2370:	Loss 1.4526	TrainAcc 0.6694	ValidAcc 0.6644	BestValid 0.6655
	Epoch 2380:	Loss 1.4527	TrainAcc 0.6694	ValidAcc 0.6643	BestValid 0.6655
	Epoch 2390:	Loss 1.4448	TrainAcc 0.6695	ValidAcc 0.6643	BestValid 0.6655
	Epoch 2400:	Loss 1.4402	TrainAcc 0.6696	ValidAcc 0.6652	BestValid 0.6655
	Epoch 2410:	Loss 1.4541	TrainAcc 0.6697	ValidAcc 0.6649	BestValid 0.6655
	Epoch 2420:	Loss 1.4468	TrainAcc 0.6699	ValidAcc 0.6652	BestValid 0.6655
	Epoch 2430:	Loss 1.4464	TrainAcc 0.6696	ValidAcc 0.6638	BestValid 0.6655
	Epoch 2440:	Loss 1.4474	TrainAcc 0.6699	ValidAcc 0.6652	BestValid 0.6655
	Epoch 2450:	Loss 1.4498	TrainAcc 0.6703	ValidAcc 0.6655	BestValid 0.6655
	Epoch 2460:	Loss 1.4506	TrainAcc 0.6701	ValidAcc 0.6645	BestValid 0.6655
	Epoch 2470:	Loss 1.4443	TrainAcc 0.6698	ValidAcc 0.6639	BestValid 0.6655
	Epoch 2480:	Loss 1.4488	TrainAcc 0.6696	ValidAcc 0.6639	BestValid 0.6655
	Epoch 2490:	Loss 1.4455	TrainAcc 0.6702	ValidAcc 0.6658	BestValid 0.6658
	Epoch 2500:	Loss 1.4430	TrainAcc 0.6701	ValidAcc 0.6643	BestValid 0.6658
	Epoch 2510:	Loss 1.4479	TrainAcc 0.6699	ValidAcc 0.6647	BestValid 0.6658
	Epoch 2520:	Loss 1.4424	TrainAcc 0.6701	ValidAcc 0.6653	BestValid 0.6658
	Epoch 2530:	Loss 1.4474	TrainAcc 0.6697	ValidAcc 0.6643	BestValid 0.6658
	Epoch 2540:	Loss 1.4444	TrainAcc 0.6699	ValidAcc 0.6649	BestValid 0.6658
	Epoch 2550:	Loss 1.4449	TrainAcc 0.6704	ValidAcc 0.6649	BestValid 0.6658
	Epoch 2560:	Loss 1.4410	TrainAcc 0.6700	ValidAcc 0.6642	BestValid 0.6658
	Epoch 2570:	Loss 1.4473	TrainAcc 0.6708	ValidAcc 0.6661	BestValid 0.6661
	Epoch 2580:	Loss 1.4469	TrainAcc 0.6705	ValidAcc 0.6649	BestValid 0.6661
	Epoch 2590:	Loss 1.4454	TrainAcc 0.6706	ValidAcc 0.6662	BestValid 0.6662
	Epoch 2600:	Loss 1.4417	TrainAcc 0.6709	ValidAcc 0.6653	BestValid 0.6662
	Epoch 2610:	Loss 1.4378	TrainAcc 0.6708	ValidAcc 0.6659	BestValid 0.6662
	Epoch 2620:	Loss 1.4464	TrainAcc 0.6706	ValidAcc 0.6653	BestValid 0.6662
	Epoch 2630:	Loss 1.4441	TrainAcc 0.6708	ValidAcc 0.6653	BestValid 0.6662
	Epoch 2640:	Loss 1.4405	TrainAcc 0.6706	ValidAcc 0.6659	BestValid 0.6662
	Epoch 2650:	Loss 1.4419	TrainAcc 0.6705	ValidAcc 0.6650	BestValid 0.6662
	Epoch 2660:	Loss 1.4415	TrainAcc 0.6705	ValidAcc 0.6641	BestValid 0.6662
	Epoch 2670:	Loss 1.4410	TrainAcc 0.6711	ValidAcc 0.6663	BestValid 0.6663
	Epoch 2680:	Loss 1.4351	TrainAcc 0.6708	ValidAcc 0.6651	BestValid 0.6663
	Epoch 2690:	Loss 1.4444	TrainAcc 0.6711	ValidAcc 0.6663	BestValid 0.6663
	Epoch 2700:	Loss 1.4489	TrainAcc 0.6708	ValidAcc 0.6653	BestValid 0.6663
	Epoch 2710:	Loss 1.4328	TrainAcc 0.6710	ValidAcc 0.6664	BestValid 0.6664
	Epoch 2720:	Loss 1.4416	TrainAcc 0.6706	ValidAcc 0.6646	BestValid 0.6664
	Epoch 2730:	Loss 1.4406	TrainAcc 0.6716	ValidAcc 0.6666	BestValid 0.6666
	Epoch 2740:	Loss 1.4408	TrainAcc 0.6713	ValidAcc 0.6668	BestValid 0.6668
	Epoch 2750:	Loss 1.4406	TrainAcc 0.6709	ValidAcc 0.6645	BestValid 0.6668
	Epoch 2760:	Loss 1.4381	TrainAcc 0.6713	ValidAcc 0.6657	BestValid 0.6668
	Epoch 2770:	Loss 1.4316	TrainAcc 0.6715	ValidAcc 0.6658	BestValid 0.6668
	Epoch 2780:	Loss 1.4398	TrainAcc 0.6713	ValidAcc 0.6653	BestValid 0.6668
	Epoch 2790:	Loss 1.4464	TrainAcc 0.6720	ValidAcc 0.6671	BestValid 0.6671
	Epoch 2800:	Loss 1.4405	TrainAcc 0.6715	ValidAcc 0.6652	BestValid 0.6671
	Epoch 2810:	Loss 1.4438	TrainAcc 0.6716	ValidAcc 0.6655	BestValid 0.6671
	Epoch 2820:	Loss 1.4385	TrainAcc 0.6720	ValidAcc 0.6661	BestValid 0.6671
	Epoch 2830:	Loss 1.4344	TrainAcc 0.6714	ValidAcc 0.6656	BestValid 0.6671
	Epoch 2840:	Loss 1.4358	TrainAcc 0.6711	ValidAcc 0.6655	BestValid 0.6671
	Epoch 2850:	Loss 1.4367	TrainAcc 0.6716	ValidAcc 0.6654	BestValid 0.6671
	Epoch 2860:	Loss 1.4372	TrainAcc 0.6713	ValidAcc 0.6643	BestValid 0.6671
	Epoch 2870:	Loss 1.4370	TrainAcc 0.6714	ValidAcc 0.6666	BestValid 0.6671
	Epoch 2880:	Loss 1.4326	TrainAcc 0.6710	ValidAcc 0.6644	BestValid 0.6671
	Epoch 2890:	Loss 1.4357	TrainAcc 0.6718	ValidAcc 0.6660	BestValid 0.6671
	Epoch 2900:	Loss 1.4400	TrainAcc 0.6721	ValidAcc 0.6657	BestValid 0.6671
	Epoch 2910:	Loss 1.4342	TrainAcc 0.6720	ValidAcc 0.6659	BestValid 0.6671
	Epoch 2920:	Loss 1.4367	TrainAcc 0.6718	ValidAcc 0.6655	BestValid 0.6671
	Epoch 2930:	Loss 1.4339	TrainAcc 0.6722	ValidAcc 0.6661	BestValid 0.6671
	Epoch 2940:	Loss 1.4338	TrainAcc 0.6717	ValidAcc 0.6650	BestValid 0.6671
	Epoch 2950:	Loss 1.4410	TrainAcc 0.6723	ValidAcc 0.6670	BestValid 0.6671
	Epoch 2960:	Loss 1.4470	TrainAcc 0.6718	ValidAcc 0.6653	BestValid 0.6671
	Epoch 2970:	Loss 1.4354	TrainAcc 0.6720	ValidAcc 0.6659	BestValid 0.6671
	Epoch 2980:	Loss 1.4322	TrainAcc 0.6725	ValidAcc 0.6666	BestValid 0.6671
	Epoch 2990:	Loss 1.4381	TrainAcc 0.6718	ValidAcc 0.6658	BestValid 0.6671
Node 0, Layer-level comm throughput (grad): 10.475 GBps
Node 0, Layer-level comm throughput (act): -nan GBps
Node 2, Layer-level comm throughput (grad): 11.012 GBps
Node 1, Layer-level comm throughput (grad): 10.990 GBps
Node 3, Layer-level comm throughput (grad): -nan GBps
Node 1, Layer-level comm throughput (act): 10.637 GBps
Node 2, Layer-level comm throughput (act): 9.976 GBps
Node 3, Layer-level comm throughput (act): 11.157 GBps
	Epoch 3000:	Loss 1.4295	TrainAcc 0.6717	ValidAcc 0.6650	BestValid 0.6671
Node 0, GPU memory consumption: 3.683 GB
Node 0, compression time: 4.830s, compression size: 629.842GB, throughput: 130.390GBps
Node 0, decompression time: 32.990s, compression size: 629.842GB, throughput: 14.686GBps
Node 0, pure compute time: 206.398 s, total compute time: 244.219 s
Node 0, wait_for_task_time: 193.266 s, wait_for_other_gpus_time: 0.047 s
------------------------node id 0,  per-epoch time: 0.173090 s---------------
Node 3, GPU memory consumption: 2.371 GB
Node 3, compression time: 5.079s, compression size: 484.494GB, throughput: 95.387GBps
Node 3, decompression time: 43.605s, compression size: 484.494GB, throughput: 14.444GBps
Node 3, pure compute time: 92.899 s, total compute time: 141.583 s
Node 3, wait_for_task_time: 126.674 s, wait_for_other_gpus_time: 0.041 s
------------------------node id 3,  per-epoch time: 0.173090 s---------------
Node 2, GPU memory consumption: 3.089 GB
Node 2, compression time: 10.401s, compression size: 1114.337GB, throughput: 107.138GBps
Node 2, decompression time: 67.227s, compression size: 1114.337GB, throughput: 16.576GBps
Node 2, pure compute time: 150.772 s, total compute time: 228.400 s
Node 2, wait_for_task_time: 114.252 s, wait_for_other_gpus_time: 0.032 s
------------------------node id 2,  per-epoch time: 0.173090 s---------------
Node 1, GPU memory consumption: 3.886 GB
Node 1, compression time: 10.369s, compression size: 1114.337GB, throughput: 107.465GBps
Node 1, decompression time: 129.447s, compression size: 1114.337GB, throughput: 8.608GBps
Node 1, pure compute time: 217.979 s, total compute time: 357.795 s
Node 1, wait_for_task_time: 58.416 s, wait_for_other_gpus_time: 0.049 s
------------------------node id 1,  per-epoch time: 0.173090 s---------------
************ Profiling Results ************
	Bubble: 75.624597 (s) (14.44 percentage)
	Compute: 283.676195 (s) (54.18 percentage)
	GradSync: 6.819020 (s) (1.30 percentage)
	GraphComm: 0.134179 (s) (0.03 percentage)
	Imbalance: 93.113533 (s) (17.78 percentage)
	LayerComm: 64.195982 (s) (12.26 percentage)
	Layer-level communication (cluster-wide, per epoch): 1.721 GB
Highest valid_acc: 0.6671
Target test_acc: 0.6494
Epoch to reach the target acc: 2790
Node 1, sent 0.000 MB data
Node 2, sent 0.000 MB data
Node 3, sent 0.000 MB data
Node 0, sent 0.000 MB data
[MPI Rank 0] Success 
[MPI Rank 3] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
