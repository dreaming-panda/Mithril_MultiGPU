g011.anvil.rcac.purdue.edu
Tue Jan 31 17:52:55 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:01:00.0 Off |                    0 |
| N/A   29C    P0    51W / 400W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-SXM...  On   | 00000000:41:00.0 Off |                    0 |
| N/A   28C    P0    49W / 400W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA A100-SXM...  On   | 00000000:81:00.0 Off |                    0 |
| N/A   29C    P0    49W / 400W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA A100-SXM...  On   | 00000000:C1:00.0 Off |                    0 |
| N/A   29C    P0    50W / 400W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2    9) numactl/2.0.14
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0    10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0   11) openmpi/4.0.6
  4) gmp/6.2.1              8) zlib/1.2.11  12) boost/1.74.0

 

[  4%] Built target context
[ 18%] Built target parallel
[ 19%] Built target core
[ 39%] Built target cudahelp
[ 43%] Built target test_cuda_data_compression
[ 46%] Built target test_cuda_pipeline_parallel
[ 75%] Built target test_nccl_thread
[ 83%] Built target test_mpi_pipelined_model_parallel
[ 75%] Built target test_graph
[ 75%] Built target test_mpi_structual_graph
[ 75%] Built target test_trivial
[ 75%] Built target test_mpi_gpu_hybrid
[ 75%] Built target test_cuda_graph
[ 75%] Built target test_cuda
[ 89%] Built target test_full_non_structual_graph
[ 89%] Built target test_cuda_model_parallel
[ 89%] Built target test_hello_world
[ 89%] Built target test_mpi_gpu_model_parallel
[ 89%] Built target test_single_node_fullgpu_training
[ 89%] Built target test_mpi_gpu_pipelined_model_parallel
[ 89%] Built target test_mpi_non_structual_graph
[ 89%] Built target test_single_node_training
[ 89%] Built target test_full_structual_graph
[ 89%] Built target test_mpi_combined
[ 89%] Built target test_mpi_loader
[ 91%] Built target estimate_comm_volume
[ 91%] Built target test_mpi_model_parallel
[ 91%] Built target test_nccl_mpi
[ 91%] Built target test_single_node_gpu_training
[ 96%] Built target test_two_layer_hybrid_parallelism_designer
[100%] Built target OSDI2023_SINGLE_NODE_gcn_inference
[100%] Built target OSDI2023_SINGLE_NODE_gcn
[100%] Built target OSDI2023_MULTI_NODES_gcn
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/reddit
The number of GCN layers: 8
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: saved_weights_pipe
The random seed: 1234
The scaling down factor of out-of-chunk gradients: 0.100000
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/reddit
The number of GCN layers: 8
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: saved_weights_pipe
The random seed: 1234
The scaling down factor of out-of-chunk gradients: 0.100000
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/reddit
The number of GCN layers: 8
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: saved_weights_pipe
The random seed: 1234
The scaling down factor of out-of-chunk gradients: 0.100000
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/reddit
The number of GCN layers: 8
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: saved_weights_pipe
The random seed: 1234
The scaling down factor of out-of-chunk gradients: 0.100000
Initialized node 1 on machine g011.anvil.rcac.purdue.edu
Initialized node 2 on machine g011.anvil.rcac.purdue.edu
Initialized node 0 on machine g011.anvil.rcac.purdue.edu
Initialized node 3 on machine g011.anvil.rcac.purdue.edu
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.975 seconds.
Building the CSC structure...
        It takes 1.983 seconds.
Building the CSC structure...
        It takes 2.017 seconds.
Building the CSC structure...
        It takes 1.993 seconds.
Building the CSC structure...
        It takes 1.821 seconds.
        It takes 1.805 seconds.
        It takes 1.883 seconds.
        It takes 1.889 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.462 seconds.
Building the Label Vector...
        It takes 0.516 seconds.
Building the Label Vector...
        It takes 0.452 seconds.
Building the Label Vector...
        It takes 0.449 seconds.
Building the Label Vector...
        It takes 0.091 seconds.
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
        It takes 0.109 seconds.
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
        It takes 0.072 seconds.
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
        It takes 0.073 seconds.
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
train nodes 153431, valid nodes 23831, test nodes 55703
train nodes 153431, valid nodes 23831, test nodes 55703
train nodes 153431, valid nodes 23831, test nodes 55703
train nodes 153431, valid nodes 23831, test nodes 55703
Number of GPUs: 4
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
GPU 2, layer [4, 6)
GPU 3, layer [6, 8)
*** Node 2, starting model training...
Number of operators: 40
0 232965 0 11
0 232965 11 21
0 232965 21 31
0 232965 31 40
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the partition [21, 31) x [0, 232965)
*** Node 2, constructing the helper classes...
Number of GPUs: 4
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
GPU 2, layer [4, 6)
GPU 3, layer [6, 8)
*** Node 0, starting model training...
Number of GPUs: 4
Number of operators: 40
0 232965 0 11
0 232965 11 21
0 232965 21 31
0 232965 31 40
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the partition [0, 11) x [0, 232965)
*** Node 0, constructing the helper classes...
Operators:
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
GPU 2, layer [4, 6)
GPU 3, layer [6, 8)
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_WEIGHT, output tensors: 1
    Op 2: type OPERATOR_MATMUL, output tensors: 2
    Op 3: type OPERATOR_AGGREGATION, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
WARNING: the current version only applies to linear GNN models!
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_WEIGHT, output tensors: 6
    Op 7: type OPERATOR_MATMUL, output tensors: 7
    Op 8: type OPERATOR_AGGREGATION, output tensors: 8
    Op 9: type OPERATOR_RELU, output tensors: 9
    Op 10: type OPERATOR_DROPOUT, output tensors: 10
    Op 11: type OPERATOR_WEIGHT, output tensors: 11
*** Node 1, starting model training...
Number of operators: 40
0 232965 0 11
0 232965 11 21
0 232965 21 31
0 232965 31 40
    Op 12: type OPERATOR_MATMUL, output tensors: 12
    Op 13: type OPERATOR_AGGREGATION, output tensors: 13
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the partition [11, 21) x [0, 232965)
*** Node 1, constructing the helper classes...
    Op 14: type OPERATOR_RELU, output tensors: 14
    Op 15: type OPERATOR_DROPOUT, output tensors: 15
    Op 16: type OPERATOR_WEIGHT, output tensors: 16
    Op 17: type OPERATOR_MATMUL, output tensors: 17
    Op 18: type OPERATOR_AGGREGATION, output tensors: 18
    Op 19: type OPERATOR_RELU, output tensors: 19
    Op 20: type OPERATOR_DROPOUT, output tensors: 20
Number of GPUs: 4
    Op 21: type OPERATOR_WEIGHT, output tensors: 21
    Op 22: type OPERATOR_MATMUL, output tensors: 22
    Op 23: type OPERATOR_AGGREGATION, output tensors: 23
    Op 24: type OPERATOR_RELU, output tensors: 24
    Op 25: type OPERATOR_DROPOUT, output tensors: 25
    Op 26: type OPERATOR_WEIGHT, output tensors: 26
    Op 27: type OPERATOR_MATMUL, output tensors: 27
    Op 28: type OPERATOR_AGGREGATION, output tensors: 28
    Op 29: type OPERATOR_RELU, output tensors: 29
    Op 30: type OPERATOR_DROPOUT, output tensors: 30
    Op 31: type OPERATOR_WEIGHT, output tensors: 31
    Op 32: type OPERATOR_MATMUL, output tensors: 32
    Op 33: type OPERATOR_AGGREGATION, output tensors: 33
    Op 34: type OPERATOR_RELU, output tensors: 34
    Op 35: type OPERATOR_DROPOUT, output tensors: 35
    Op 36: type OPERATOR_WEIGHT, output tensors: 36
    Op 37: type OPERATOR_MATMUL, output tensors: 37
    Op 38: type OPERATOR_AGGREGATION, output tensors: 38
    Op 39: type OPERATOR_SOFTMAX, output tensors: 39
WARNING: the current version only applies to linear GNN models!
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
GPU 2, layer [4, 6)
GPU 3, layer [6, 8)
*** Node 3, starting model training...
Number of operators: 40
0 232965 0 11
0 232965 11 21
0 232965 21 31
0 232965 31 40
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the partition [31, 40) x [0, 232965)
*** Node 3, constructing the helper classes...
(Forwarding) Node 3 (fragment 0) depends on nodes: 2 (Tensor: 30)
(Backwarding) Node 3 (fragment 0) depends on nodes:
(I-link dependencies): node 3 should send activation to nodes:
(I-link dependencies): node 3 should receive activation from nodes:
(I-link dependencies): node 3 should send gradient to nodes:
(I-link dependencies): node 3 should receive gradient from nodes:
(Forwarding) Node 2 (fragment 0) depends on nodes: 1 (Tensor: 20)
(Backwarding) Node 2 (fragment 0) depends on nodes: 3 (Tensor: 30)
(I-link dependencies): node 2 should send activation to nodes:
(I-link dependencies): node 2 should receive activation from nodes:
(I-link dependencies): node 2 should send gradient to nodes:
(I-link dependencies): node 2 should receive gradient from nodes:
(Forwarding) Node 1 (fragment 0) depends on nodes: 0 (Tensor: 10)
(Backwarding) Node 1 (fragment 0) depends on nodes: 2 (Tensor: 20)
(I-link dependencies): node 1 should send activation to nodes:
(I-link dependencies): node 1 should receive activation from nodes:
(I-link dependencies): node 1 should send gradient to nodes:
(I-link dependencies): node 1 should receive gradient from nodes:
Boundaries: 0 0 0 0 232965 232965 232965 232965
Fragments: [0, 232965)
Chunks (number of global chunks: 16): 0-[0, 14561) 1-[14561, 29122) 2-[29122, 43683) 3-[43683, 58244) 4-[58244, 72805) 5-[72805, 87366) 6-[87366, 101927) 7-[101927, 116488) 8-[116488, 131049) ... 15-[218415, 232965)
(Forwarding) Node 0 (fragment 0) depends on nodes:
(Backwarding) Node 0 (fragment 0) depends on nodes: 1 (Tensor: 10)
(I-link dependencies): node 0 should send activation to nodes:
(I-link dependencies): node 0 should receive activation from nodes:
(I-link dependencies): node 0 should send gradient to nodes:
(I-link dependencies): node 0 should receive gradient from nodes:
232965, 114848857, 114848857
Number of vertices per chunk: 14561
232965, 114848857, 114848857
232965, 114848857, 114848857
Number of vertices per chunk: 14561
Number of vertices per chunk: 14561
232965, 114848857, 114848857
Number of vertices per chunk: 14561
csr in-out ready !*** Node 0, setting up some other necessary information...
csr in-out ready !*** Node 2, setting up some other necessary information...
csr in-out ready !*** Node 3, setting up some other necessary information...
csr in-out ready !*** Node 1, setting up some other necessary information...
*** Node 2, starting the helper threads...
*** Node 1, starting the helper threads...
*** Node 3, starting the helper threads...
*** Node 0, starting the helper threads...
+++++++++ Node 0 initializing the weights for op[0, 11)...
+++++++++ Node 0, mapping weight op 1
+++++++++ Node 2 initializing the weights for op[21, 31)...
+++++++++ Node 2, mapping weight op 21
+++++++++ Node 1 initializing the weights for op[11, 21)...
+++++++++ Node 1, mapping weight op 11
+++++++++ Node 3 initializing the weights for op[31, 40)...
+++++++++ Node 3, mapping weight op 31
+++++++++ Node 2, mapping weight op 26
+++++++++ Node 1, mapping weight op 16
+++++++++ Node 3, mapping weight op 36
+++++++++ Node 0, mapping weight op 6
RANDOMLY DISPATCH THE CHUNKS...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
    Epoch 9:	Loss 3.18218	TrainAcc 0.1988	ValidAcc 0.2101	TestAcc 0.2036
    Epoch 19:	Loss 2.68988	TrainAcc 0.3883	ValidAcc 0.3875	TestAcc 0.3801
    Epoch 29:	Loss 1.93553	TrainAcc 0.4353	ValidAcc 0.4362	TestAcc 0.4305
    Epoch 39:	Loss 1.93517	TrainAcc 0.4286	ValidAcc 0.4253	TestAcc 0.4179
    Epoch 49:	Loss 1.74004	TrainAcc 0.4703	ValidAcc 0.4648	TestAcc 0.4607
    Epoch 59:	Loss 1.36228	TrainAcc 0.6578	ValidAcc 0.6844	TestAcc 0.6799
    Epoch 69:	Loss 1.24124	TrainAcc 0.6447	ValidAcc 0.6572	TestAcc 0.6511
    Epoch 79:	Loss 1.03549	TrainAcc 0.7583	ValidAcc 0.7652	TestAcc 0.7666
    Epoch 89:	Loss 0.96954	TrainAcc 0.7615	ValidAcc 0.7834	TestAcc 0.7788
Node 0, Layer-level comm throughput (act): -nan GBps
Node 1, Layer-level comm throughput (act): 12.288 GBps
Node 2, Layer-level comm throughput (act): 13.546 GBps
Node 3, Layer-level comm throughput (act): 16.598 GBps
Node 3, Layer-level comm throughput (grad): -nan GBps
Node 2, Layer-level comm throughput (grad): 17.065 GBps
Node 1, Layer-level comm throughput (grad): 15.026 GBps
Node 0, Layer-level comm throughput (grad): 11.950 GBps
Node 1, compression time: 0.250s, compression size: 22.217GB, throughput: 88.738GBps
Node 1, decompression time: 0.481s, compression size: 22.217GB, throughput: 46.161GBps
Node 1, pure compute time: 6.494 s, total compute time: 7.226 s
Node 1, wait_for_task_time: 2.548 s, wait_for_other_gpus_time: 0.001 s
------------------------node id 1,  per-epoch time: 0.100090 s---------------
Node 2, compression time: 0.259s, compression size: 22.217GB, throughput: 85.849GBps
Node 2, decompression time: 0.471s, compression size: 22.217GB, throughput: 47.164GBps
Node 2, pure compute time: 6.507 s, total compute time: 7.237 s
Node 2, wait_for_task_time: 2.009 s, wait_for_other_gpus_time: 0.000 s
------------------------node id 2,  per-epoch time: 0.100090 s---------------
    Epoch 99:	Loss 1.07884	TrainAcc 0.7017	ValidAcc 0.7245	TestAcc 0.7212
Node 0, compression time: 0.120s, compression size: 11.109GB, throughput: 92.554GBps
Node 0, decompression time: 0.192s, compression size: 11.109GB, throughput: 57.927GBps
Node 0, pure compute time: 7.141 s, total compute time: 7.453 s
Node 0, wait_for_task_time: 2.847 s, wait_for_other_gpus_time: 0.001 s
------------------------node id 0,  per-epoch time: 0.100090 s---------------
Node 3, compression time: 0.162s, compression size: 11.109GB, throughput: 68.444GBps
Node 3, decompression time: 0.177s, compression size: 11.109GB, throughput: 62.894GBps
Node 3, pure compute time: 5.874 s, total compute time: 6.213 s
Node 3, wait_for_task_time: 2.098 s, wait_for_other_gpus_time: 0.001 s
------------------------node id 3,  per-epoch time: 0.100090 s---------------
************ Profiling Results ************
	Bubble: 2.277440 (s) (21.31 percentage)
	Compute: 7.316030 (s) (68.47 percentage)
	GradSync: 0.094649 (s) (0.89 percentage)
	GraphComm: 0.003120 (s) (0.03 percentage)
	Imbalance: 0.875175 (s) (8.19 percentage)
	LayerComm: 0.119189 (s) (1.12 percentage)
	Layer-level communication (cluster-wide, per epoch): 0.208 GB
Highest valid_acc: 0.7834
Target test_acc: 0.7788
Epoch to reach the target acc: 90
[MPI Rank 2] Success 
[MPI Rank 1] Success 
[MPI Rank 3] Success 
[MPI Rank 0] Success 
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/reddit
The number of GCN layers: 8
The number of hidden units: 128
The number of training epoches: 0
Learning rate: 0.000000
Initialized node g011.anvil.rcac.purdue.edu
Building the CSR structure...
        It takes 1.766 seconds.
Building the CSC structure...
        It takes 1.728 seconds.
Building the Feature Vector...
        It takes 0.304 seconds.
Building the Label Vector...
        It takes 0.027 seconds.
Number of classes: 41
Number of feature dimensions: 602
Dropout: 0.000 
train nodes 153431, valid nodes 23831, test nodes 55703
*** Allocating resources for all tensors...
    OP_TYPE: OPERATOR_INPUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_SOFTMAX
*** Done allocating resource.
*** Preparing the input tensor...
*** Done preparing the input tensor.
*** Preparing the STD tensor...
    Number of labels: 41
    Number of vertices: 232965
*** Done preparing the STD tensor.
Version 0	TrainAcc 0.2214	ValidAcc 0.2303	TestAcc 0.2231
Version 1	TrainAcc 0.3830	ValidAcc 0.3798	TestAcc 0.3723
Version 2	TrainAcc 0.4023	ValidAcc 0.4087	TestAcc 0.3991
Version 3	TrainAcc 0.4533	ValidAcc 0.4498	TestAcc 0.4403
Version 4	TrainAcc 0.6073	ValidAcc 0.6313	TestAcc 0.6264
Version 5	TrainAcc 0.6279	ValidAcc 0.6599	TestAcc 0.6511
Version 6	TrainAcc 0.7168	ValidAcc 0.7314	TestAcc 0.7283
Version 7	TrainAcc 0.6636	ValidAcc 0.6390	TestAcc 0.6360
Version 8	TrainAcc 0.6958	ValidAcc 0.7173	TestAcc 0.7133
Version 9	TrainAcc 0.7121	ValidAcc 0.7281	TestAcc 0.7256
Version 6 achieved the highest validation accuracy 0.7314 (test accuracy: 0.7283)
