g012.anvil.rcac.purdue.edu
Tue Apr 18 22:14:17 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:81:00.0 Off |                    0 |
| N/A   32C    P0    52W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2       9) zlib/1.2.11
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0       10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0      11) openmpi/4.0.6
  4) gmp/6.2.1              8) numactl/2.0.14  12) boost/1.74.0

 

[  4%] Built target context
[ 16%] Built target parallel
[ 19%] Built target core
[ 39%] Built target cudahelp
[ 64%] Built target test_full_non_structual_graph
[ 53%] Built target test_mpi_gpu_model_parallel
[ 53%] Built target test_cuda_model_parallel
[ 53%] Built target test_mpi_gpu_hybrid
[ 65%] Built target test_trivial
[ 65%] Built target test_cuda_graph
[ 65%] Built target test_mpi_gpu_pipelined_model_parallel
[ 72%] Built target test_graph
[ 65%] Built target test_cuda_pipeline_parallel
[ 65%] Built target test_mpi_combined
[ 72%] Built target test_single_node_fullgpu_training
[ 72%] Built target test_cuda
[ 72%] Built target test_mpi_structual_graph
[ 82%] Built target test_single_node_training
[ 88%] Built target test_single_node_gpu_training
[ 82%] Built target test_nccl_thread
[ 82%] Built target test_mpi_loader
[ 89%] Built target test_mpi_non_structual_graph
[ 89%] Built target estimate_comm_volume
[ 89%] Built target test_nccl_mpi
[ 89%] Built target test_mpi_pipelined_model_parallel
[ 89%] Built target test_hello_world
[ 89%] Built target test_full_structual_graph
[ 89%] Built target test_mpi_model_parallel
[ 93%] Built target OSDI2023_MULTI_NODES_gcnii
[ 98%] Built target test_two_layer_hybrid_parallelism_designer
[100%] Built target OSDI2023_MULTI_NODES_gcn
[100%] Built target OSDI2023_SINGLE_NODE_gcn_inference
[100%] Built target OSDI2023_SINGLE_NODE_gcn
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/citeseer
The number of GCNII layers: 32
The number of hidden units: 256
The number of training epoches: 1500
The number of startup epoches: 0
Learning rate: 0.010000
The partition strategy: model
The dropout rate: 0.600
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 3
The scaling down factor of out-of-chunk gradients: 0.100000
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.600000
Initialized node 0 on machine g012.anvil.rcac.purdue.edu
Building the CSR structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.000 seconds.
Building the Feature Vector...
        It takes 0.093 seconds.
Building the Label Vector...
        It takes 0.018 seconds.
Number of classes: 6
Number of feature dimensions: 3703
Number of vertices: 3327
train nodes 120, valid nodes 500, test nodes 1000
Number of GPUs: 1
GPU 0, layer [0, 34)
WARNING: the current version only applies to linear GNN models!
*** Node 0, starting model training...
Number of operators: 233
0 3327 0 233
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the partition [0, 233) x [0, 3327)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_DROPOUT, output tensors: 1
    Op 2: type OPERATOR_WEIGHT, output tensors: 2
    Op 3: type OPERATOR_MATMUL, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_AGGREGATION, output tensors: 6
    Op 7: type OPERATOR_ADD, output tensors: 7
    Op 8: type OPERATOR_WEIGHT, output tensors: 8
    Op 9: type OPERATOR_MATMUL, output tensors: 9
    Op 10: type OPERATOR_ADD, output tensors: 10
    Op 11: type OPERATOR_RELU, output tensors: 11
    Op 12: type OPERATOR_DROPOUT, output tensors: 12
    Op 13: type OPERATOR_AGGREGATION, output tensors: 13
    Op 14: type OPERATOR_ADD, output tensors: 14
    Op 15: type OPERATOR_WEIGHT, output tensors: 15
    Op 16: type OPERATOR_MATMUL, output tensors: 16
    Op 17: type OPERATOR_ADD, output tensors: 17
    Op 18: type OPERATOR_RELU, output tensors: 18
    Op 19: type OPERATOR_DROPOUT, output tensors: 19
    Op 20: type OPERATOR_AGGREGATION, output tensors: 20
    Op 21: type OPERATOR_ADD, output tensors: 21
    Op 22: type OPERATOR_WEIGHT, output tensors: 22
    Op 23: type OPERATOR_MATMUL, output tensors: 23
    Op 24: type OPERATOR_ADD, output tensors: 24
    Op 25: type OPERATOR_RELU, output tensors: 25
    Op 26: type OPERATOR_DROPOUT, output tensors: 26
    Op 27: type OPERATOR_AGGREGATION, output tensors: 27
    Op 28: type OPERATOR_ADD, output tensors: 28
    Op 29: type OPERATOR_WEIGHT, output tensors: 29
    Op 30: type OPERATOR_MATMUL, output tensors: 30
    Op 31: type OPERATOR_ADD, output tensors: 31
    Op 32: type OPERATOR_RELU, output tensors: 32
    Op 33: type OPERATOR_DROPOUT, output tensors: 33
    Op 34: type OPERATOR_AGGREGATION, output tensors: 34
    Op 35: type OPERATOR_ADD, output tensors: 35
    Op 36: type OPERATOR_WEIGHT, output tensors: 36
    Op 37: type OPERATOR_MATMUL, output tensors: 37
    Op 38: type OPERATOR_ADD, output tensors: 38
    Op 39: type OPERATOR_RELU, output tensors: 39
    Op 40: type OPERATOR_DROPOUT, output tensors: 40
    Op 41: type OPERATOR_AGGREGATION, output tensors: 41
    Op 42: type OPERATOR_ADD, output tensors: 42
    Op 43: type OPERATOR_WEIGHT, output tensors: 43
    Op 44: type OPERATOR_MATMUL, output tensors: 44
    Op 45: type OPERATOR_ADD, output tensors: 45
    Op 46: type OPERATOR_RELU, output tensors: 46
    Op 47: type OPERATOR_DROPOUT, output tensors: 47
    Op 48: type OPERATOR_AGGREGATION, output tensors: 48
    Op 49: type OPERATOR_ADD, output tensors: 49
    Op 50: type OPERATOR_WEIGHT, output tensors: 50
    Op 51: type OPERATOR_MATMUL, output tensors: 51
    Op 52: type OPERATOR_ADD, output tensors: 52
    Op 53: type OPERATOR_RELU, output tensors: 53
    Op 54: type OPERATOR_DROPOUT, output tensors: 54
    Op 55: type OPERATOR_AGGREGATION, output tensors: 55
    Op 56: type OPERATOR_ADD, output tensors: 56
    Op 57: type OPERATOR_WEIGHT, output tensors: 57
    Op 58: type OPERATOR_MATMUL, output tensors: 58
    Op 59: type OPERATOR_ADD, output tensors: 59
    Op 60: type OPERATOR_RELU, output tensors: 60
    Op 61: type OPERATOR_DROPOUT, output tensors: 61
    Op 62: type OPERATOR_AGGREGATION, output tensors: 62
    Op 63: type OPERATOR_ADD, output tensors: 63
    Op 64: type OPERATOR_WEIGHT, output tensors: 64
    Op 65: type OPERATOR_MATMUL, output tensors: 65
    Op 66: type OPERATOR_ADD, output tensors: 66
    Op 67: type OPERATOR_RELU, output tensors: 67
    Op 68: type OPERATOR_DROPOUT, output tensors: 68
    Op 69: type OPERATOR_AGGREGATION, output tensors: 69
    Op 70: type OPERATOR_ADD, output tensors: 70
    Op 71: type OPERATOR_WEIGHT, output tensors: 71
    Op 72: type OPERATOR_MATMUL, output tensors: 72
    Op 73: type OPERATOR_ADD, output tensors: 73
    Op 74: type OPERATOR_RELU, output tensors: 74
    Op 75: type OPERATOR_DROPOUT, output tensors: 75
    Op 76: type OPERATOR_AGGREGATION, output tensors: 76
    Op 77: type OPERATOR_ADD, output tensors: 77
    Op 78: type OPERATOR_WEIGHT, output tensors: 78
    Op 79: type OPERATOR_MATMUL, output tensors: 79
    Op 80: type OPERATOR_ADD, output tensors: 80
    Op 81: type OPERATOR_RELU, output tensors: 81
    Op 82: type OPERATOR_DROPOUT, output tensors: 82
    Op 83: type OPERATOR_AGGREGATION, output tensors: 83
    Op 84: type OPERATOR_ADD, output tensors: 84
    Op 85: type OPERATOR_WEIGHT, output tensors: 85
    Op 86: type OPERATOR_MATMUL, output tensors: 86
    Op 87: type OPERATOR_ADD, output tensors: 87
    Op 88: type OPERATOR_RELU, output tensors: 88
    Op 89: type OPERATOR_DROPOUT, output tensors: 89
    Op 90: type OPERATOR_AGGREGATION, output tensors: 90
    Op 91: type OPERATOR_ADD, output tensors: 91
    Op 92: type OPERATOR_WEIGHT, output tensors: 92
    Op 93: type OPERATOR_MATMUL, output tensors: 93
    Op 94: type OPERATOR_ADD, output tensors: 94
    Op 95: type OPERATOR_RELU, output tensors: 95
    Op 96: type OPERATOR_DROPOUT, output tensors: 96
    Op 97: type OPERATOR_AGGREGATION, output tensors: 97
    Op 98: type OPERATOR_ADD, output tensors: 98
    Op 99: type OPERATOR_WEIGHT, output tensors: 99
    Op 100: type OPERATOR_MATMUL, output tensors: 100
    Op 101: type OPERATOR_ADD, output tensors: 101
    Op 102: type OPERATOR_RELU, output tensors: 102
    Op 103: type OPERATOR_DROPOUT, output tensors: 103
    Op 104: type OPERATOR_AGGREGATION, output tensors: 104
    Op 105: type OPERATOR_ADD, output tensors: 105
    Op 106: type OPERATOR_WEIGHT, output tensors: 106
    Op 107: type OPERATOR_MATMUL, output tensors: 107
    Op 108: type OPERATOR_ADD, output tensors: 108
    Op 109: type OPERATOR_RELU, output tensors: 109
    Op 110: type OPERATOR_DROPOUT, output tensors: 110
    Op 111: type OPERATOR_AGGREGATION, output tensors: 111
    Op 112: type OPERATOR_ADD, output tensors: 112
    Op 113: type OPERATOR_WEIGHT, output tensors: 113
    Op 114: type OPERATOR_MATMUL, output tensors: 114
    Op 115: type OPERATOR_ADD, output tensors: 115
    Op 116: type OPERATOR_RELU, output tensors: 116
    Op 117: type OPERATOR_DROPOUT, output tensors: 117
    Op 118: type OPERATOR_AGGREGATION, output tensors: 118
    Op 119: type OPERATOR_ADD, output tensors: 119
    Op 120: type OPERATOR_WEIGHT, output tensors: 120
    Op 121: type OPERATOR_MATMUL, output tensors: 121
    Op 122: type OPERATOR_ADD, output tensors: 122
    Op 123: type OPERATOR_RELU, output tensors: 123
    Op 124: type OPERATOR_DROPOUT, output tensors: 124
    Op 125: type OPERATOR_AGGREGATION, output tensors: 125
    Op 126: type OPERATOR_ADD, output tensors: 126
    Op 127: type OPERATOR_WEIGHT, output tensors: 127
    Op 128: type OPERATOR_MATMUL, output tensors: 128
    Op 129: type OPERATOR_ADD, output tensors: 129
    Op 130: type OPERATOR_RELU, output tensors: 130
    Op 131: type OPERATOR_DROPOUT, output tensors: 131
    Op 132: type OPERATOR_AGGREGATION, output tensors: 132
    Op 133: type OPERATOR_ADD, output tensors: 133
    Op 134: type OPERATOR_WEIGHT, output tensors: 134
    Op 135: type OPERATOR_MATMUL, output tensors: 135
    Op 136: type OPERATOR_ADD, output tensors: 136
    Op 137: type OPERATOR_RELU, output tensors: 137
    Op 138: type OPERATOR_DROPOUT, output tensors: 138
    Op 139: type OPERATOR_AGGREGATION, output tensors: 139
    Op 140: type OPERATOR_ADD, output tensors: 140
    Op 141: type OPERATOR_WEIGHT, output tensors: 141
    Op 142: type OPERATOR_MATMUL, output tensors: 142
    Op 143: type OPERATOR_ADD, output tensors: 143
    Op 144: type OPERATOR_RELU, output tensors: 144
    Op 145: type OPERATOR_DROPOUT, output tensors: 145
    Op 146: type OPERATOR_AGGREGATION, output tensors: 146
    Op 147: type OPERATOR_ADD, output tensors: 147
    Op 148: type OPERATOR_WEIGHT, output tensors: 148
    Op 149: type OPERATOR_MATMUL, output tensors: 149
    Op 150: type OPERATOR_ADD, output tensors: 150
    Op 151: type OPERATOR_RELU, output tensors: 151
    Op 152: type OPERATOR_DROPOUT, output tensors: 152
    Op 153: type OPERATOR_AGGREGATION, output tensors: 153
    Op 154: type OPERATOR_ADD, output tensors: 154
    Op 155: type OPERATOR_WEIGHT, output tensors: 155
    Op 156: type OPERATOR_MATMUL, output tensors: 156
    Op 157: type OPERATOR_ADD, output tensors: 157
    Op 158: type OPERATOR_RELU, output tensors: 158
    Op 159: type OPERATOR_DROPOUT, output tensors: 159
    Op 160: type OPERATOR_AGGREGATION, output tensors: 160
    Op 161: type OPERATOR_ADD, output tensors: 161
    Op 162: type OPERATOR_WEIGHT, output tensors: 162
    Op 163: type OPERATOR_MATMUL, output tensors: 163
    Op 164: type OPERATOR_ADD, output tensors: 164
    Op 165: type OPERATOR_RELU, output tensors: 165
    Op 166: type OPERATOR_DROPOUT, output tensors: 166
    Op 167: type OPERATOR_AGGREGATION, output tensors: 167
    Op 168: type OPERATOR_ADD, output tensors: 168
    Op 169: type OPERATOR_WEIGHT, output tensors: 169
    Op 170: type OPERATOR_MATMUL, output tensors: 170
    Op 171: type OPERATOR_ADD, output tensors: 171
    Op 172: type OPERATOR_RELU, output tensors: 172
    Op 173: type OPERATOR_DROPOUT, output tensors: 173
    Op 174: type OPERATOR_AGGREGATION, output tensors: 174
    Op 175: type OPERATOR_ADD, output tensors: 175
    Op 176: type OPERATOR_WEIGHT, output tensors: 176
    Op 177: type OPERATOR_MATMUL, output tensors: 177
    Op 178: type OPERATOR_ADD, output tensors: 178
    Op 179: type OPERATOR_RELU, output tensors: 179
    Op 180: type OPERATOR_DROPOUT, output tensors: 180
    Op 181: type OPERATOR_AGGREGATION, output tensors: 181
    Op 182: type OPERATOR_ADD, output tensors: 182
    Op 183: type OPERATOR_WEIGHT, output tensors: 183
    Op 184: type OPERATOR_MATMUL, output tensors: 184
    Op 185: type OPERATOR_ADD, output tensors: 185
    Op 186: type OPERATOR_RELU, output tensors: 186
    Op 187: type OPERATOR_DROPOUT, output tensors: 187
    Op 188: type OPERATOR_AGGREGATION, output tensors: 188
    Op 189: type OPERATOR_ADD, output tensors: 189
    Op 190: type OPERATOR_WEIGHT, output tensors: 190
    Op 191: type OPERATOR_MATMUL, output tensors: 191
    Op 192: type OPERATOR_ADD, output tensors: 192
    Op 193: type OPERATOR_RELU, output tensors: 193
    Op 194: type OPERATOR_DROPOUT, output tensors: 194
    Op 195: type OPERATOR_AGGREGATION, output tensors: 195
    Op 196: type OPERATOR_ADD, output tensors: 196
    Op 197: type OPERATOR_WEIGHT, output tensors: 197
    Op 198: type OPERATOR_MATMUL, output tensors: 198
    Op 199: type OPERATOR_ADD, output tensors: 199
    Op 200: type OPERATOR_RELU, output tensors: 200
    Op 201: type OPERATOR_DROPOUT, output tensors: 201
    Op 202: type OPERATOR_AGGREGATION, output tensors: 202
    Op 203: type OPERATOR_ADD, output tensors: 203
    Op 204: type OPERATOR_WEIGHT, output tensors: 204
    Op 205: type OPERATOR_MATMUL, output tensors: 205
    Op 206: type OPERATOR_ADD, output tensors: 206
    Op 207: type OPERATOR_RELU, output tensors: 207
    Op 208: type OPERATOR_DROPOUT, output tensors: 208
    Op 209: type OPERATOR_AGGREGATION, output tensors: 209
    Op 210: type OPERATOR_ADD, output tensors: 210
    Op 211: type OPERATOR_WEIGHT, output tensors: 211
    Op 212: type OPERATOR_MATMUL, output tensors: 212
    Op 213: type OPERATOR_ADD, output tensors: 213
    Op 214: type OPERATOR_RELU, output tensors: 214
    Op 215: type OPERATOR_DROPOUT, output tensors: 215
    Op 216: type OPERATOR_AGGREGATION, output tensors: 216
    Op 217: type OPERATOR_ADD, output tensors: 217
    Op 218: type OPERATOR_WEIGHT, output tensors: 218
    Op 219: type OPERATOR_MATMUL, output tensors: 219
    Op 220: type OPERATOR_ADD, output tensors: 220
    Op 221: type OPERATOR_RELU, output tensors: 221
    Op 222: type OPERATOR_DROPOUT, output tensors: 222
    Op 223: type OPERATOR_AGGREGATION, output tensors: 223
    Op 224: type OPERATOR_ADD, output tensors: 224
    Op 225: type OPERATOR_WEIGHT, output tensors: 225
    Op 226: type OPERATOR_MATMUL, output tensors: 226
    Op 227: type OPERATOR_ADD, output tensors: 227
    Op 228: type OPERATOR_RELU, output tensors: 228
    Op 229: type OPERATOR_DROPOUT, output tensors: 229
    Op 230: type OPERATOR_WEIGHT, output tensors: 230
    Op 231: type OPERATOR_MATMUL, output tensors: 231
    Op 232: type OPERATOR_SOFTMAX, output tensors: 232
Boundaries: 0 3327
Fragments: [0, 3327)
Chunks (number of global chunks: 1): 0-[0, 3327)
(Forwarding) Node 0 (fragment 0) depends on nodes:
(Backwarding) Node 0 (fragment 0) depends on nodes:
(I-link dependencies): node 0 should send activation to nodes:
(I-link dependencies): node 0 should receive activation from nodes:
(I-link dependencies): node 0 should send gradient to nodes:
(I-link dependencies): node 0 should receive gradient from nodes:
3327, 12431, 12431
Number of vertices per chunk: 3327
csr in-out ready !*** Node 0, setting up some other necessary information...
*** Node 0, starting the helper threads...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 0, mapping weight op 120
+++++++++ Node 0, mapping weight op 127
+++++++++ Node 0, mapping weight op 8
+++++++++ Node 0, mapping weight op 106
+++++++++ Node 0, mapping weight op 113
+++++++++ Node 0, mapping weight op 141
+++++++++ Node 0, mapping weight op 148
+++++++++ Node 0, mapping weight op 92
+++++++++ Node 0, mapping weight op 99
+++++++++ Node 0, mapping weight op 78
+++++++++ Node 0, mapping weight op 85
+++++++++ Node 0, mapping weight op 64
+++++++++ Node 0, mapping weight op 71
+++++++++ Node 0, mapping weight op 155
+++++++++ Node 0, mapping weight op 162
+++++++++ Node 0, mapping weight op 169
+++++++++ Node 0, mapping weight op 218
+++++++++ Node 0, mapping weight op 225
+++++++++ Node 0, mapping weight op 230
+++++++++ Node 0, mapping weight op 204
+++++++++ Node 0, mapping weight op 211
+++++++++ Node 0, mapping weight op 57
+++++++++ Node 0, mapping weight op 2
+++++++++ Node 0, mapping weight op 43
+++++++++ Node 0, mapping weight op 50
+++++++++ Node 0, mapping weight op 29
+++++++++ Node 0, mapping weight op 36
+++++++++ Node 0, mapping weight op 15
+++++++++ Node 0, mapping weight op 22
+++++++++ Node 0, mapping weight op 134
+++++++++ Node 0, mapping weight op 176
+++++++++ Node 0, mapping weight op 183
+++++++++ Node 0, mapping weight op 190
+++++++++ Node 0, mapping weight op 197
RANDOMLY DISPATCH THE CHUNKS...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.010000000
    Epoch 9:	Loss 21.87456	TrainAcc 0.2083	ValidAcc 0.2160	TestAcc 0.1730
    Epoch 19:	Loss 23.25611	TrainAcc 0.1583	ValidAcc 0.2340	TestAcc 0.1860
    Epoch 29:	Loss 22.56533	TrainAcc 0.1833	ValidAcc 0.2320	TestAcc 0.1820
    Epoch 39:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2300	TestAcc 0.1840
    Epoch 49:	Loss 22.79559	TrainAcc 0.1750	ValidAcc 0.2320	TestAcc 0.1820
    Epoch 59:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 69:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 79:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 89:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 99:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 109:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 119:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 129:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 139:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 149:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 159:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 169:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 179:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 189:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 199:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 209:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 219:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 229:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 239:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 249:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 259:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 269:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 279:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 289:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 299:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 309:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 319:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 329:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 339:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 349:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 359:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 369:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 379:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 389:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 399:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 409:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 419:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 429:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 439:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 449:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 459:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 469:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 479:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 489:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 499:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 509:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 519:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 529:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 539:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 549:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 559:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 569:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 579:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 589:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 599:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 609:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 619:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 629:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 639:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 649:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 659:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 669:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 679:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 689:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 699:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 709:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 719:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 729:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 739:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 749:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 759:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 769:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 779:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 789:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 799:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 809:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 819:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 829:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 839:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 849:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 859:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 869:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 879:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 889:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 899:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 909:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 919:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 929:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 939:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 949:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 959:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 969:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 979:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 989:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 999:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1009:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1019:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1029:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1039:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1049:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1059:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1069:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1079:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1089:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1099:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1109:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1119:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1129:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1139:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1149:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1159:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1169:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1179:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1189:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1199:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1209:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1219:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1229:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1239:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1249:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1259:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1269:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1279:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1289:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1299:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1309:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1319:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1329:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1339:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1349:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1359:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1369:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1379:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1389:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1399:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1409:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1419:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1429:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1439:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1449:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1459:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1469:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1479:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
    Epoch 1489:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
Node 0, Layer-level comm throughput (act): -nan GBps
Node 0, Layer-level comm throughput (grad): -nan GBps
    Epoch 1499:	Loss 23.02585	TrainAcc 0.1667	ValidAcc 0.2320	TestAcc 0.1810
Node 0, GPU memory consumption: 3.251 GB
Node 0, compression time: 0.000s, compression size: 0.000GB, throughput: -nanGBps
Node 0, decompression time: 0.000s, compression size: 0.000GB, throughput: -nanGBps
Node 0, pure compute time: 19.732 s, total compute time: 19.732 s
Node 0, wait_for_task_time: 0.002 s, wait_for_other_gpus_time: 0.000 s
------------------------node id 0,  per-epoch time: 0.019455 s---------------
************ Profiling Results ************
	Bubble: 0.629474 (s) (2.15 percentage)
	Compute: 23.396433 (s) (79.88 percentage)
	GradSync: 5.227485 (s) (17.85 percentage)
	GraphComm: 0.035738 (s) (0.12 percentage)
	Imbalance: 0.001050 (s) (0.00 percentage)
	LayerComm: 0.000000 (s) (0.00 percentage)
	Layer-level communication (cluster-wide, per epoch): 0.000 GB
Highest valid_acc: 0.2340
Target test_acc: 0.1860
Epoch to reach the target acc: 20
[MPI Rank 0] Success 
