g011.anvil.rcac.purdue.edu
Tue Jan 10 18:28:44 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:01:00.0 Off |                    0 |
| N/A   28C    P0    50W / 400W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-SXM...  On   | 00000000:41:00.0 Off |                    0 |
| N/A   27C    P0    49W / 400W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA A100-SXM...  On   | 00000000:81:00.0 Off |                    0 |
| N/A   27C    P0    49W / 400W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA A100-SXM...  On   | 00000000:C1:00.0 Off |                    0 |
| N/A   27C    P0    50W / 400W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2    9) numactl/2.0.14
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0    10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0   11) openmpi/4.0.6
  4) gmp/6.2.1              8) zlib/1.2.11  12) boost/1.74.0

 

[  4%] Built target context
[ 18%] Built target parallel
[ 19%] Built target core
[ 39%] Built target cudahelp
[ 61%] Built target test_mpi_gpu_pipelined_model_parallel
[ 61%] Built target test_mpi_gpu_hybrid
[ 61%] Built target test_cuda
[ 61%] Built target test_cuda_data_compression
[ 82%] Built target test_mpi_gpu_model_parallel
[ 89%] Built target test_nccl_mpi
[ 82%] Built target test_mpi_structual_graph
[ 82%] Built target test_cuda_graph
[ 82%] Built target test_trivial
[ 82%] Built target test_nccl_thread
[ 82%] Built target test_cuda_model_parallel
[ 82%] Built target test_cuda_pipeline_parallel
[ 82%] Built target test_graph
[ 82%] Built target test_mpi_combined
[ 91%] Built target test_mpi_model_parallel
[ 91%] Built target test_single_node_fullgpu_training
[ 91%] Built target test_mpi_pipelined_model_parallel
[ 91%] Built target test_single_node_gpu_training
[ 91%] Built target test_mpi_non_structual_graph
[ 91%] Built target test_full_structual_graph
[ 82%] Built target test_hello_world
[ 82%] Built target test_mpi_loader
[ 91%] Built target estimate_comm_volume
[ 91%] Built target test_full_non_structual_graph
[ 91%] Built target test_single_node_training
[100%] Built target OSDI2023_SINGLE_NODE_gcn
[100%] Built target OSDI2023_SINGLE_NODE_gcn_inference
[100%] Built target OSDI2023_MULTI_NODES_gcn
[100%] Built target test_two_layer_hybrid_parallelism_designer
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/reddit
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/reddit
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/reddit
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/reddit
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
Initialized node g011.anvil.rcac.purdue.edu
Initialized node g011.anvil.rcac.purdue.edu
Initialized node g011.anvil.rcac.purdue.edu
Initialized node g011.anvil.rcac.purdue.edu
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.960 seconds.
Building the CSC structure...
        It takes 2.016 seconds.
Building the CSC structure...
        It takes 2.018 seconds.
Building the CSC structure...
        It takes 2.030 seconds.
Building the CSC structure...
        It takes 1.798 seconds.
        It takes 1.847 seconds.
        It takes 1.859 seconds.
        It takes 1.893 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.419 seconds.
Building the Label Vector...
        It takes 0.051 seconds.
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
        It takes 0.403 seconds.
Building the Label Vector...
        It takes 0.033 seconds.
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
        It takes 0.409 seconds.
Building the Label Vector...
        It takes 0.032 seconds.
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
        It takes 0.417 seconds.
Building the Label Vector...
        It takes 0.029 seconds.
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
train nodes 153431, valid nodes 23831, test nodes 55703
train nodes 153431, valid nodes 23831, test nodes 55703
train nodes 153431, valid nodes 23831, test nodes 55703
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
*** Node 0, starting model training...
Number of operators: 20
0 232965 0 6
0 232965 6 11
0 232965 11 16
0 232965 16 20
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the partition [0, 6) x [0, 232965)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_WEIGHT, output tensors: 1
    Op 2: type OPERATOR_MATMUL, output tensors: 2
    Op 3: type OPERATOR_AGGREGATION, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_WEIGHT, output tensors: 6
    Op 7: type OPERATOR_MATMUL, output tensors: 7
    Op 8: type OPERATOR_AGGREGATION, output tensors: 8
    Op 9: type OPERATOR_RELU, output tensors: 9
    Op 10: type OPERATOR_DROPOUT, output tensors: 10
    Op 11: type OPERATOR_WEIGHT, output tensors: 11
    Op 12: type OPERATOR_MATMUL, output tensors: 12
    Op 13: type OPERATOR_AGGREGATION, output tensors: 13
    Op 14: type OPERATOR_RELU, output tensors: 14
    Op 15: type OPERATOR_DROPOUT, output tensors: 15
    Op 16: type OPERATOR_WEIGHT, output tensors: 16
    Op 17: type OPERATOR_MATMUL, output tensors: 17
    Op 18: type OPERATOR_AGGREGATION, output tensors: 18
    Op 19: type OPERATOR_SOFTMAX, output tensors: 19
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
Number of GPUs: 4
WARNING: the current version only applies to linear GNN models!
*** Node 3, starting model training...
Number of operators: 20
0 232965 0 6
0 232965 6 11
0 232965 11 16
0 232965 16 20
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the partition [16, 20) x [0, 232965)
*** Node 3, constructing the helper classes...
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
*** Node 1, starting model training...
Number of operators: 20
0 232965 0 6
0 232965 6 11
0 232965 11 16
0 232965 16 20
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the partition [6, 11) x [0, 232965)
*** Node 1, constructing the helper classes...
WARNING: the current version only applies to linear GNN models!
train nodes 153431, valid nodes 23831, test nodes 55703
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
*** Node 2, starting model training...
Number of operators: 20
0 232965 0 6
0 232965 6 11
0 232965 11 16
0 232965 16 20
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the partition [11, 16) x [0, 232965)
*** Node 2, constructing the helper classes...
(Forwarding) Node 3 (fragment 0) depends on nodes: 2 (Tensor: 15)
(Backwarding) Node 3 (fragment 0) depends on nodes:
(I-link dependencies): node 3 should send activation to nodes:
(I-link dependencies): node 3 should receive activation from nodes:
(I-link dependencies): node 3 should send gradient to nodes:
(I-link dependencies): node 3 should receive gradient from nodes:
(Forwarding) Node 1 (fragment 0) depends on nodes: 0 (Tensor: 5)
(Backwarding) Node 1 (fragment 0) depends on nodes: 2 (Tensor: 10)
(I-link dependencies): node 1 should send activation to nodes:
(I-link dependencies): node 1 should receive activation from nodes:
(I-link dependencies): node 1 should send gradient to nodes:
(I-link dependencies): node 1 should receive gradient from nodes:
Boundaries: 0 0 0 0 232965 232965 232965 232965
Fragments: [0, 232965)
Chunks (number of global chunks: 33): 0-[0, 7280) 1-[7280, 14560) 2-[14560, 21840) 3-[21840, 29120) 4-[29120, 36400) 5-[36400, 43680) 6-[43680, 50960) 7-[50960, 58240) 8-[58240, 65520) ... 32-[232960, 232965)
(Forwarding) Node 0 (fragment 0) depends on nodes:
(Backwarding) Node 0 (fragment 0) depends on nodes: 1 (Tensor: 5)
(I-link dependencies): node 0 should send activation to nodes:
(I-link dependencies): node 0 should receive activation from nodes:
(I-link dependencies): node 0 should send gradient to nodes:
(I-link dependencies): node 0 should receive gradient from nodes:
(Forwarding) Node 2 (fragment 0) depends on nodes: 1 (Tensor: 10)
(Backwarding) Node 2 (fragment 0) depends on nodes: 3 (Tensor: 15)
(I-link dependencies): node 2 should send activation to nodes:
(I-link dependencies): node 2 should receive activation from nodes:
(I-link dependencies): node 2 should send gradient to nodes:
(I-link dependencies): node 2 should receive gradient from nodes:
232965, 114848857, 114848857
232965, 114848857, 114848857
232965, 114848857, 114848857
232965, 114848857, 114848857
csr in-out ready !*** Node 0, setting up some other necessary information...
csr in-out ready !*** Node 3, setting up some other necessary information...
csr in-out ready !*** Node 1, setting up some other necessary information...
csr in-out ready !*** Node 2, setting up some other necessary information...
*** Node 0, starting the helper threads...
*** Node 3, starting the helper threads...
+++++++++ Node 0 initializing the weights for op[0, 6)...
+++++++++ Node 0, mapping weight op 1
+++++++++ Node 3 initializing the weights for op[16, 20)...
+++++++++ Node 3, mapping weight op 16
*** Node 1, starting the helper threads...
*** Node 2, starting the helper threads...
+++++++++ Node 1 initializing the weights for op[6, 11)...
+++++++++ Node 1, mapping weight op 6
+++++++++ Node 2 initializing the weights for op[11, 16)...
+++++++++ Node 2, mapping weight op 11
RANDOMLY DISPATCH THE CHUNKS...
*** Node 0, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 2, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
    Epoch 0:	Loss 3.71274	TrainAcc 0.0136	ValidAcc 0.0127	TestAcc 0.0129
    Epoch 1:	Loss 3.66738	TrainAcc 0.0594	ValidAcc 0.0778	TestAcc 0.0757
    Epoch 2:	Loss 3.54792	TrainAcc 0.1894	ValidAcc 0.2060	TestAcc 0.2000
    Epoch 3:	Loss 3.44217	TrainAcc 0.2300	ValidAcc 0.2401	TestAcc 0.2329
    Epoch 4:	Loss 3.35254	TrainAcc 0.3224	ValidAcc 0.3240	TestAcc 0.3196
    Epoch 5:	Loss 3.26484	TrainAcc 0.3347	ValidAcc 0.3378	TestAcc 0.3304
    Epoch 6:	Loss 3.17909	TrainAcc 0.3404	ValidAcc 0.3443	TestAcc 0.3375
    Epoch 7:	Loss 3.09174	TrainAcc 0.3770	ValidAcc 0.4016	TestAcc 0.3933
    Epoch 8:	Loss 2.99722	TrainAcc 0.4289	ValidAcc 0.4693	TestAcc 0.4634
    Epoch 9:	Loss 2.89886	TrainAcc 0.4424	ValidAcc 0.4813	TestAcc 0.4748
    Epoch 10:	Loss 2.79994	TrainAcc 0.4499	ValidAcc 0.4880	TestAcc 0.4817
    Epoch 11:	Loss 2.69810	TrainAcc 0.4546	ValidAcc 0.4918	TestAcc 0.4850
    Epoch 12:	Loss 2.59363	TrainAcc 0.4667	ValidAcc 0.5035	TestAcc 0.4962
    Epoch 13:	Loss 2.48364	TrainAcc 0.4846	ValidAcc 0.5193	TestAcc 0.5142
    Epoch 14:	Loss 2.36944	TrainAcc 0.5251	ValidAcc 0.5599	TestAcc 0.5556
    Epoch 15:	Loss 2.25987	TrainAcc 0.5585	ValidAcc 0.5947	TestAcc 0.5907
    Epoch 16:	Loss 2.18084	TrainAcc 0.5580	ValidAcc 0.5979	TestAcc 0.5920
    Epoch 17:	Loss 2.12369	TrainAcc 0.5281	ValidAcc 0.5717	TestAcc 0.5652
    Epoch 18:	Loss 2.06044	TrainAcc 0.4831	ValidAcc 0.5224	TestAcc 0.5166
    Epoch 19:	Loss 1.96558	TrainAcc 0.5071	ValidAcc 0.5512	TestAcc 0.5434
    Epoch 20:	Loss 1.86852	TrainAcc 0.5362	ValidAcc 0.5780	TestAcc 0.5696
    Epoch 21:	Loss 1.78102	TrainAcc 0.5949	ValidAcc 0.6249	TestAcc 0.6196
    Epoch 22:	Loss 1.71098	TrainAcc 0.6397	ValidAcc 0.6621	TestAcc 0.6591
    Epoch 23:	Loss 1.67663	TrainAcc 0.6495	ValidAcc 0.6703	TestAcc 0.6660
    Epoch 24:	Loss 1.66340	TrainAcc 0.6333	ValidAcc 0.6562	TestAcc 0.6529
    Epoch 25:	Loss 1.63726	TrainAcc 0.6432	ValidAcc 0.6654	TestAcc 0.6615
    Epoch 26:	Loss 1.57274	TrainAcc 0.6703	ValidAcc 0.6933	TestAcc 0.6903
    Epoch 27:	Loss 1.49735	TrainAcc 0.6749	ValidAcc 0.6997	TestAcc 0.6941
    Epoch 28:	Loss 1.43033	TrainAcc 0.6846	ValidAcc 0.7079	TestAcc 0.7020
    Epoch 29:	Loss 1.38939	TrainAcc 0.7058	ValidAcc 0.7300	TestAcc 0.7244
    Epoch 30:	Loss 1.36660	TrainAcc 0.6964	ValidAcc 0.7230	TestAcc 0.7178
    Epoch 31:	Loss 1.34735	TrainAcc 0.6739	ValidAcc 0.7057	TestAcc 0.6965
    Epoch 32:	Loss 1.33070	TrainAcc 0.6531	ValidAcc 0.6855	TestAcc 0.6760
    Epoch 33:	Loss 1.30889	TrainAcc 0.6649	ValidAcc 0.6982	TestAcc 0.6896
    Epoch 34:	Loss 1.27827	TrainAcc 0.6616	ValidAcc 0.6954	TestAcc 0.6888
    Epoch 35:	Loss 1.24119	TrainAcc 0.6902	ValidAcc 0.7155	TestAcc 0.7109
    Epoch 36:	Loss 1.19348	TrainAcc 0.7279	ValidAcc 0.7475	TestAcc 0.7451
    Epoch 37:	Loss 1.14721	TrainAcc 0.7343	ValidAcc 0.7553	TestAcc 0.7514
    Epoch 38:	Loss 1.11114	TrainAcc 0.7422	ValidAcc 0.7607	TestAcc 0.7559
    Epoch 39:	Loss 1.09033	TrainAcc 0.7579	ValidAcc 0.7745	TestAcc 0.7721
    Epoch 40:	Loss 1.05172	TrainAcc 0.7759	ValidAcc 0.7911	TestAcc 0.7878
    Epoch 41:	Loss 1.01851	TrainAcc 0.7821	ValidAcc 0.7958	TestAcc 0.7926
    Epoch 42:	Loss 0.99025	TrainAcc 0.7757	ValidAcc 0.7904	TestAcc 0.7868
    Epoch 43:	Loss 0.99629	TrainAcc 0.7706	ValidAcc 0.7844	TestAcc 0.7786
    Epoch 44:	Loss 1.02463	TrainAcc 0.7612	ValidAcc 0.7774	TestAcc 0.7712
    Epoch 45:	Loss 1.05023	TrainAcc 0.7517	ValidAcc 0.7682	TestAcc 0.7613
    Epoch 46:	Loss 1.03665	TrainAcc 0.7418	ValidAcc 0.7609	TestAcc 0.7516
    Epoch 47:	Loss 0.98729	TrainAcc 0.7538	ValidAcc 0.7741	TestAcc 0.7645
    Epoch 48:	Loss 0.93434	TrainAcc 0.7705	ValidAcc 0.7888	TestAcc 0.7820
    Epoch 49:	Loss 0.88929	TrainAcc 0.8012	ValidAcc 0.8174	TestAcc 0.8118
    Epoch 50:	Loss 0.88063	TrainAcc 0.7942	ValidAcc 0.8076	TestAcc 0.8029
    Epoch 51:	Loss 0.91320	TrainAcc 0.7776	ValidAcc 0.7943	TestAcc 0.7893
    Epoch 52:	Loss 0.95482	TrainAcc 0.7707	ValidAcc 0.7879	TestAcc 0.7825
    Epoch 53:	Loss 0.98378	TrainAcc 0.7670	ValidAcc 0.7839	TestAcc 0.7774
    Epoch 54:	Loss 0.98518	TrainAcc 0.7681	ValidAcc 0.7838	TestAcc 0.7791
    Epoch 55:	Loss 0.94892	TrainAcc 0.7801	ValidAcc 0.7992	TestAcc 0.7960
    Epoch 56:	Loss 0.91238	TrainAcc 0.8068	ValidAcc 0.8238	TestAcc 0.8184
    Epoch 57:	Loss 0.88953	TrainAcc 0.8176	ValidAcc 0.8339	TestAcc 0.8283
    Epoch 58:	Loss 0.87868	TrainAcc 0.8181	ValidAcc 0.8334	TestAcc 0.8292
    Epoch 59:	Loss 0.87619	TrainAcc 0.8138	ValidAcc 0.8290	TestAcc 0.8246
    Epoch 60:	Loss 0.87088	TrainAcc 0.8099	ValidAcc 0.8258	TestAcc 0.8205
    Epoch 61:	Loss 0.87172	TrainAcc 0.8027	ValidAcc 0.8191	TestAcc 0.8152
    Epoch 62:	Loss 0.86339	TrainAcc 0.7948	ValidAcc 0.8113	TestAcc 0.8054
    Epoch 63:	Loss 0.84616	TrainAcc 0.7982	ValidAcc 0.8152	TestAcc 0.8097
    Epoch 64:	Loss 0.83162	TrainAcc 0.7963	ValidAcc 0.8128	TestAcc 0.8079
    Epoch 65:	Loss 0.81163	TrainAcc 0.7992	ValidAcc 0.8166	TestAcc 0.8111
    Epoch 66:	Loss 0.78319	TrainAcc 0.8162	ValidAcc 0.8305	TestAcc 0.8260
    Epoch 67:	Loss 0.75293	TrainAcc 0.8337	ValidAcc 0.8487	TestAcc 0.8452
    Epoch 68:	Loss 0.74285	TrainAcc 0.8459	ValidAcc 0.8613	TestAcc 0.8569
    Epoch 69:	Loss 0.75532	TrainAcc 0.8385	ValidAcc 0.8529	TestAcc 0.8479
    Epoch 70:	Loss 0.77641	TrainAcc 0.8290	ValidAcc 0.8447	TestAcc 0.8398
    Epoch 71:	Loss 0.79712	TrainAcc 0.8237	ValidAcc 0.8395	TestAcc 0.8337
    Epoch 72:	Loss 0.79670	TrainAcc 0.8259	ValidAcc 0.8392	TestAcc 0.8350
    Epoch 73:	Loss 0.79787	TrainAcc 0.8259	ValidAcc 0.8402	TestAcc 0.8350
    Epoch 74:	Loss 0.79269	TrainAcc 0.8325	ValidAcc 0.8472	TestAcc 0.8410
    Epoch 75:	Loss 0.77568	TrainAcc 0.8410	ValidAcc 0.8541	TestAcc 0.8482
    Epoch 76:	Loss 0.75521	TrainAcc 0.8453	ValidAcc 0.8579	TestAcc 0.8532
    Epoch 77:	Loss 0.73218	TrainAcc 0.8471	ValidAcc 0.8598	TestAcc 0.8549
    Epoch 78:	Loss 0.71147	TrainAcc 0.8593	ValidAcc 0.8688	TestAcc 0.8669
    Epoch 79:	Loss 0.69860	TrainAcc 0.8628	ValidAcc 0.8748	TestAcc 0.8702
    Epoch 80:	Loss 0.69396	TrainAcc 0.8554	ValidAcc 0.8666	TestAcc 0.8633
    Epoch 81:	Loss 0.69287	TrainAcc 0.8526	ValidAcc 0.8634	TestAcc 0.8604
    Epoch 82:	Loss 0.69838	TrainAcc 0.8510	ValidAcc 0.8618	TestAcc 0.8584
    Epoch 83:	Loss 0.70057	TrainAcc 0.8477	ValidAcc 0.8606	TestAcc 0.8560
    Epoch 84:	Loss 0.69560	TrainAcc 0.8468	ValidAcc 0.8592	TestAcc 0.8546
    Epoch 85:	Loss 0.68981	TrainAcc 0.8473	ValidAcc 0.8601	TestAcc 0.8558
    Epoch 86:	Loss 0.67538	TrainAcc 0.8509	ValidAcc 0.8616	TestAcc 0.8602
    Epoch 87:	Loss 0.66342	TrainAcc 0.8548	ValidAcc 0.8653	TestAcc 0.8629
    Epoch 88:	Loss 0.65033	TrainAcc 0.8630	ValidAcc 0.8760	TestAcc 0.8719
    Epoch 89:	Loss 0.64556	TrainAcc 0.8688	ValidAcc 0.8806	TestAcc 0.8788
    Epoch 90:	Loss 0.65136	TrainAcc 0.8572	ValidAcc 0.8705	TestAcc 0.8688
    Epoch 91:	Loss 0.65656	TrainAcc 0.8545	ValidAcc 0.8662	TestAcc 0.8651
    Epoch 92:	Loss 0.66209	TrainAcc 0.8527	ValidAcc 0.8659	TestAcc 0.8635
    Epoch 93:	Loss 0.65732	TrainAcc 0.8549	ValidAcc 0.8660	TestAcc 0.8627
    Epoch 94:	Loss 0.65563	TrainAcc 0.8561	ValidAcc 0.8678	TestAcc 0.8642
    Epoch 95:	Loss 0.64582	TrainAcc 0.8592	ValidAcc 0.8712	TestAcc 0.8682
    Epoch 96:	Loss 0.63444	TrainAcc 0.8636	ValidAcc 0.8765	TestAcc 0.8713
    Epoch 97:	Loss 0.62372	TrainAcc 0.8696	ValidAcc 0.8814	TestAcc 0.8785
    Epoch 98:	Loss 0.61844	TrainAcc 0.8724	ValidAcc 0.8833	TestAcc 0.8809
    Epoch 99:	Loss 0.61251	TrainAcc 0.8695	ValidAcc 0.8827	TestAcc 0.8794
------------------------node id 0,  total time 9.722863s (per-epoch: 0.097229s)---------------
------------------------node id 3,  total time 9.722854s (per-epoch: 0.097229s)---------------
------------------------node id 2,  total time 9.722834s (per-epoch: 0.097228s)---------------
------------------------node id 1,  total time 9.722850s (per-epoch: 0.097228s)---------------
************ Profiling Results ************
	Bubble: 1.414729 (s) (14.55 percentage)
	Compute: 5.967354 (s) (61.39 percentage)
	GradSync: 0.058546 (s) (0.60 percentage)
	GraphComm: 0.004383 (s) (0.05 percentage)
	Imbalance: 0.988321 (s) (10.17 percentage)
	LayerComm: 1.287105 (s) (13.24 percentage)
	Graph-level communication (cluster-wide, per epoch): 0.000 GB
	Layer-level communication (cluster-wide, per epoch): 0.667 GB
	Graph+Layer-level communication (cluster-wide, per epoch): 0.667 GB
	Parameter-server communication (cluster-wide, per epoch): 0.002 GB
	Graph-level dev2host communication time: 0.000 s, throughput: -nan GBps
	Graph-level memcpy communication time: 0.000 s, throughput: -nan GBps
	Graph-level net Activation communication time: 0.000 s, throughput: -nan GBps
	Graph-level net Gradient communication time: 0.000 s, throughput: -nan GBps
	Graph-level network batch size: -nan Bytes
Highest valid_acc: 0.8833
Target test_acc: 0.8809
Epoch to reach the target acc: 99
[MPI Rank 3] Success 
[MPI Rank 2] Success 
[MPI Rank 0] Success 
[MPI Rank 1] Success 
