Thu Sep 21 11:49:25 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   30C    P8    20W / 230W |      3MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   30C    P8    18W / 230W |      3MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   30C    P8    13W / 230W |      3MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   30C    P8    16W / 230W |      3MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 10%] Built target context
[ 52%] Built target core
[ 70%] Built target cudahelp
[ 90%] Built target estimate_comm_volume
[ 90%] Built target OSDI2023_MULTI_NODES_resgcn
[ 90%] Built target OSDI2023_MULTI_NODES_resgcn_plus
[ 90%] Built target OSDI2023_MULTI_NODES_graphsage
[ 90%] Built target OSDI2023_MULTI_NODES_gcn
[ 90%] Built target OSDI2023_MULTI_NODES_gcnii
Running experiments...
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
DONE MPI INIT
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7
DONE MPI INITDONE MPI INIT
Initialized node 4 on machine gnerv8

Initialized node 5 on machine gnerv8
DONE MPI INIT
DONE MPI INIT
Initialized node 6 on machine gnerv8
Initialized node 7 on machine gnerv8
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.238 seconds.
Building the CSC structure...
        It takes 0.241 seconds.
Building the CSC structure...
        It takes 0.240 seconds.
Building the CSC structure...
        It takes 0.245 seconds.
Building the CSC structure...
        It takes 0.252 seconds.
Building the CSC structure...
        It takes 0.255 seconds.
Building the CSC structure...
        It takes 0.266 seconds.
Building the CSC structure...
        It takes 0.266 seconds.
Building the CSC structure...
        It takes 0.221 seconds.
        It takes 0.225 seconds.
        It takes 0.232 seconds.
        It takes 0.241 seconds.
        It takes 0.230 seconds.
        It takes 0.250 seconds.
        It takes 0.257 seconds.
        It takes 0.260 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.238 seconds.
Building the Label Vector...
        It takes 0.244 seconds.
Building the Label Vector...
        It takes 0.275 seconds.
Building the Label Vector...
        It takes 0.271 seconds.
Building the Label Vector...
        It takes 0.269 seconds.
Building the Label Vector...
        It takes 0.239 seconds.
Building the Label Vector...
        It takes 0.240 seconds.
Building the Label Vector...
        It takes 0.276 seconds.
Building the Label Vector...
        It takes 0.525 seconds.
        It takes 0.596 seconds.
        It takes 0.544 seconds.
        It takes 0.585 seconds.
        It takes 0.639 seconds.
        It takes 0.639 seconds.
        It takes 0.625 seconds.
        It takes 0.641 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/ogbn_mag/8_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 349
Number of feature dimensions: 128
Number of vertices: 736389
Number of GPUs: 8
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 92049
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 92049
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 92049
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
736389, 11529061, 11529061
Number of vertices per chunk: 92049
736389, 11529061, 11529061
Number of vertices per chunk: 92049
736389, 11529061, 11529061
Number of vertices per chunk: 92049
train nodes 629571, valid nodes 64879, test nodes 41939
GPU 0, layer [0, 33)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 92048) 1-[92048, 184097) 2-[184097, 276145) 3-[276145, 368194) 4-[368194, 460242) 5-[460242, 552291) 6-[552291, 644340) 7-[644340, 736389)
736389, 11529061, 11529061
Number of vertices per chunk: 92049
csr in-out ready !Start Cost Model Initialization...
736389, 11529061, 11529061
Number of vertices per chunk: 92049
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 60.364 Gbps (per GPU), 482.915 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 60.067 Gbps (per GPU), 480.536 Gbps (aggregated)
The layer-level communication performance: 60.066 Gbps (per GPU), 480.527 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.828 Gbps (per GPU), 478.622 Gbps (aggregated)
The layer-level communication performance: 59.802 Gbps (per GPU), 478.417 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.600 Gbps (per GPU), 476.803 Gbps (aggregated)
The layer-level communication performance: 59.543 Gbps (per GPU), 476.347 Gbps (aggregated)
The layer-level communication performance: 59.511 Gbps (per GPU), 476.089 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.740 Gbps (per GPU), 1269.919 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.734 Gbps (per GPU), 1269.871 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.742 Gbps (per GPU), 1269.939 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.734 Gbps (per GPU), 1269.871 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.722 Gbps (per GPU), 1269.775 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.734 Gbps (per GPU), 1269.875 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.764 Gbps (per GPU), 1270.111 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.737 Gbps (per GPU), 1269.897 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 104.632 Gbps (per GPU), 837.054 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.631 Gbps (per GPU), 837.047 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.633 Gbps (per GPU), 837.061 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.630 Gbps (per GPU), 837.040 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.633 Gbps (per GPU), 837.068 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.632 Gbps (per GPU), 837.054 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.633 Gbps (per GPU), 837.061 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 104.632 Gbps (per GPU), 837.054 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 39.377 Gbps (per GPU), 315.017 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.377 Gbps (per GPU), 315.014 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.377 Gbps (per GPU), 315.018 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.377 Gbps (per GPU), 315.013 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.376 Gbps (per GPU), 315.007 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.376 Gbps (per GPU), 315.008 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.376 Gbps (per GPU), 315.004 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 39.374 Gbps (per GPU), 314.995 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.39ms  5.42ms  9.13ms  3.81 92.05K  2.94M
 chk_1  2.39ms  4.64ms  8.33ms  3.48 92.05K  1.61M
 chk_2  2.39ms  4.53ms  8.21ms  3.43 92.05K  1.55M
 chk_3  2.39ms  4.36ms  8.03ms  3.36 92.05K  0.83M
 chk_4  2.39ms  4.57ms  8.24ms  3.44 92.05K  1.15M
 chk_5  2.39ms  4.37ms  8.03ms  3.36 92.05K  0.77M
 chk_6  2.40ms  4.60ms  8.26ms  3.44 92.05K  1.04M
 chk_7  2.39ms  4.40ms  8.06ms  3.37 92.05K  0.91M
   Avg  2.39  4.61  8.29
   Max  2.40  5.42  9.13
   Min  2.39  4.36  8.03
 Ratio  1.00  1.24  1.14
   Var  0.00  0.10  0.11
Profiling takes 1.514 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 4, starting model training...
*** Node 3, starting model training...
*** Node 5, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 368194, Num Local Vertices: 92048
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 460242, Num Local Vertices: 92049
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 552291, Num Local Vertices: 92049
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 644340, Num Local Vertices: 92049
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 92048
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 92048, Num Local Vertices: 92049
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 184097, Num Local Vertices: 92048
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 276145, Num Local Vertices: 92049
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 262493
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
*** Node 4, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 5.8664	TrainAcc 0.0005	ValidAcc 0.0004	TestAcc 0.0003	BestValid 0.0004
	Epoch 50:	Loss 4.8876	TrainAcc 0.1058	ValidAcc 0.1015	TestAcc 0.0811	BestValid 0.1015
	Epoch 100:	Loss 4.2924	TrainAcc 0.1467	ValidAcc 0.1232	TestAcc 0.2073	BestValid 0.1232
	Epoch 150:	Loss 3.9192	TrainAcc 0.1827	ValidAcc 0.1929	TestAcc 0.2359	BestValid 0.1929
	Epoch 200:	Loss 3.7083	TrainAcc 0.2050	ValidAcc 0.2005	TestAcc 0.2236	BestValid 0.2005
	Epoch 250:	Loss 3.5905	TrainAcc 0.2214	ValidAcc 0.2085	TestAcc 0.2393	BestValid 0.2085
	Epoch 300:	Loss 3.5129	TrainAcc 0.2313	ValidAcc 0.2099	TestAcc 0.2420	BestValid 0.2099
	Epoch 350:	Loss 3.4486	TrainAcc 0.2379	ValidAcc 0.2121	TestAcc 0.2442	BestValid 0.2121
	Epoch 400:	Loss 3.4038	TrainAcc 0.2426	ValidAcc 0.2195	TestAcc 0.2527	BestValid 0.2195
	Epoch 450:	Loss 3.3697	TrainAcc 0.2470	ValidAcc 0.2211	TestAcc 0.2496	BestValid 0.2211
	Epoch 500:	Loss 3.3429	TrainAcc 0.2502	ValidAcc 0.2179	TestAcc 0.2421	BestValid 0.2211
	Epoch 550:	Loss 3.3229	TrainAcc 0.2530	ValidAcc 0.2288	TestAcc 0.2555	BestValid 0.2288
	Epoch 600:	Loss 3.3038	TrainAcc 0.2553	ValidAcc 0.2252	TestAcc 0.2534	BestValid 0.2288
	Epoch 650:	Loss 3.2897	TrainAcc 0.2569	ValidAcc 0.2285	TestAcc 0.2559	BestValid 0.2288
	Epoch 700:	Loss 3.2758	TrainAcc 0.2586	ValidAcc 0.2303	TestAcc 0.2584	BestValid 0.2303
	Epoch 750:	Loss 3.2648	TrainAcc 0.2606	ValidAcc 0.2308	TestAcc 0.2618	BestValid 0.2308
	Epoch 800:	Loss 3.2513	TrainAcc 0.2622	ValidAcc 0.2338	TestAcc 0.2678	BestValid 0.2338
	Epoch 850:	Loss 3.2426	TrainAcc 0.2630	ValidAcc 0.2320	TestAcc 0.2656	BestValid 0.2338
	Epoch 900:	Loss 3.2334	TrainAcc 0.2642	ValidAcc 0.2345	TestAcc 0.2698	BestValid 0.2345
	Epoch 950:	Loss 3.2248	TrainAcc 0.2652	ValidAcc 0.2373	TestAcc 0.2818	BestValid 0.2373
	Epoch 1000:	Loss 3.2188	TrainAcc 0.2657	ValidAcc 0.2357	TestAcc 0.2781	BestValid 0.2373
	Epoch 1050:	Loss 3.2118	TrainAcc 0.2671	ValidAcc 0.2341	TestAcc 0.2792	BestValid 0.2373
	Epoch 1100:	Loss 3.2053	TrainAcc 0.2682	ValidAcc 0.2329	TestAcc 0.2796	BestValid 0.2373
	Epoch 1150:	Loss 3.1967	TrainAcc 0.2686	ValidAcc 0.2369	TestAcc 0.2920	BestValid 0.2373
	Epoch 1200:	Loss 3.1912	TrainAcc 0.2699	ValidAcc 0.2367	TestAcc 0.2924	BestValid 0.2373
	Epoch 1250:	Loss 3.1853	TrainAcc 0.2704	ValidAcc 0.2404	TestAcc 0.2997	BestValid 0.2404
	Epoch 1300:	Loss 3.1797	TrainAcc 0.2717	ValidAcc 0.2422	TestAcc 0.3034	BestValid 0.2422
	Epoch 1350:	Loss 3.1753	TrainAcc 0.2719	ValidAcc 0.2408	TestAcc 0.3016	BestValid 0.2422
	Epoch 1400:	Loss 3.1683	TrainAcc 0.2725	ValidAcc 0.2402	TestAcc 0.3021	BestValid 0.2422
	Epoch 1450:	Loss 3.1660	TrainAcc 0.2728	ValidAcc 0.2404	TestAcc 0.3046	BestValid 0.2422
	Epoch 1500:	Loss 3.1583	TrainAcc 0.2737	ValidAcc 0.2359	TestAcc 0.2997	BestValid 0.2422
	Epoch 1550:	Loss 3.1560	TrainAcc 0.2745	ValidAcc 0.2390	TestAcc 0.3039	BestValid 0.2422
	Epoch 1600:	Loss 3.1513	TrainAcc 0.2758	ValidAcc 0.2448	TestAcc 0.3095	BestValid 0.2448
	Epoch 1650:	Loss 3.1464	TrainAcc 0.2758	ValidAcc 0.2417	TestAcc 0.3068	BestValid 0.2448
	Epoch 1700:	Loss 3.1407	TrainAcc 0.2757	ValidAcc 0.2423	TestAcc 0.3084	BestValid 0.2448
	Epoch 1750:	Loss 3.1399	TrainAcc 0.2766	ValidAcc 0.2424	TestAcc 0.3080	BestValid 0.2448
	Epoch 1800:	Loss 3.1333	TrainAcc 0.2767	ValidAcc 0.2391	TestAcc 0.3066	BestValid 0.2448
	Epoch 1850:	Loss 3.1307	TrainAcc 0.2774	ValidAcc 0.2380	TestAcc 0.3046	BestValid 0.2448
	Epoch 1900:	Loss 3.1255	TrainAcc 0.2783	ValidAcc 0.2393	TestAcc 0.3074	BestValid 0.2448
	Epoch 1950:	Loss 3.1240	TrainAcc 0.2786	ValidAcc 0.2435	TestAcc 0.3109	BestValid 0.2448
	Epoch 2000:	Loss 3.1208	TrainAcc 0.2791	ValidAcc 0.2422	TestAcc 0.3108	BestValid 0.2448
	Epoch 2050:	Loss 3.1151	TrainAcc 0.2794	ValidAcc 0.2394	TestAcc 0.3079	BestValid 0.2448
	Epoch 2100:	Loss 3.1100	TrainAcc 0.2805	ValidAcc 0.2448	TestAcc 0.3128	BestValid 0.2448
	Epoch 2150:	Loss 3.1084	TrainAcc 0.2803	ValidAcc 0.2429	TestAcc 0.3123	BestValid 0.2448
	Epoch 2200:	Loss 3.1070	TrainAcc 0.2812	ValidAcc 0.2462	TestAcc 0.3139	BestValid 0.2462
	Epoch 2250:	Loss 3.1024	TrainAcc 0.2811	ValidAcc 0.2425	TestAcc 0.3126	BestValid 0.2462
	Epoch 2300:	Loss 3.1023	TrainAcc 0.2815	ValidAcc 0.2410	TestAcc 0.3101	BestValid 0.2462
	Epoch 2350:	Loss 3.0980	TrainAcc 0.2818	ValidAcc 0.2400	TestAcc 0.3090	BestValid 0.2462
	Epoch 2400:	Loss 3.0949	TrainAcc 0.2831	ValidAcc 0.2462	TestAcc 0.3149	BestValid 0.2462
	Epoch 2450:	Loss 3.0915	TrainAcc 0.2835	ValidAcc 0.2415	TestAcc 0.3098	BestValid 0.2462
	Epoch 2500:	Loss 3.0909	TrainAcc 0.2836	ValidAcc 0.2472	TestAcc 0.3150	BestValid 0.2472
	Epoch 2550:	Loss 3.0851	TrainAcc 0.2835	ValidAcc 0.2415	TestAcc 0.3102	BestValid 0.2472
	Epoch 2600:	Loss 3.0841	TrainAcc 0.2850	ValidAcc 0.2482	TestAcc 0.3158	BestValid 0.2482
	Epoch 2650:	Loss 3.0774	TrainAcc 0.2847	ValidAcc 0.2446	TestAcc 0.3136	BestValid 0.2482
	Epoch 2700:	Loss 3.0748	TrainAcc 0.2852	ValidAcc 0.2467	TestAcc 0.3141	BestValid 0.2482
	Epoch 2750:	Loss 3.0739	TrainAcc 0.2853	ValidAcc 0.2425	TestAcc 0.3095	BestValid 0.2482
	Epoch 2800:	Loss 3.0700	TrainAcc 0.2861	ValidAcc 0.2446	TestAcc 0.3127	BestValid 0.2482
	Epoch 2850:	Loss 3.0697	TrainAcc 0.2863	ValidAcc 0.2456	TestAcc 0.3137	BestValid 0.2482
	Epoch 2900:	Loss 3.0651	TrainAcc 0.2867	ValidAcc 0.2459	TestAcc 0.3145	BestValid 0.2482
	Epoch 2950:	Loss 3.0660	TrainAcc 0.2869	ValidAcc 0.2473	TestAcc 0.3142	BestValid 0.2482
	Epoch 3000:	Loss 3.0630	TrainAcc 0.2876	ValidAcc 0.2449	TestAcc 0.3118	BestValid 0.2482
	Epoch 3050:	Loss 3.0599	TrainAcc 0.2876	ValidAcc 0.2457	TestAcc 0.3132	BestValid 0.2482
	Epoch 3100:	Loss 3.0552	TrainAcc 0.2880	ValidAcc 0.2456	TestAcc 0.3137	BestValid 0.2482
	Epoch 3150:	Loss 3.0564	TrainAcc 0.2873	ValidAcc 0.2481	TestAcc 0.3148	BestValid 0.2482
	Epoch 3200:	Loss 3.0526	TrainAcc 0.2893	ValidAcc 0.2495	TestAcc 0.3153	BestValid 0.2495
	Epoch 3250:	Loss 3.0522	TrainAcc 0.2891	ValidAcc 0.2452	TestAcc 0.3122	BestValid 0.2495
	Epoch 3300:	Loss 3.0472	TrainAcc 0.2896	ValidAcc 0.2469	TestAcc 0.3138	BestValid 0.2495
	Epoch 3350:	Loss 3.0442	TrainAcc 0.2900	ValidAcc 0.2481	TestAcc 0.3144	BestValid 0.2495
	Epoch 3400:	Loss 3.0424	TrainAcc 0.2898	ValidAcc 0.2486	TestAcc 0.3143	BestValid 0.2495
	Epoch 3450:	Loss 3.0405	TrainAcc 0.2902	ValidAcc 0.2479	TestAcc 0.3149	BestValid 0.2495
	Epoch 3500:	Loss 3.0402	TrainAcc 0.2907	ValidAcc 0.2475	TestAcc 0.3148	BestValid 0.2495
	Epoch 3550:	Loss 3.0383	TrainAcc 0.2910	ValidAcc 0.2526	TestAcc 0.3176	BestValid 0.2526
	Epoch 3600:	Loss 3.0331	TrainAcc 0.2913	ValidAcc 0.2465	TestAcc 0.3138	BestValid 0.2526
	Epoch 3650:	Loss 3.0314	TrainAcc 0.2906	ValidAcc 0.2460	TestAcc 0.3123	BestValid 0.2526
	Epoch 3700:	Loss 3.0315	TrainAcc 0.2910	ValidAcc 0.2508	TestAcc 0.3153	BestValid 0.2526
	Epoch 3750:	Loss 3.0267	TrainAcc 0.2920	ValidAcc 0.2488	TestAcc 0.3155	BestValid 0.2526
	Epoch 3800:	Loss 3.0274	TrainAcc 0.2922	ValidAcc 0.2519	TestAcc 0.3167	BestValid 0.2526
	Epoch 3850:	Loss 3.0253	TrainAcc 0.2923	ValidAcc 0.2488	TestAcc 0.3148	BestValid 0.2526
	Epoch 3900:	Loss 3.0219	TrainAcc 0.2925	ValidAcc 0.2512	TestAcc 0.3160	BestValid 0.2526
	Epoch 3950:	Loss 3.0198	TrainAcc 0.2928	ValidAcc 0.2493	TestAcc 0.3144	BestValid 0.2526
	Epoch 4000:	Loss 3.0218	TrainAcc 0.2930	ValidAcc 0.2527	TestAcc 0.3176	BestValid 0.2527
	Epoch 4050:	Loss 3.0179	TrainAcc 0.2929	ValidAcc 0.2494	TestAcc 0.3147	BestValid 0.2527
	Epoch 4100:	Loss 3.0181	TrainAcc 0.2938	ValidAcc 0.2503	TestAcc 0.3162	BestValid 0.2527
	Epoch 4150:	Loss 3.0145	TrainAcc 0.2936	ValidAcc 0.2536	TestAcc 0.3188	BestValid 0.2536
	Epoch 4200:	Loss 3.0143	TrainAcc 0.2939	ValidAcc 0.2542	TestAcc 0.3193	BestValid 0.2542
	Epoch 4250:	Loss 3.0114	TrainAcc 0.2942	ValidAcc 0.2537	TestAcc 0.3205	BestValid 0.2542
	Epoch 4300:	Loss 3.0070	TrainAcc 0.2947	ValidAcc 0.2541	TestAcc 0.3201	BestValid 0.2542
	Epoch 4350:	Loss 3.0054	TrainAcc 0.2947	ValidAcc 0.2505	TestAcc 0.3163	BestValid 0.2542
	Epoch 4400:	Loss 3.0037	TrainAcc 0.2950	ValidAcc 0.2479	TestAcc 0.3156	BestValid 0.2542
	Epoch 4450:	Loss 3.0048	TrainAcc 0.2949	ValidAcc 0.2532	TestAcc 0.3188	BestValid 0.2542
	Epoch 4500:	Loss 3.0000	TrainAcc 0.2950	ValidAcc 0.2539	TestAcc 0.3198	BestValid 0.2542
	Epoch 4550:	Loss 3.0033	TrainAcc 0.2952	ValidAcc 0.2511	TestAcc 0.3173	BestValid 0.2542
	Epoch 4600:	Loss 2.9970	TrainAcc 0.2953	ValidAcc 0.2553	TestAcc 0.3207	BestValid 0.2553
	Epoch 4650:	Loss 2.9989	TrainAcc 0.2950	ValidAcc 0.2529	TestAcc 0.3185	BestValid 0.2553
	Epoch 4700:	Loss 2.9980	TrainAcc 0.2955	ValidAcc 0.2540	TestAcc 0.3201	BestValid 0.2553
	Epoch 4750:	Loss 2.9957	TrainAcc 0.2954	ValidAcc 0.2513	TestAcc 0.3171	BestValid 0.2553
	Epoch 4800:	Loss 2.9951	TrainAcc 0.2954	ValidAcc 0.2551	TestAcc 0.3205	BestValid 0.2553
	Epoch 4850:	Loss 2.9920	TrainAcc 0.2958	ValidAcc 0.2502	TestAcc 0.3163	BestValid 0.2553
	Epoch 4900:	Loss 2.9906	TrainAcc 0.2965	ValidAcc 0.2554	TestAcc 0.3221	BestValid 0.2554
	Epoch 4950:	Loss 2.9894	TrainAcc 0.2961	ValidAcc 0.2526	TestAcc 0.3188	BestValid 0.2554
	Epoch 5000:	Loss 2.9887	TrainAcc 0.2964	ValidAcc 0.2530	TestAcc 0.3194	BestValid 0.2554
****** Epoch Time (Excluding Evaluation Cost): 0.357 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.682 ms (Max: 1.034, Min: 0.158, Sum: 5.452)
Cluster-Wide Average, Compute: 168.911 ms (Max: 186.236, Min: 163.262, Sum: 1351.288)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.065)
Cluster-Wide Average, Bubble-Imbalance: 0.014 ms (Max: 0.015, Min: 0.013, Sum: 0.116)
Cluster-Wide Average, Communication-Graph: 176.495 ms (Max: 181.604, Min: 159.786, Sum: 1411.959)
Cluster-Wide Average, Optimization: 2.845 ms (Max: 2.867, Min: 2.820, Sum: 22.756)
Cluster-Wide Average, Others: 7.845 ms (Max: 8.343, Min: 7.225, Sum: 62.762)
****** Breakdown Sum: 356.800 ms ******
Cluster-Wide Average, GPU Memory Consumption: 19.169 GB (Max: 20.046, Min: 19.030, Sum: 153.349)
Cluster-Wide Average, Graph-Level Communication Throughput: 40.699 Gbps (Max: 50.880, Min: 23.810, Sum: 325.595)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 6.258 GB
Weight-sync communication (cluster-wide, per-epoch): 0.019 GB
Total communication (cluster-wide, per-epoch): 6.277 GB
****** Accuracy Results ******
Highest valid_acc: 0.2554
Target test_acc: 0.3221
Epoch to reach the target acc: 4899
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
