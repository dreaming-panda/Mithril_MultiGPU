Sun Sep 17 21:22:31 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   34C    P8    21W / 230W |     19MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   34C    P8    17W / 230W |      3MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   34C    P8    14W / 230W |      3MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   35C    P8    17W / 230W |      3MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
gnerv7
gnerv7
gnerv7
gnerv7
[ 13%] Built target context
[ 34%] Built target core
[ 73%] Built target cudahelp
[ 92%] Built target OSDI2023_MULTI_NODES_resgcn
[ 89%] Built target OSDI2023_MULTI_NODES_gcnii
[ 92%] Built target OSDI2023_MULTI_NODES_graphsage
[ 92%] Built target OSDI2023_MULTI_NODES_gcn
[ 92%] Built target estimate_comm_volume
Running experiments...
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 0 on machine gnerv7
DONE MPI INIT
Initialized node 1 on machine gnerv7
DONE MPI INIT
Initialized node 3 on machine gnerv7
DONE MPI INIT
Initialized node 2 on machine gnerv7
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.000 seconds.
Building the CSC structure...
        It takes 0.000 seconds.
Building the CSC structure...
        It takes 0.000 seconds.
Building the CSC structure...
        It takes 0.000 seconds.
Building the CSC structure...
        It takes 0.000 seconds.
        It takes 0.000 seconds.
        It takes 0.000 seconds.
        It takes 0.000 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.008 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/cora/4_parts
The number of GCN layers: 2
The number of hidden units: 100
The number of training epoches: 1000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 7
Number of feature dimensions: 1433
Number of vertices: 2708
Number of GPUs: 4
        It takes 0.008 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.009 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
        It takes 0.009 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
GPU 0, layer [0, 2)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
2708, 13264, 13264
Number of vertices per chunk: 677
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 2)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
2708, 13264, 13264
Number of vertices per chunk: 677
csr in-out ready !Start Cost Model Initialization...
train nodes 140, valid nodes 500, test nodes 1000
GPU 0, layer [0, 2)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 4): 0-[0, 677) 1-[677, 1354) 2-[1354, 2031) 3-[2031, 2708)
2708, 13264, 13264
Number of vertices per chunk: 677
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 2)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
2708, 13264, 13264
Number of vertices per chunk: 677
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 115.611 Gbps (per GPU), 462.443 Gbps (aggregated)
The layer-level communication performance: 115.452 Gbps (per GPU), 461.807 Gbps (aggregated)
The layer-level communication performance: 115.283 Gbps (per GPU), 461.131 Gbps (aggregated)
The layer-level communication performance: 115.280 Gbps (per GPU), 461.122 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.119 Gbps (per GPU), 636.477 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.083 Gbps (per GPU), 636.332 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.107 Gbps (per GPU), 636.429 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.083 Gbps (per GPU), 636.332 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 105.057 Gbps (per GPU), 420.229 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.060 Gbps (per GPU), 420.239 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.061 Gbps (per GPU), 420.243 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 105.061 Gbps (per GPU), 420.243 Gbps (aggregated, cluster-wide)
 LType   LT0   LT2 Ratio  VSum  ESum
 chk_0  0.25ms  0.14ms  1.79  0.68K  0.00M
 chk_1  0.25ms  0.14ms  1.76  0.68K  0.00M
 chk_2  0.25ms  0.14ms  1.79  0.68K  0.00M
 chk_3  0.25ms  0.14ms  1.75  0.68K  0.00M
   Avg  0.25  0.14
   Max  0.25  0.14
   Min  0.25  0.14
 Ratio  1.01  1.02
   Var  0.00  0.00
Profiling takes 0.070 s
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 10)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 677
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 10)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 677, Num Local Vertices: 677
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 10)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 1354, Num Local Vertices: 677
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 10)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 2031, Num Local Vertices: 677
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 10)...
+++++++++ Node 1 initializing the weights for op[0, 10)...
+++++++++ Node 2 initializing the weights for op[0, 10)...
+++++++++ Node 3 initializing the weights for op[0, 10)...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 465
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.9458	TrainAcc 0.2571	ValidAcc 0.2120	TestAcc 0.2290	BestValid 0.2120
	Epoch 50:	Loss 1.5541	TrainAcc 0.9857	ValidAcc 0.7700	TestAcc 0.7840	BestValid 0.7700
	Epoch 100:	Loss 1.0535	TrainAcc 0.9857	ValidAcc 0.7860	TestAcc 0.7990	BestValid 0.7860
	Epoch 150:	Loss 0.6849	TrainAcc 0.9857	ValidAcc 0.7940	TestAcc 0.8160	BestValid 0.7940
	Epoch 200:	Loss 0.4727	TrainAcc 0.9857	ValidAcc 0.7880	TestAcc 0.8140	BestValid 0.7940
	Epoch 250:	Loss 0.3610	TrainAcc 0.9857	ValidAcc 0.7960	TestAcc 0.8150	BestValid 0.7960
	Epoch 300:	Loss 0.2611	TrainAcc 0.9857	ValidAcc 0.7920	TestAcc 0.8160	BestValid 0.7960
	Epoch 350:	Loss 0.2170	TrainAcc 1.0000	ValidAcc 0.7940	TestAcc 0.8140	BestValid 0.7960
	Epoch 400:	Loss 0.1808	TrainAcc 1.0000	ValidAcc 0.7900	TestAcc 0.8120	BestValid 0.7960
	Epoch 450:	Loss 0.1471	TrainAcc 1.0000	ValidAcc 0.7840	TestAcc 0.8090	BestValid 0.7960
	Epoch 500:	Loss 0.1263	TrainAcc 1.0000	ValidAcc 0.7840	TestAcc 0.8080	BestValid 0.7960
	Epoch 550:	Loss 0.1089	TrainAcc 1.0000	ValidAcc 0.7840	TestAcc 0.8090	BestValid 0.7960
	Epoch 600:	Loss 0.0965	TrainAcc 1.0000	ValidAcc 0.7840	TestAcc 0.8050	BestValid 0.7960
	Epoch 650:	Loss 0.0835	TrainAcc 1.0000	ValidAcc 0.7820	TestAcc 0.8070	BestValid 0.7960
	Epoch 700:	Loss 0.0746	TrainAcc 1.0000	ValidAcc 0.7840	TestAcc 0.8060	BestValid 0.7960
	Epoch 750:	Loss 0.0683	TrainAcc 1.0000	ValidAcc 0.7840	TestAcc 0.8060	BestValid 0.7960
	Epoch 800:	Loss 0.0616	TrainAcc 1.0000	ValidAcc 0.7820	TestAcc 0.8060	BestValid 0.7960
	Epoch 850:	Loss 0.0516	TrainAcc 1.0000	ValidAcc 0.7820	TestAcc 0.8060	BestValid 0.7960
	Epoch 900:	Loss 0.0495	TrainAcc 1.0000	ValidAcc 0.7820	TestAcc 0.8050	BestValid 0.7960
	Epoch 950:	Loss 0.0437	TrainAcc 1.0000	ValidAcc 0.7820	TestAcc 0.8050	BestValid 0.7960
	Epoch 1000:	Loss 0.0421	TrainAcc 1.0000	ValidAcc 0.7820	TestAcc 0.8040	BestValid 0.7960
****** Epoch Time (Excluding Evaluation Cost): 0.001 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.036 ms (Max: 0.044, Min: 0.032, Sum: 0.144)
Cluster-Wide Average, Compute: 0.556 ms (Max: 0.563, Min: 0.542, Sum: 2.225)
Cluster-Wide Average, Communication-Layer: 0.008 ms (Max: 0.009, Min: 0.007, Sum: 0.034)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.017, Min: 0.013, Sum: 0.061)
Cluster-Wide Average, Communication-Graph: 0.389 ms (Max: 0.412, Min: 0.377, Sum: 1.556)
Cluster-Wide Average, Optimization: 0.274 ms (Max: 0.275, Min: 0.272, Sum: 1.097)
Cluster-Wide Average, Others: 0.098 ms (Max: 0.101, Min: 0.091, Sum: 0.391)
****** Breakdown Sum: 1.377 ms ******
Cluster-Wide Average, GPU Memory Consumption: 1.069 GB (Max: 1.108, Min: 1.030, Sum: 4.276)
Cluster-Wide Average, Graph-Level Communication Throughput: 5.603 Gbps (Max: 6.431, Min: 3.741, Sum: 22.410)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 0.001 GB
Weight-sync communication (cluster-wide, per-epoch): 0.003 GB
Total communication (cluster-wide, per-epoch): 0.004 GB
****** Accuracy Results ******
Highest valid_acc: 0.7960
Target test_acc: 0.8150
Epoch to reach the target acc: 249
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
