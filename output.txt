g000.anvil.rcac.purdue.edu
Wed Jan 18 01:25:18 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:81:00.0 Off |                    0 |
| N/A   37C    P0    52W / 400W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2    9) numactl/2.0.14
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0    10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0   11) openmpi/4.0.6
  4) gmp/6.2.1              8) zlib/1.2.11  12) boost/1.74.0

 

[  4%] Built target context
[ 17%] Built target parallel
[ 19%] Built target core
[ 39%] Built target cudahelp
[ 54%] Built target test_hello_world
[ 62%] Built target test_cuda
[ 54%] Built target test_cuda_data_compression
[ 73%] Built target test_mpi_gpu_model_parallel
[ 73%] Built target test_mpi_gpu_hybrid
[ 73%] Built target test_full_structual_graph
[ 73%] Built target test_mpi_pipelined_model_parallel
[ 73%] Built target test_trivial
[ 73%] Built target test_mpi_structual_graph
[ 73%] Built target test_mpi_combined
[ 73%] Built target test_mpi_gpu_pipelined_model_parallel
[ 87%] Built target test_single_node_gpu_training
[ 73%] Built target test_cuda_graph
[ 89%] Built target test_mpi_model_parallel
[ 87%] Built target test_nccl_mpi
[ 87%] Built target test_single_node_fullgpu_training
[ 87%] Built target test_full_non_structual_graph
[ 87%] Built target test_single_node_training
[ 87%] Built target test_mpi_non_structual_graph
[ 87%] Built target test_graph
[ 91%] Built target test_nccl_thread
[ 91%] Built target test_cuda_model_parallel
[ 91%] Built target estimate_comm_volume
[ 91%] Built target test_cuda_pipeline_parallel
[ 91%] Built target test_mpi_loader
[ 95%] Built target OSDI2023_MULTI_NODES_gcn
[100%] Built target OSDI2023_SINGLE_NODE_gcn
[100%] Built target test_two_layer_hybrid_parallelism_designer
[100%] Built target OSDI2023_SINGLE_NODE_gcn_inference
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
Initialized node 1 on machine g001.anvil.rcac.purdue.edu
Initialized node 0 on machine g000.anvil.rcac.purdue.edu
Initialized node 3 on machine g003.anvil.rcac.purdue.edu
Initialized node 2 on machine g002.anvil.rcac.purdue.edu
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.886 seconds.
Building the CSC structure...
        It takes 1.887 seconds.
Building the CSC structure...
        It takes 1.893 seconds.
Building the CSC structure...
        It takes 1.942 seconds.
Building the CSC structure...
        It takes 1.851 seconds.
        It takes 1.857 seconds.
        It takes 1.861 seconds.
        It takes 1.916 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.560 seconds.
Building the Label Vector...
        It takes 0.576 seconds.
Building the Label Vector...
        It takes 0.579 seconds.
Building the Label Vector...
        It takes 0.592 seconds.
Building the Label Vector...
        It takes 0.310 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
        It takes 0.310 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
        It takes 0.316 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
        It takes 0.320 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
*** Node 0, starting model training...
Number of operators: 20
0 2449029 0 6
0 2449029 6 11
0 2449029 11 16
0 2449029 16 20
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the partition [0, 6) x [0, 2449029)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_WEIGHT, output tensors: 1
    Op 2: type OPERATOR_MATMUL, output tensors: 2
    Op 3: type OPERATOR_AGGREGATION, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_WEIGHT, output tensors: 6
    Op 7: type OPERATOR_MATMUL, output tensors: 7
    Op 8: type OPERATOR_AGGREGATION, output tensors: 8
    Op 9: type OPERATOR_RELU, output tensors: 9
    Op 10: type OPERATOR_DROPOUT, output tensors: 10
    Op 11: type OPERATOR_WEIGHT, output tensors: 11
    Op 12: type OPERATOR_MATMUL, output tensors: 12
    Op 13: type OPERATOR_AGGREGATION, output tensors: 13
    Op 14: type OPERATOR_RELU, output tensors: 14
    Op 15: type OPERATOR_DROPOUT, output tensors: 15
    Op 16: type OPERATOR_WEIGHT, output tensors: 16
    Op 17: type OPERATOR_MATMUL, output tensors: 17
    Op 18: type OPERATOR_AGGREGATION, output tensors: 18
    Op 19: type OPERATOR_SOFTMAX, output tensors: 19
Boundaries: 0 0 0 0 2449029 2449029 2449029 2449029
Fragments: [0, 2449029)
Chunks (number of global chunks: 64): 0-[0, 38267) 1-[38267, 76534) 2-[76534, 114801) 3-[114801, 153068) 4-[153068, 191335) 5-[191335, 229602) 6-[229602, 267869) 7-[267869, 306136) 8-[306136, 344403) ... 63-[2410821, 2449029)
(Forwarding) Node 0 (fragment 0) depends on nodes:
(Backwarding) Node 0 (fragment 0) depends on nodes: 1 (Tensor: 5)
(I-link dependencies): node 0 should send activation to nodes:
(I-link dependencies): node 0 should receive activation from nodes:
(I-link dependencies): node 0 should send gradient to nodes:
(I-link dependencies): node 0 should receive gradient from nodes:
train nodes 196615, valid nodes 39323, test nodes 2213091
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
*** Node 1, starting model training...
Number of operators: 20
0 2449029 0 6
0 2449029 6 11
0 2449029 11 16
0 2449029 16 20
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the partition [6, 11) x [0, 2449029)
*** Node 1, constructing the helper classes...
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
*** Node 2, starting model training...
Number of operators: 20
0 2449029 0 6
0 2449029 6 11
0 2449029 11 16
0 2449029 16 20
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the partition [11, 16) x [0, 2449029)
*** Node 2, constructing the helper classes...
(Forwarding) Node 1 (fragment 0) depends on nodes: 0 (Tensor: 5)
(Backwarding) Node 1 (fragment 0) depends on nodes: 2 (Tensor: 10)
(I-link dependencies): node 1 should send activation to nodes:
(I-link dependencies): node 1 should receive activation from nodes:
(I-link dependencies): node 1 should send gradient to nodes:
(I-link dependencies): node 1 should receive gradient from nodes:
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
*** Node 3, starting model training...
Number of operators: 20
0 2449029 0 6
0 2449029 6 11
0 2449029 11 16
0 2449029 16 20
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the partition [16, 20) x [0, 2449029)
*** Node 3, constructing the helper classes...
(Forwarding) Node 3 (fragment 0) depends on nodes: 2 (Tensor: 15)
(Backwarding) Node 3 (fragment 0) depends on nodes:
(I-link dependencies): node 3 should send activation to nodes:
(I-link dependencies): node 3 should receive activation from nodes:
(I-link dependencies): node 3 should send gradient to nodes:
(I-link dependencies): node 3 should receive gradient from nodes:
(Forwarding) Node 2 (fragment 0) depends on nodes: 1 (Tensor: 10)
(Backwarding) Node 2 (fragment 0) depends on nodes: 3 (Tensor: 15)
(I-link dependencies): node 2 should send activation to nodes:
(I-link dependencies): node 2 should receive activation from nodes:
(I-link dependencies): node 2 should send gradient to nodes:
(I-link dependencies): node 2 should receive gradient from nodes:
2449029, 126167053, 126167053
2449029, 126167053, 126167053
2449029, 126167053, 126167053
2449029, 126167053, 126167053
csr in-out ready !*** Node 0, setting up some other necessary information...
csr in-out ready !*** Node 2, setting up some other necessary information...
csr in-out ready !*** Node 1, setting up some other necessary information...
csr in-out ready !*** Node 3, setting up some other necessary information...
*** Node 0, starting the helper threads...
*** Node 2, starting the helper threads...
*** Node 1, starting the helper threads...
*** Node 3, starting the helper threads...
+++++++++ Node 0 initializing the weights for op[0, 6)...
+++++++++ Node 0, mapping weight op 1
+++++++++ Node 3 initializing the weights for op[16, 20)...
+++++++++ Node 3, mapping weight op 16
+++++++++ Node 1 initializing the weights for op[6, 11)...
+++++++++ Node 1, mapping weight op 6
+++++++++ Node 2 initializing the weights for op[11, 16)...
+++++++++ Node 2, mapping weight op 11
RANDOMLY DISPATCH THE CHUNKS...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
    Epoch 9:	Loss 56.27461	TrainAcc 0.0000	ValidAcc 0.0000	TestAcc 0.0147
    Epoch 19:	Loss 56.73465	TrainAcc 0.0000	ValidAcc 0.0000	TestAcc 0.0147
    Epoch 29:	Loss 56.73465	TrainAcc 0.0000	ValidAcc 0.0000	TestAcc 0.0147
    Epoch 39:	Loss 56.73465	TrainAcc 0.0000	ValidAcc 0.0000	TestAcc 0.0147
    Epoch 49:	Loss 56.73465	TrainAcc 0.0000	ValidAcc 0.0000	TestAcc 0.0147
    Epoch 59:	Loss 56.73465	TrainAcc 0.0000	ValidAcc 0.0000	TestAcc 0.0147
    Epoch 69:	Loss 56.73465	TrainAcc 0.0000	ValidAcc 0.0000	TestAcc 0.0147
    Epoch 79:	Loss 56.73465	TrainAcc 0.0000	ValidAcc 0.0000	TestAcc 0.0147
    Epoch 89:	Loss 56.73465	TrainAcc 0.0000	ValidAcc 0.0000	TestAcc 0.0147
Node 0, Layer-level comm throughput (act): -nan GBps
Node 1, Layer-level comm throughput (act): 10.851 GBps
Node 2, Layer-level comm throughput (act): 10.722 GBps
Node 3, Layer-level comm throughput (act): 10.963 GBps
    Epoch 99:	Loss 56.73465	TrainAcc 0.0000	ValidAcc 0.0000	TestAcc 0.0147
Node 0, compression time: 0.589s, compression size: 116.779GB, throughput: 198.275GBps
Node 0, decompression time: 0.000s, compression size: 116.779GB, throughput: -nanGBps
Node 0, pure compute time: 4.774 s, total compute time: 5.363 s
Node 0, wait_for_task_time: 0.002 s, wait_for_other_gpus_time: 0.000 s
------------------------node id 0,  per-epoch time: 0.067951 s---------------
Node 3, compression time: 0.000s, compression size: 0.000GB, throughput: -nanGBps
Node 3, decompression time: 0.141s, compression size: 0.000GB, throughput: 827.852GBps
Node 3, pure compute time: 4.593 s, total compute time: 4.734 s
Node 0, wait_for_task_time: 2.226 s, wait_for_other_gpus_time: 0.001 s
Node 2, compression time: 0.643s, compression size: 116.779GB, throughput: 181.579GBps
Node 2, decompression time: 0.128s, compression size: 116.779GB, throughput: 909.791GBps
Node 2, pure compute time: 5.319 s, total compute time: 6.091 s
Node 0, wait_for_task_time: 0.669 s, wait_for_other_gpus_time: 0.001 s
------------------------node id 2,  per-epoch time: 0.067950 s---------------
Node 1, compression time: 0.580s, compression size: 116.779GB, throughput: 201.490GBps
Node 1, decompression time: 0.135s, compression size: 116.779GB, throughput: 862.409GBps
Node 1, pure compute time: 5.375 s, total compute time: 6.089 s
Node 0, wait_for_task_time: 0.232 s, wait_for_other_gpus_time: 0.001 s
------------------------node id 3,  per-epoch time: 0.067951 s---------------
------------------------node id 1,  per-epoch time: 0.067950 s---------------
************ Profiling Results ************
	Bubble: 0.733176 (s) (10.67 percentage)
	Compute: 5.029996 (s) (73.18 percentage)
	GradSync: 0.038327 (s) (0.56 percentage)
	GraphComm: 0.004876 (s) (0.07 percentage)
	Imbalance: 0.647294 (s) (9.42 percentage)
	LayerComm: 0.419922 (s) (6.11 percentage)
	Layer-level communication (cluster-wide, per epoch): 1.041 GB
Highest valid_acc: 0.0000
Target test_acc: 0.0000
Epoch to reach the target acc: 0
[MPI Rank 0] Success 
[MPI Rank 3] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
