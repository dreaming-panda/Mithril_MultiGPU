g000.anvil.rcac.purdue.edu
Thu Jan 26 02:12:21 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:01:00.0 Off |                    0 |
| N/A   33C    P0    53W / 400W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2    9) numactl/2.0.14
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0    10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0   11) openmpi/4.0.6
  4) gmp/6.2.1              8) zlib/1.2.11  12) boost/1.74.0

 

[  4%] Built target context
[ 18%] Built target parallel
[ 19%] Built target core
[ 39%] Built target cudahelp
[ 45%] Built target test_mpi_gpu_model_parallel
[ 58%] Built target test_cuda_model_parallel
[ 58%] Built target test_cuda_data_compression
[ 58%] Built target test_cuda_graph
[ 58%] Built target test_cuda
[ 71%] Built target test_trivial
[ 78%] Built target test_hello_world
[ 71%] Built target test_mpi_combined
[ 71%] Built target test_mpi_structual_graph
[ 71%] Built target test_cuda_pipeline_parallel
[ 71%] Built target test_mpi_gpu_pipelined_model_parallel
[ 85%] Built target test_nccl_thread
[ 85%] Built target test_mpi_pipelined_model_parallel
[ 85%] Built target test_full_structual_graph
[ 90%] Built target test_nccl_mpi
[ 85%] Built target test_mpi_gpu_hybrid
[ 85%] Built target test_full_non_structual_graph
[ 85%] Built target test_graph
[ 85%] Built target test_mpi_non_structual_graph
[ 85%] Built target test_mpi_loader
[ 90%] Built target test_single_node_gpu_training
[ 90%] Built target test_single_node_fullgpu_training
[ 90%] Built target estimate_comm_volume
[ 91%] Built target test_single_node_training
[ 91%] Built target test_mpi_model_parallel
[ 97%] Built target OSDI2023_MULTI_NODES_gcn
[ 97%] Built target OSDI2023_SINGLE_NODE_gcn_inference
[ 97%] Built target OSDI2023_SINGLE_NODE_gcn
[100%] Built target test_two_layer_hybrid_parallelism_designer
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_arxiv
The number of GCN layers: 3
The number of hidden units: 128
The number of training epoches: 200
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: saved_weights_pipe
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_arxiv
The number of GCN layers: 3
The number of hidden units: 128
The number of training epoches: 200
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: saved_weights_pipe
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_arxiv
The number of GCN layers: 3
The number of hidden units: 128
The number of training epoches: 200
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: saved_weights_pipe
Initialized node 0 on machine g000.anvil.rcac.purdue.edu
Initialized node 1 on machine g002.anvil.rcac.purdue.edu
Initialized node 2 on machine g007.anvil.rcac.purdue.edu
Building the CSR structure...
Building the CSR structure...
        It takes 0.037 seconds.
Building the CSC structure...
Building the CSR structure...
        It takes 0.037 seconds.
Building the CSC structure...
        It takes 0.037 seconds.
        It takes 0.038 seconds.
Building the CSC structure...
        It takes 0.037 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.038 seconds.
Building the Feature Vector...
        It takes 0.050 seconds.
Building the Label Vector...
        It takes 0.020 seconds.
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
        It takes 0.050 seconds.
Building the Label Vector...
        It takes 0.020 seconds.
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
        It takes 0.053 seconds.
Building the Label Vector...
        It takes 0.021 seconds.
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
train nodes 90941, valid nodes 29799, test nodes 48603
Number of GPUs: 3
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
*** Node 0, starting model training...
Number of operators: 15
0 169343 0 6
0 169343 6 11
0 169343 11 15
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the partition [0, 6) x [0, 169343)
*** Node 0, constructing the helper classes...
WARNING: the current version only applies to linear GNN models!
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_WEIGHT, output tensors: 1
    Op 2: type OPERATOR_MATMUL, output tensors: 2
    Op 3: type OPERATOR_AGGREGATION, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_WEIGHT, output tensors: 6
    Op 7: type OPERATOR_MATMUL, output tensors: 7
    Op 8: type OPERATOR_AGGREGATION, output tensors: 8
    Op 9: type OPERATOR_RELU, output tensors: 9
    Op 10: type OPERATOR_DROPOUT, output tensors: 10
    Op 11: type OPERATOR_WEIGHT, output tensors: 11
    Op 12: type OPERATOR_MATMUL, output tensors: 12
    Op 13: type OPERATOR_AGGREGATION, output tensors: 13
    Op 14: type OPERATOR_SOFTMAX, output tensors: 14
Boundaries: 0 0 0 169343 169343 169343
Fragments: [0, 169343)
Chunks (number of global chunks: 16): 0-[0, 10584) 1-[10584, 21168) 2-[21168, 31752) 3-[31752, 42336) 4-[42336, 52920) 5-[52920, 63504) 6-[63504, 74088) 7-[74088, 84672) 8-[84672, 95256) ... 15-[158760, 169343)
(Forwarding) Node 0 (fragment 0) depends on nodes:
(Backwarding) Node 0 (fragment 0) depends on nodes: 1 (Tensor: 5)
(I-link dependencies): node 0 should send activation to nodes:
(I-link dependencies): node 0 should receive activation from nodes:
(I-link dependencies): node 0 should send gradient to nodes:
(I-link dependencies): node 0 should receive gradient from nodes:
169343, 2484941, 2484941
train nodes 90941, valid nodes 29799, test nodes 48603
Number of GPUs: 3
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
*** Node 1, starting model training...
Number of operators: 15
0 169343 0 6
0 169343 6 11
0 169343 11 15
WARNING: the current version only applies to linear GNN models!
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the partition [6, 11) x [0, 169343)
*** Node 1, constructing the helper classes...
(Forwarding) Node 1 (fragment 0) depends on nodes: 0 (Tensor: 5)
(Backwarding) Node 1 (fragment 0) depends on nodes: 2 (Tensor: 10)
(I-link dependencies): node 1 should send activation to nodes:
(I-link dependencies): node 1 should receive activation from nodes:
(I-link dependencies): node 1 should send gradient to nodes:
(I-link dependencies): node 1 should receive gradient from nodes:
train nodes 90941, valid nodes 29799, test nodes 48603
169343, 2484941, 2484941
Number of GPUs: 3
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
*** Node 2, starting model training...
Number of operators: 15
WARNING: the current version only applies to linear GNN models!
0 169343 0 6
0 169343 6 11
0 169343 11 15
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the partition [11, 15) x [0, 169343)
*** Node 2, constructing the helper classes...
(Forwarding) Node 2 (fragment 0) depends on nodes: 1 (Tensor: 10)
(Backwarding) Node 2 (fragment 0) depends on nodes:
(I-link dependencies): node 2 should send activation to nodes:
(I-link dependencies): node 2 should receive activation from nodes:
(I-link dependencies): node 2 should send gradient to nodes:
(I-link dependencies): node 2 should receive gradient from nodes:
169343, 2484941, 2484941
csr in-out ready !*** Node 0, setting up some other necessary information...
csr in-out ready !*** Node 2, setting up some other necessary information...
csr in-out ready !*** Node 1, setting up some other necessary information...
*** Node 0, starting the helper threads...
*** Node 1, starting the helper threads...
*** Node 2, starting the helper threads...
+++++++++ Node 0 initializing the weights for op[0, 6)...
+++++++++ Node 0, mapping weight op 1
+++++++++ Node 1 initializing the weights for op[6, 11)...
+++++++++ Node 1, mapping weight op 6
+++++++++ Node 2 initializing the weights for op[11, 15)...
+++++++++ Node 2, mapping weight op 11
RANDOMLY DISPATCH THE CHUNKS...
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
    Epoch 9:	Loss 3.50321	TrainAcc 0.1792	ValidAcc 0.0766	TestAcc 0.0588
    Epoch 19:	Loss 3.10888	TrainAcc 0.2448	ValidAcc 0.2413	TestAcc 0.2714
    Epoch 29:	Loss 2.88223	TrainAcc 0.2978	ValidAcc 0.3114	TestAcc 0.2793
    Epoch 39:	Loss 2.63316	TrainAcc 0.3947	ValidAcc 0.4382	TestAcc 0.4522
    Epoch 49:	Loss 2.39804	TrainAcc 0.4395	ValidAcc 0.4893	TestAcc 0.4954
    Epoch 59:	Loss 2.20756	TrainAcc 0.4421	ValidAcc 0.4409	TestAcc 0.4093
    Epoch 69:	Loss 2.06448	TrainAcc 0.4701	ValidAcc 0.5005	TestAcc 0.5038
    Epoch 79:	Loss 1.95413	TrainAcc 0.5003	ValidAcc 0.5259	TestAcc 0.5196
    Epoch 89:	Loss 1.87234	TrainAcc 0.5112	ValidAcc 0.5297	TestAcc 0.5200
    Epoch 99:	Loss 1.80635	TrainAcc 0.5213	ValidAcc 0.5526	TestAcc 0.5587
    Epoch 109:	Loss 1.75470	TrainAcc 0.5275	ValidAcc 0.5396	TestAcc 0.5241
    Epoch 119:	Loss 1.70947	TrainAcc 0.5478	ValidAcc 0.5703	TestAcc 0.5662
    Epoch 129:	Loss 1.67337	TrainAcc 0.5531	ValidAcc 0.5752	TestAcc 0.5725
    Epoch 139:	Loss 1.63990	TrainAcc 0.5599	ValidAcc 0.5729	TestAcc 0.5600
    Epoch 149:	Loss 1.61125	TrainAcc 0.5669	ValidAcc 0.5754	TestAcc 0.5646
    Epoch 159:	Loss 1.58359	TrainAcc 0.5738	ValidAcc 0.5968	TestAcc 0.5989
    Epoch 169:	Loss 1.55810	TrainAcc 0.5777	ValidAcc 0.5923	TestAcc 0.5869
    Epoch 179:	Loss 1.53594	TrainAcc 0.5832	ValidAcc 0.5953	TestAcc 0.5871
    Epoch 189:	Loss 1.51505	TrainAcc 0.5876	ValidAcc 0.6036	TestAcc 0.6016
Node 0, Layer-level comm throughput (act): -nan GBps
Node 1, Layer-level comm throughput (act): 10.187 GBps
Node 2, Layer-level comm throughput (act): 10.026 GBps
Node 2, Layer-level comm throughput (grad): -nan GBps
Node 1, Layer-level comm throughput (grad): 10.204 GBps
Node 0, Layer-level comm throughput (grad): 10.070 GBps
    Epoch 199:	Loss 1.49779	TrainAcc 0.5892	ValidAcc 0.6042	TestAcc 0.5951
Node 0, compression time: 0.190s, compression size: 16.150GB, throughput: 85.217GBps
Node 0, decompression time: 0.357s, compression size: 16.150GB, throughput: 45.228GBps
Node 0, pure compute time: 1.405 s, total compute time: 1.952 s
Node 0, wait_for_task_time: 1.497 s, wait_for_other_gpus_time: 0.001 s
------------------------node id 0,  per-epoch time: 0.017710 s---------------
Node 2, compression time: 0.228s, compression size: 16.150GB, throughput: 70.799GBps
Node 2, decompression time: 0.358s, compression size: 16.150GB, throughput: 45.138GBps
Node 2, pure compute time: 1.577 s, total compute time: 2.163 s
Node 2, wait_for_task_time: 0.269 s, wait_for_other_gpus_time: 0.001 s
------------------------node id 2,  per-epoch time: 0.017710 s---------------
Node 1, compression time: 0.427s, compression size: 32.300GB, throughput: 75.559GBps
Node 1, decompression time: 1.021s, compression size: 32.300GB, throughput: 31.627GBps
Node 1, pure compute time: 1.205 s, total compute time: 2.654 s
Node 1, wait_for_task_time: 0.576 s, wait_for_other_gpus_time: 0.001 s
------------------------node id 1,  per-epoch time: 0.017710 s---------------
ERROR: undercount the overhead: breakdown sum / all time: 0.589
************ Profiling Results ************
	Bubble: 0.225537 (s) (10.15 percentage)
	Compute: 1.398766 (s) (62.95 percentage)
	GradSync: 0.049899 (s) (2.25 percentage)
	GraphComm: 0.005299 (s) (0.24 percentage)
	Imbalance: 0.346172 (s) (15.58 percentage)
	LayerComm: 0.196258 (s) (8.83 percentage)
	Layer-level communication (cluster-wide, per epoch): 0.129 GB
Highest valid_acc: 0.6042
Target test_acc: 0.5951
Epoch to reach the target acc: 200
ERROR: undercount the overhead: breakdown sum / all time: 0.589
ERROR: undercount the overhead: breakdown sum / all time: 0.589
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 0] Success 
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_arxiv
The number of GCN layers: 3
The number of hidden units: 128
The number of training epoches: 0
Learning rate: 0.000000
Initialized node g000.anvil.rcac.purdue.edu
Building the CSR structure...
        It takes 0.038 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the Feature Vector...
        It takes 0.052 seconds.
Building the Label Vector...
        It takes 0.020 seconds.
Number of classes: 40
Number of feature dimensions: 128
Dropout: 0.000 
train nodes 90941, valid nodes 29799, test nodes 48603
*** Allocating resources for all tensors...
    OP_TYPE: OPERATOR_INPUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_SOFTMAX
*** Done allocating resource.
*** Preparing the input tensor...
*** Done preparing the input tensor.
*** Preparing the STD tensor...
    Number of labels: 40
    Number of vertices: 169343
*** Done preparing the STD tensor.
Version 0	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586
Version 1	TrainAcc 0.2211	ValidAcc 0.1744	TestAcc 0.2040
Version 2	TrainAcc 0.3013	ValidAcc 0.3129	TestAcc 0.2798
Version 3	TrainAcc 0.4159	ValidAcc 0.4174	TestAcc 0.3950
Version 4	TrainAcc 0.4738	ValidAcc 0.5355	TestAcc 0.5547
Version 5	TrainAcc 0.4757	ValidAcc 0.4785	TestAcc 0.4470
Version 6	TrainAcc 0.5026	ValidAcc 0.5185	TestAcc 0.5054
Version 7	TrainAcc 0.5303	ValidAcc 0.5538	TestAcc 0.5476
Version 8	TrainAcc 0.5462	ValidAcc 0.5554	TestAcc 0.5401
Version 9	TrainAcc 0.5613	ValidAcc 0.5885	TestAcc 0.5958
Version 10	TrainAcc 0.5587	ValidAcc 0.5672	TestAcc 0.5565
Version 11	TrainAcc 0.5728	ValidAcc 0.5775	TestAcc 0.5662
Version 12	TrainAcc 0.5907	ValidAcc 0.6123	TestAcc 0.6129
Version 13	TrainAcc 0.5936	ValidAcc 0.6009	TestAcc 0.5890
Version 14	TrainAcc 0.5992	ValidAcc 0.6002	TestAcc 0.5836
Version 15	TrainAcc 0.6034	ValidAcc 0.6196	TestAcc 0.6185
Version 16	TrainAcc 0.6094	ValidAcc 0.6224	TestAcc 0.6138
Version 17	TrainAcc 0.6121	ValidAcc 0.6189	TestAcc 0.6031
Version 18	TrainAcc 0.6179	ValidAcc 0.6327	TestAcc 0.6247
Version 19	TrainAcc 0.6181	ValidAcc 0.6287	TestAcc 0.6212
Version 18 achieved the highest validation accuracy 0.6327 (test accuracy: 0.6247)
