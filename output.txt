g009.anvil.rcac.purdue.edu
Tue Jan 17 13:48:23 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:41:00.0 Off |                    0 |
| N/A   28C    P0    48W / 400W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2    9) numactl/2.0.14
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0    10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0   11) openmpi/4.0.6
  4) gmp/6.2.1              8) zlib/1.2.11  12) boost/1.74.0

 

[  4%] Built target context
[ 13%] Built target parallel
[ 19%] Built target core
[ 20%] Building CXX object CMakeFiles/cudahelp.dir/core/src/cuda/cuda_hybrid_parallel.cc.o
[ 21%] Linking CXX static library libcudahelp.a
[ 39%] Built target cudahelp
[ 40%] Linking CXX executable test_hello_world
[ 46%] Linking CXX executable test_mpi_structual_graph
[ 46%] Linking CXX executable test_mpi_non_structual_graph
[ 46%] Linking CXX executable test_graph
[ 46%] Linking CXX executable test_mpi_loader
[ 53%] Linking CXX executable estimate_comm_volume
[ 53%] Linking CXX executable test_full_structual_graph
[ 53%] Linking CXX executable test_nccl_thread
[ 53%] Linking CXX executable test_cuda_graph
[ 53%] Linking CXX executable test_cuda
[ 46%] Linking CXX executable test_full_non_structual_graph
[ 53%] Linking CXX executable test_nccl_mpi
[ 53%] Linking CXX executable test_trivial
[ 55%] Linking CXX executable test_mpi_combined
[ 55%] Linking CXX executable test_cuda_data_compression
[ 58%] Linking CXX executable test_single_node_fullgpu_training
[ 60%] Linking CXX executable test_mpi_gpu_hybrid
[ 60%] Linking CXX executable test_mpi_gpu_model_parallel
[ 63%] Linking CXX executable test_single_node_gpu_training
[ 63%] Linking CXX executable test_mpi_model_parallel
[ 63%] Linking CXX executable test_cuda_model_parallel
[ 64%] Linking CXX executable test_cuda_pipeline_parallel
[ 64%] Linking CXX executable test_mpi_pipelined_model_parallel
[ 64%] Linking CXX executable test_mpi_gpu_pipelined_model_parallel
[ 65%] Linking CXX executable test_single_node_training
[ 66%] Linking CXX executable gcn
[ 69%] Linking CXX executable test_two_layer_hybrid_parallelism_designer
[ 69%] Linking CXX executable gcn
[ 69%] Linking CXX executable gcn_inference
[ 70%] Built target test_mpi_non_structual_graph
[ 71%] Built target test_full_non_structual_graph
[ 78%] Built target test_mpi_combined
[ 78%] Built target test_hello_world
[ 78%] Built target test_cuda_data_compression
[ 78%] Built target test_graph
[ 78%] Built target test_mpi_structual_graph
[ 78%] Built target test_full_structual_graph
[ 79%] Built target test_trivial
[ 84%] Built target test_mpi_model_parallel
[ 84%] Built target test_cuda_graph
[ 84%] Built target estimate_comm_volume
[ 84%] Built target test_single_node_training
[ 85%] Built target test_mpi_pipelined_model_parallel
[ 86%] Built target test_mpi_loader
[ 86%] Built target test_cuda
[ 89%] Built target test_two_layer_hybrid_parallelism_designer
[ 89%] Built target test_mpi_gpu_model_parallel
[ 90%] Built target test_single_node_gpu_training
[ 90%] Built target test_mpi_gpu_pipelined_model_parallel
[ 91%] Built target test_single_node_fullgpu_training
[ 92%] Built target test_mpi_gpu_hybrid
[ 93%] Built target test_cuda_pipeline_parallel
[ 95%] Built target OSDI2023_SINGLE_NODE_gcn_inference
[ 95%] Built target OSDI2023_SINGLE_NODE_gcn
[ 96%] Built target OSDI2023_MULTI_NODES_gcn
[ 97%] Built target test_nccl_thread
[ 98%] Built target test_nccl_mpi
[100%] Built target test_cuda_model_parallel
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 100
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
Initialized node 1 on machine g010.anvil.rcac.purdue.edu
Initialized node 3 on machine g012.anvil.rcac.purdue.edu
Initialized node 0 on machine g009.anvil.rcac.purdue.edu
Initialized node 2 on machine g011.anvil.rcac.purdue.edu
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.947 seconds.
Building the CSC structure...
        It takes 1.907 seconds.
Building the CSC structure...
        It takes 1.959 seconds.
Building the CSC structure...
        It takes 1.989 seconds.
Building the CSC structure...
        It takes 1.880 seconds.
        It takes 1.922 seconds.
        It takes 1.934 seconds.
        It takes 1.964 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.603 seconds.
Building the Label Vector...
        It takes 0.601 seconds.
Building the Label Vector...
        It takes 0.609 seconds.
Building the Label Vector...
        It takes 0.594 seconds.
Building the Label Vector...
        It takes 0.321 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
        It takes 0.323 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
        It takes 0.326 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
        It takes 0.318 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
*** Node 2, starting model training...
Number of operators: 20
0 2449029 0 6
0 2449029 6 11
0 2449029 11 16
0 2449029 16 20
WARNING: the current version only applies to linear GNN models!
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the partition [11, 16) x [0, 2449029)
*** Node 2, constructing the helper classes...
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
*** Node 0, starting model training...
Number of operators: 20
0 2449029 0 6
0 2449029 6 11
0 2449029 11 16
0 2449029 16 20
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the partition [0, 6) x [0, 2449029)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_WEIGHT, output tensors: 1
    Op 2: type OPERATOR_MATMUL, output tensors: 2
    Op 3: type OPERATOR_AGGREGATION, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_WEIGHT, output tensors: 6
    Op 7: type OPERATOR_MATMUL, output tensors: 7
    Op 8: type OPERATOR_AGGREGATION, output tensors: 8
    Op 9: type OPERATOR_RELU, output tensors: 9
    Op 10: type OPERATOR_DROPOUT, output tensors: 10
    Op 11: type OPERATOR_WEIGHT, output tensors: 11
    Op 12: type OPERATOR_MATMUL, output tensors: 12
    Op 13: type OPERATOR_AGGREGATION, output tensors: 13
    Op 14: type OPERATOR_RELU, output tensors: 14
    Op 15: type OPERATOR_DROPOUT, output tensors: 15
    Op 16: type OPERATOR_WEIGHT, output tensors: 16
    Op 17: type OPERATOR_MATMUL, output tensors: 17
    Op 18: type OPERATOR_AGGREGATION, output tensors: 18
    Op 19: type OPERATOR_SOFTMAX, output tensors: 19
(Forwarding) Node 2 (fragment 0) depends on nodes: 1 (Tensor: 10)
(Backwarding) Node 2 (fragment 0) depends on nodes: 3 (Tensor: 15)
(I-link dependencies): node 2 should send activation to nodes:
(I-link dependencies): node 2 should receive activation from nodes:
(I-link dependencies): node 2 should send gradient to nodes:
(I-link dependencies): node 2 should receive gradient from nodes:
Boundaries: 0 0 0 0 2449029 2449029 2449029 2449029
Fragments: [0, 2449029)
Chunks (number of global chunks: 64): 0-[0, 38267) 1-[38267, 76534) 2-[76534, 114801) 3-[114801, 153068) 4-[153068, 191335) 5-[191335, 229602) 6-[229602, 267869) 7-[267869, 306136) 8-[306136, 344403) ... 63-[2410821, 2449029)
(Forwarding) Node 0 (fragment 0) depends on nodes:
(Backwarding) Node 0 (fragment 0) depends on nodes: 1 (Tensor: 5)
(I-link dependencies): node 0 should send activation to nodes:
(I-link dependencies): node 0 should receive activation from nodes:
(I-link dependencies): node 0 should send gradient to nodes:
(I-link dependencies): node 0 should receive gradient from nodes:
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
*** Node 1, starting model training...
Number of operators: 20
0 2449029 0 6
0 2449029 6 11
0 2449029 11 16
0 2449029 16 20
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the partition [6, 11) x [0, 2449029)
*** Node 1, constructing the helper classes...
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
(Forwarding) Node 1 (fragment 0) depends on nodes: 0 (Tensor: 5)
(Backwarding) Node 1 (fragment 0) depends on nodes: 2 (Tensor: 10)
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
(I-link dependencies): node 1 should send activation to nodes:
(I-link dependencies): node 1 should receive activation from nodes:
WARNING: the current version only applies to linear GNN models!
*** Node 3, starting model training...
Number of operators: 20
0 2449029 0 6
0 2449029 6 11
0 2449029 11 16
0 2449029 16 20
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the partition [16, 20) x [0, 2449029)
(I-link dependencies): node 1 should send gradient to nodes:
(I-link dependencies): node 1 should receive gradient from nodes:
*** Node 3, constructing the helper classes...
(Forwarding) Node 3 (fragment 0) depends on nodes: 2 (Tensor: 15)
(Backwarding) Node 3 (fragment 0) depends on nodes:
(I-link dependencies): node 3 should send activation to nodes:
(I-link dependencies): node 3 should receive activation from nodes:
(I-link dependencies): node 3 should send gradient to nodes:
(I-link dependencies): node 3 should receive gradient from nodes:
2449029, 126167053, 126167053
2449029, 126167053, 126167053
2449029, 126167053, 126167053
2449029, 126167053, 126167053
csr in-out ready !*** Node 0, setting up some other necessary information...
csr in-out ready !*** Node 2, setting up some other necessary information...
csr in-out ready !*** Node 3, setting up some other necessary information...
csr in-out ready !*** Node 1, setting up some other necessary information...
Setting up the MPI window for chunk 0
Setting up the MPI window for chunk 1
Setting up the MPI window for chunk 2
Setting up the MPI window for chunk 3
Setting up the MPI window for chunk 4
Setting up the MPI window for chunk 5
Setting up the MPI window for chunk 6
Setting up the MPI window for chunk 7
Setting up the MPI window for chunk 8
Setting up the MPI window for chunk 9
Setting up the MPI window for chunk 10
Setting up the MPI window for chunk 11
Setting up the MPI window for chunk 12
Setting up the MPI window for chunk 13
Setting up the MPI window for chunk 14
Setting up the MPI window for chunk 15
Setting up the MPI window for chunk 16
Setting up the MPI window for chunk 17
Setting up the MPI window for chunk 18
Setting up the MPI window for chunk 19
Setting up the MPI window for chunk 20
Setting up the MPI window for chunk 21
Setting up the MPI window for chunk 22
Setting up the MPI window for chunk 23
Setting up the MPI window for chunk 24
Setting up the MPI window for chunk 25
Setting up the MPI window for chunk 26
Setting up the MPI window for chunk 27
Setting up the MPI window for chunk 28
Setting up the MPI window for chunk 29
Setting up the MPI window for chunk 30
Setting up the MPI window for chunk 31
Setting up the MPI window for chunk 32
Setting up the MPI window for chunk 33
Setting up the MPI window for chunk 34
Setting up the MPI window for chunk 35
Setting up the MPI window for chunk 36
Setting up the MPI window for chunk 37
Setting up the MPI window for chunk 38
Setting up the MPI window for chunk 39
Setting up the MPI window for chunk 40
Setting up the MPI window for chunk 41
Setting up the MPI window for chunk 42
Setting up the MPI window for chunk 43
Setting up the MPI window for chunk 44
Setting up the MPI window for chunk 45
Setting up the MPI window for chunk 46
Setting up the MPI window for chunk 47
Setting up the MPI window for chunk 48
Setting up the MPI window for chunk 49
Setting up the MPI window for chunk 50
Setting up the MPI window for chunk 51
Setting up the MPI window for chunk 52
Setting up the MPI window for chunk 53
Setting up the MPI window for chunk 54
Setting up the MPI window for chunk 55
Setting up the MPI window for chunk 56
Setting up the MPI window for chunk 57
Setting up the MPI window for chunk 58
Setting up the MPI window for chunk 59
Setting up the MPI window for chunk 60
Setting up the MPI window for chunk 61
Setting up the MPI window for chunk 62
Setting up the MPI window for chunk 63
*** Node 0, starting the helper threads...
*** Node 2, starting the helper threads...
*** Node 1, starting the helper threads...
*** Node 3, starting the helper threads...
+++++++++ Node 0 initializing the weights for op[0, 6)...
+++++++++ Node 0, mapping weight op 1
+++++++++ Node 2 initializing the weights for op[11, 16)...
+++++++++ Node 2, mapping weight op 11
+++++++++ Node 3 initializing the weights for op[16, 20)...
+++++++++ Node 3, mapping weight op 16
+++++++++ Node 1 initializing the weights for op[6, 11)...
+++++++++ Node 1, mapping weight op 6
RANDOMLY DISPATCH THE CHUNKS...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
    Epoch 9:	Loss 38.92037	TrainAcc 0.0061	ValidAcc 0.0065	TestAcc 0.0131
    Epoch 19:	Loss 38.94063	TrainAcc 0.0049	ValidAcc 0.0058	TestAcc 0.0125
    Epoch 29:	Loss 38.94045	TrainAcc 0.0056	ValidAcc 0.0066	TestAcc 0.0125
    Epoch 39:	Loss 38.94208	TrainAcc 0.0050	ValidAcc 0.0057	TestAcc 0.0127
    Epoch 49:	Loss 38.94443	TrainAcc 0.0054	ValidAcc 0.0058	TestAcc 0.0123
    Epoch 59:	Loss 38.94099	TrainAcc 0.0055	ValidAcc 0.0056	TestAcc 0.0132
    Epoch 69:	Loss 38.94112	TrainAcc 0.0052	ValidAcc 0.0061	TestAcc 0.0126
    Epoch 79:	Loss 38.94170	TrainAcc 0.0052	ValidAcc 0.0060	TestAcc 0.0127
    Epoch 89:	Loss 38.94293	TrainAcc 0.0053	ValidAcc 0.0062	TestAcc 0.0126
Node 0, Layer-level comm throughput (act): -nan GBps
Node 1, Layer-level comm throughput (act): 10.937 GBps
Node 2, Layer-level comm throughput (act): 10.804 GBps
Node 3, Layer-level comm throughput (act): 11.086 GBps
    Epoch 99:	Loss 38.94215	TrainAcc 0.0053	ValidAcc 0.0063	TestAcc 0.0124
Node 0, compression time: 0.590s, compression size: 116.779GB, throughput: 197.849GBps
Node 0, decompression time: 0.000s, compression size: 116.779GB, throughput: -nanGBps
Node 0, pure compute time: 4.591 s, total compute time: 5.181 s
Node 0, wait_for_task_time: 0.003 s, wait_for_other_gpus_time: 0.001 s
------------------------node id 0,  per-epoch time: 0.073769 s---------------
Node 2, compression time: 0.739s, compression size: 116.779GB, throughput: 158.015GBps
Node 2, decompression time: 2.284s, compression size: 116.779GB, throughput: 51.131GBps
Node 1, compression time: 0.601s, compression size: 116.779GB, throughput: 194.376GBps
Node 1, decompression time: 2.763s, compression size: 116.779GB, throughput: 42.273GBps
Node 1, pure compute time: 3.441 s, total compute time: 6.804 s
Node 0, wait_for_task_time: 0.173 s, wait_for_other_gpus_time: 0.001 s
------------------------node id 1,  per-epoch time: 0.073769 s---------------
Node 3, compression time: 0.000s, compression size: 0.000GB, throughput: -nanGBps
Node 3, decompression time: 0.782s, compression size: 0.000GB, throughput: 149.357GBps
Node 3, pure compute time: 4.652 s, total compute time: 5.434 s
Node 0, wait_for_task_time: 2.120 s, wait_for_other_gpus_time: 0.001 s
Node 2, pure compute time: 3.543 s, total compute time: 6.566 s
Node 0, wait_for_task_time: 0.793 s, wait_for_other_gpus_time: 0.001 s
------------------------node id 2,  per-epoch time: 0.073769 s---------------
------------------------node id 3,  per-epoch time: 0.073769 s---------------
************ Profiling Results ************
	Bubble: 1.513989 (s) (20.81 percentage)
	Compute: 4.071627 (s) (55.96 percentage)
	GradSync: 0.037658 (s) (0.52 percentage)
	GraphComm: 0.002538 (s) (0.03 percentage)
	Imbalance: 0.827548 (s) (11.37 percentage)
	LayerComm: 0.822668 (s) (11.31 percentage)
	Graph-level communication (cluster-wide, per epoch): 0.000 GB
	Layer-level communication (cluster-wide, per epoch): 0.990 GB
	Graph+Layer-level communication (cluster-wide, per epoch): 0.990 GB
	Parameter-server communication (cluster-wide, per epoch): 0.001 GB
	Graph-level dev2host communication time: 0.000 s, throughput: 0.000000 GBps
	Graph-level memcpy communication time: 0.000 s, throughput: 0.000000 GBps
	Graph-level net Activation communication time: 0.000 s, throughput: 0.000000 GBps
	Graph-level net Gradient communication time: 0.000 s, throughput: 0.000000 GBps
	Graph-level network batch size: -0.000 Bytes
Highest valid_acc: 0.0066
Target test_acc: 0.0125
Epoch to reach the target acc: 30
[MPI Rank 0] Success 
[MPI Rank 3] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
