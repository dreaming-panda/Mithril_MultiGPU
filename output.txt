g000.anvil.rcac.purdue.edu
Wed Jan 18 00:20:57 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:41:00.0 Off |                    0 |
| N/A   26C    P0    49W / 400W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2    9) numactl/2.0.14
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0    10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0   11) openmpi/4.0.6
  4) gmp/6.2.1              8) zlib/1.2.11  12) boost/1.74.0

 

[  4%] Built target context
[ 17%] Built target parallel
[ 19%] Built target core
[ 39%] Built target cudahelp
[ 42%] Built target test_graph
[ 44%] Built target test_cuda_data_compression
[ 45%] Built target test_mpi_non_structual_graph
[ 50%] Built target test_mpi_gpu_hybrid
[ 67%] Built target test_mpi_combined
[ 67%] Built target test_mpi_gpu_model_parallel
[ 79%] Built target test_hello_world
[ 87%] Built target test_mpi_pipelined_model_parallel
[ 87%] Built target test_cuda
[ 87%] Built target test_full_structual_graph
[ 87%] Built target test_single_node_training
[ 91%] Built target estimate_comm_volume
[ 87%] Built target test_single_node_fullgpu_training
[ 87%] Built target test_mpi_loader
[ 87%] Built target test_mpi_gpu_pipelined_model_parallel
[ 87%] Built target test_cuda_graph
[ 87%] Built target test_mpi_model_parallel
[ 87%] Built target test_mpi_structual_graph
[ 87%] Built target test_trivial
[ 91%] Built target test_nccl_mpi
[ 87%] Built target test_single_node_gpu_training
[ 91%] Built target test_cuda_model_parallel
[ 91%] Built target test_nccl_thread
[ 87%] Built target test_full_non_structual_graph
[ 87%] Built target test_cuda_pipeline_parallel
[ 95%] Built target OSDI2023_MULTI_NODES_gcn
[ 96%] Built target test_two_layer_hybrid_parallelism_designer
[ 98%] Built target OSDI2023_SINGLE_NODE_gcn_inference
[100%] Built target OSDI2023_SINGLE_NODE_gcn
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 500
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 500
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 500
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/ogbn_products
The number of GCN layers: 4
The number of hidden units: 128
The number of training epoches: 500
The number of startup epoches: 0
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
Initialized node 0 on machine g000.anvil.rcac.purdue.edu
Initialized node 1 on machine g001.anvil.rcac.purdue.edu
Initialized node 3 on machine g003.anvil.rcac.purdue.edu
Initialized node 2 on machine g002.anvil.rcac.purdue.edu
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.940 seconds.
Building the CSC structure...
        It takes 1.925 seconds.
Building the CSC structure...
        It takes 1.985 seconds.
Building the CSC structure...
        It takes 1.970 seconds.
Building the CSC structure...
        It takes 1.818 seconds.
        It takes 1.903 seconds.
        It takes 1.882 seconds.
        It takes 1.954 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.557 seconds.
Building the Label Vector...
        It takes 0.577 seconds.
Building the Label Vector...
        It takes 0.596 seconds.
Building the Label Vector...
        It takes 0.585 seconds.
Building the Label Vector...
        It takes 0.310 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
        It takes 0.310 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
        It takes 0.326 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
        It takes 0.321 seconds.
Number of classes: 47
Number of feature dimensions: 100
Number of vertices: 2449029
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
*** Node 0, starting model training...
Number of operators: 20
0 2449029 0 6
0 2449029 6 11
0 2449029 11 16
0 2449029 16 20
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the partition [0, 6) x [0, 2449029)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_WEIGHT, output tensors: 1
    Op 2: type OPERATOR_MATMUL, output tensors: 2
    Op 3: type OPERATOR_AGGREGATION, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_WEIGHT, output tensors: 6
    Op 7: type OPERATOR_MATMUL, output tensors: 7
    Op 8: type OPERATOR_AGGREGATION, output tensors: 8
    Op 9: type OPERATOR_RELU, output tensors: 9
    Op 10: type OPERATOR_DROPOUT, output tensors: 10
    Op 11: type OPERATOR_WEIGHT, output tensors: 11
    Op 12: type OPERATOR_MATMUL, output tensors: 12
    Op 13: type OPERATOR_AGGREGATION, output tensors: 13
    Op 14: type OPERATOR_RELU, output tensors: 14
    Op 15: type OPERATOR_DROPOUT, output tensors: 15
    Op 16: type OPERATOR_WEIGHT, output tensors: 16
    Op 17: type OPERATOR_MATMUL, output tensors: 17
    Op 18: type OPERATOR_AGGREGATION, output tensors: 18
    Op 19: type OPERATOR_SOFTMAX, output tensors: 19
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
*** Node 1, starting model training...
Number of operators: 20
0 2449029 0 6
0 2449029 6 11
0 2449029 11 16
0 2449029 16 20
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the partition [6, 11) x [0, 2449029)
*** Node 1, constructing the helper classes...
Boundaries: 0 0 0 0 2449029 2449029 2449029 2449029
Fragments: [0, 2449029)
Chunks (number of global chunks: 64): 0-[0, 38267) 1-[38267, 76534) 2-[76534, 114801) 3-[114801, 153068) 4-[153068, 191335) 5-[191335, 229602) 6-[229602, 267869) 7-[267869, 306136) 8-[306136, 344403) ... 63-[2410821, 2449029)
(Forwarding) Node 0 (fragment 0) depends on nodes:
(Backwarding) Node 0 (fragment 0) depends on nodes: 1 (Tensor: 5)
(I-link dependencies): node 0 should send activation to nodes:
(I-link dependencies): node 0 should receive activation from nodes:
(I-link dependencies): node 0 should send gradient to nodes:
(I-link dependencies): node 0 should receive gradient from nodes:
(Forwarding) Node 1 (fragment 0) depends on nodes: 0 (Tensor: 5)
(Backwarding) Node 1 (fragment 0) depends on nodes: 2 (Tensor: 10)
(I-link dependencies): node 1 should send activation to nodes:
(I-link dependencies): node 1 should receive activation from nodes:
(I-link dependencies): node 1 should send gradient to nodes:
(I-link dependencies): node 1 should receive gradient from nodes:
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
*** Node 2, starting model training...
Number of operators: 20
0 2449029 0 6
0 2449029 6 11
0 2449029 11 16
0 2449029 16 20
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the partition [11, 16) x [0, 2449029)
*** Node 2, constructing the helper classes...
(Forwarding) Node 2 (fragment 0) depends on nodes: 1 (Tensor: 10)
(Backwarding) Node 2 (fragment 0) depends on nodes: 3 (Tensor: 15)
(I-link dependencies): node 2 should send activation to nodes:
(I-link dependencies): node 2 should receive activation from nodes:
(I-link dependencies): node 2 should send gradient to nodes:
(I-link dependencies): node 2 should receive gradient from nodes:
train nodes 196615, valid nodes 39323, test nodes 2213091
Number of GPUs: 4
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
GPU 3, layer [3, 4)
WARNING: the current version only applies to linear GNN models!
*** Node 3, starting model training...
Number of operators: 20
0 2449029 0 6
0 2449029 6 11
0 2449029 11 16
0 2449029 16 20
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the partition [16, 20) x [0, 2449029)
*** Node 3, constructing the helper classes...
(Forwarding) Node 3 (fragment 0) depends on nodes: 2 (Tensor: 15)
(Backwarding) Node 3 (fragment 0) depends on nodes:
(I-link dependencies): node 3 should send activation to nodes:
(I-link dependencies): node 3 should receive activation from nodes:
(I-link dependencies): node 3 should send gradient to nodes:
(I-link dependencies): node 3 should receive gradient from nodes:
2449029, 126167053, 126167053
2449029, 126167053, 126167053
2449029, 126167053, 126167053
2449029, 126167053, 126167053
csr in-out ready !*** Node 0, setting up some other necessary information...
csr in-out ready !*** Node 3, setting up some other necessary information...
csr in-out ready !*** Node 2, setting up some other necessary information...
csr in-out ready !*** Node 1, setting up some other necessary information...
*** Node 0, starting the helper threads...
*** Node 1, starting the helper threads...
*** Node 3, starting the helper threads...
*** Node 2, starting the helper threads...
+++++++++ Node 1 initializing the weights for op[6, 11)...
+++++++++ Node 1, mapping weight op 6
+++++++++ Node 2 initializing the weights for op[11, 16)...
+++++++++ Node 2, mapping weight op 11
+++++++++ Node 0 initializing the weights for op[0, 6)...
+++++++++ Node 0, mapping weight op 1
+++++++++ Node 3 initializing the weights for op[16, 20)...
+++++++++ Node 3, mapping weight op 16
RANDOMLY DISPATCH THE CHUNKS...
*** Node 1, starting task scheduling...
*** Node 3, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 2, starting task scheduling...



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
    Epoch 9:	Loss 38.92037	TrainAcc 0.0061	ValidAcc 0.0065	TestAcc 0.0131
    Epoch 19:	Loss 38.94063	TrainAcc 0.0049	ValidAcc 0.0058	TestAcc 0.0125
    Epoch 29:	Loss 38.94045	TrainAcc 0.0056	ValidAcc 0.0066	TestAcc 0.0125
    Epoch 39:	Loss 38.94208	TrainAcc 0.0050	ValidAcc 0.0057	TestAcc 0.0127
    Epoch 49:	Loss 38.94443	TrainAcc 0.0054	ValidAcc 0.0058	TestAcc 0.0123
    Epoch 59:	Loss 38.94099	TrainAcc 0.0055	ValidAcc 0.0056	TestAcc 0.0132
    Epoch 69:	Loss 38.94112	TrainAcc 0.0052	ValidAcc 0.0061	TestAcc 0.0126
    Epoch 79:	Loss 38.94170	TrainAcc 0.0052	ValidAcc 0.0060	TestAcc 0.0127
    Epoch 89:	Loss 38.94293	TrainAcc 0.0053	ValidAcc 0.0062	TestAcc 0.0126
    Epoch 99:	Loss 38.94215	TrainAcc 0.0053	ValidAcc 0.0063	TestAcc 0.0124
    Epoch 109:	Loss 38.94339	TrainAcc 0.0061	ValidAcc 0.0067	TestAcc 0.0134
    Epoch 119:	Loss 38.94290	TrainAcc 0.0052	ValidAcc 0.0063	TestAcc 0.0125
    Epoch 129:	Loss 38.94150	TrainAcc 0.0062	ValidAcc 0.0068	TestAcc 0.0129
    Epoch 139:	Loss 38.94041	TrainAcc 0.0054	ValidAcc 0.0063	TestAcc 0.0126
    Epoch 149:	Loss 38.94194	TrainAcc 0.0057	ValidAcc 0.0071	TestAcc 0.0130
    Epoch 159:	Loss 38.94216	TrainAcc 0.0056	ValidAcc 0.0064	TestAcc 0.0127
    Epoch 169:	Loss 38.93994	TrainAcc 0.0052	ValidAcc 0.0062	TestAcc 0.0130
    Epoch 179:	Loss 38.94455	TrainAcc 0.0056	ValidAcc 0.0061	TestAcc 0.0126
    Epoch 189:	Loss 38.93967	TrainAcc 0.0059	ValidAcc 0.0061	TestAcc 0.0125
    Epoch 199:	Loss 38.93996	TrainAcc 0.0055	ValidAcc 0.0065	TestAcc 0.0128
    Epoch 209:	Loss 38.94241	TrainAcc 0.0053	ValidAcc 0.0069	TestAcc 0.0129
    Epoch 219:	Loss 38.94407	TrainAcc 0.0060	ValidAcc 0.0071	TestAcc 0.0127
    Epoch 229:	Loss 38.94163	TrainAcc 0.0056	ValidAcc 0.0066	TestAcc 0.0132
    Epoch 239:	Loss 38.94125	TrainAcc 0.0054	ValidAcc 0.0063	TestAcc 0.0127
    Epoch 249:	Loss 38.94252	TrainAcc 0.0054	ValidAcc 0.0062	TestAcc 0.0123
    Epoch 259:	Loss 38.93846	TrainAcc 0.0053	ValidAcc 0.0054	TestAcc 0.0128
    Epoch 269:	Loss 38.94353	TrainAcc 0.0052	ValidAcc 0.0060	TestAcc 0.0128
    Epoch 279:	Loss 38.94270	TrainAcc 0.0052	ValidAcc 0.0065	TestAcc 0.0128
    Epoch 289:	Loss 38.94320	TrainAcc 0.0046	ValidAcc 0.0055	TestAcc 0.0121
    Epoch 299:	Loss 38.94202	TrainAcc 0.0049	ValidAcc 0.0055	TestAcc 0.0129
    Epoch 309:	Loss 38.94110	TrainAcc 0.0055	ValidAcc 0.0062	TestAcc 0.0127
    Epoch 319:	Loss 38.94249	TrainAcc 0.0054	ValidAcc 0.0063	TestAcc 0.0129
    Epoch 329:	Loss 38.93929	TrainAcc 0.0057	ValidAcc 0.0067	TestAcc 0.0129
    Epoch 339:	Loss 38.94098	TrainAcc 0.0064	ValidAcc 0.0063	TestAcc 0.0128
    Epoch 349:	Loss 38.94017	TrainAcc 0.0057	ValidAcc 0.0061	TestAcc 0.0129
    Epoch 359:	Loss 38.94064	TrainAcc 0.0057	ValidAcc 0.0063	TestAcc 0.0128
    Epoch 369:	Loss 38.94402	TrainAcc 0.0048	ValidAcc 0.0054	TestAcc 0.0126
    Epoch 379:	Loss 38.94259	TrainAcc 0.0050	ValidAcc 0.0055	TestAcc 0.0126
    Epoch 389:	Loss 38.94352	TrainAcc 0.0050	ValidAcc 0.0060	TestAcc 0.0132
    Epoch 399:	Loss 38.94181	TrainAcc 0.0054	ValidAcc 0.0058	TestAcc 0.0124
    Epoch 409:	Loss 38.93584	TrainAcc 0.0052	ValidAcc 0.0063	TestAcc 0.0124
    Epoch 419:	Loss 38.94141	TrainAcc 0.0054	ValidAcc 0.0064	TestAcc 0.0126
    Epoch 429:	Loss 38.94039	TrainAcc 0.0051	ValidAcc 0.0052	TestAcc 0.0124
    Epoch 439:	Loss 38.94209	TrainAcc 0.0059	ValidAcc 0.0065	TestAcc 0.0128
    Epoch 449:	Loss 38.94268	TrainAcc 0.0046	ValidAcc 0.0055	TestAcc 0.0125
    Epoch 459:	Loss 38.94030	TrainAcc 0.0049	ValidAcc 0.0062	TestAcc 0.0122
    Epoch 469:	Loss 38.94405	TrainAcc 0.0060	ValidAcc 0.0063	TestAcc 0.0125
    Epoch 479:	Loss 38.94567	TrainAcc 0.0053	ValidAcc 0.0063	TestAcc 0.0120
    Epoch 489:	Loss 38.94051	TrainAcc 0.0051	ValidAcc 0.0061	TestAcc 0.0123
Node 0, Layer-level comm throughput (act): -nan GBps
Node 1, Layer-level comm throughput (act): 11.095 GBps
Node 2, Layer-level comm throughput (act): 10.563 GBps
Node 3, Layer-level comm throughput (act): 10.378 GBps
    Epoch 499:	Loss 38.94370	TrainAcc 0.0058	ValidAcc 0.0066	TestAcc 0.0131
Node 0, compression time: 2.919s, compression size: 583.894GB, throughput: 200.037GBps
Node 0, decompression time: 0.000s, compression size: 583.894GB, throughput: -nanGBps
Node 0, pure compute time: 22.162 s, total compute time: 25.081 s
Node 0, wait_for_task_time: 0.009 s, wait_for_other_gpus_time: 0.004 s
------------------------node id 0,  per-epoch time: 0.088921 s---------------
Node 1, compression time: 2.945s, compression size: 583.894GB, throughput: 198.252GBps
Node 1, decompression time: 13.829s, compression size: 583.894GB, throughput: 42.222GBps
Node 1, pure compute time: 15.894 s, total compute time: 32.668 s
Node 0, wait_for_task_time: 0.815 s, wait_for_other_gpus_time: 0.004 s
------------------------node id 1,  per-epoch time: 0.088921 s---------------
Node 3, compression time: 0.000s, compression size: 0.000GB, throughput: -nanGBps
Node 3, decompression time: 3.902s, compression size: 0.000GB, throughput: 149.652GBps
Node 3, pure compute time: 21.203 s, total compute time: 25.105 s
Node 0, wait_for_task_time: 19.031 s, wait_for_other_gpus_time: 0.005 s
------------------------node id 3,  per-epoch time: 0.088921 s---------------
Node 2, compression time: 3.128s, compression size: 583.894GB, throughput: 186.689GBps
Node 2, decompression time: 21.132s, compression size: 583.894GB, throughput: 27.631GBps
Node 2, pure compute time: 16.487 s, total compute time: 40.746 s
Node 0, wait_for_task_time: 2.448 s, wait_for_other_gpus_time: 0.003 s
------------------------node id 2,  per-epoch time: 0.088921 s---------------
************ Profiling Results ************
	Bubble: 7.808783 (s) (18.79 percentage)
	Compute: 19.012140 (s) (45.74 percentage)
	GradSync: 0.196902 (s) (0.47 percentage)
	GraphComm: 0.017077 (s) (0.04 percentage)
	Imbalance: 7.864060 (s) (18.92 percentage)
	LayerComm: 6.663802 (s) (16.03 percentage)
	Layer-level communication (cluster-wide, per epoch): 0.990 GB
Highest valid_acc: 0.0071
Target test_acc: 0.0130
Epoch to reach the target acc: 150
[MPI Rank 0] Success 
[MPI Rank 3] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
