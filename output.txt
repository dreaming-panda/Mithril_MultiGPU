g009.anvil.rcac.purdue.edu
Sun Apr  2 22:52:38 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:01:00.0 Off |                    0 |
| N/A   29C    P0    51W / 400W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Currently Loaded Modules:
  1) modtree/gpu            5) mpfr/4.0.2    9) numactl/2.0.14
  2) nccl/cuda-11.2_2.8.4   6) mpc/1.1.0    10) cuda/11.4.2
  3) cudnn/cuda-11.2_8.1    7) gcc/11.2.0   11) openmpi/4.0.6
  4) gmp/6.2.1              8) zlib/1.2.11  12) boost/1.74.0

 

[  4%] Built target context
[ 17%] Built target parallel
[ 19%] Built target core
[ 39%] Built target cudahelp
[ 44%] Built target test_full_non_structual_graph
[ 65%] Built target test_full_structual_graph
[ 65%] Built target test_mpi_gpu_hybrid
[ 65%] Built target test_mpi_gpu_pipelined_model_parallel
[ 85%] Built target test_cuda_pipeline_parallel
[ 85%] Built target test_mpi_non_structual_graph
[ 67%] Built target test_cuda_graph
[ 85%] Built target test_graph
[ 85%] Built target test_cuda_data_compression
[ 85%] Built target test_nccl_thread
[ 67%] Built target test_mpi_gpu_model_parallel
[ 85%] Built target test_mpi_combined
[ 85%] Built target test_hello_world
[ 85%] Built target test_mpi_loader
[ 85%] Built target test_cuda
[ 85%] Built target test_cuda_model_parallel
[ 85%] Built target test_trivial
[ 91%] Built target test_single_node_gpu_training
[ 91%] Built target test_mpi_model_parallel
[ 91%] Built target estimate_comm_volume
[ 91%] Built target test_mpi_structual_graph
[ 91%] Built target test_mpi_pipelined_model_parallel
[ 91%] Built target test_single_node_training
[ 91%] Built target test_nccl_mpi
[ 91%] Built target test_single_node_fullgpu_training
[ 97%] Built target OSDI2023_MULTI_NODES_gcn
[ 97%] Built target OSDI2023_SINGLE_NODE_gcn
[100%] Built target test_two_layer_hybrid_parallelism_designer
[100%] Built target OSDI2023_SINGLE_NODE_gcn_inference
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/reddit
The number of GCN layers: 3
The number of hidden units: 256
The number of training epoches: 1000
The number of startup epoches: 0
Learning rate: 0.003000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 3
The scaling down factor of out-of-chunk gradients: 0.100000
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/reddit
The number of GCN layers: 3
The number of hidden units: 256
The number of training epoches: 1000
The number of startup epoches: 0
Learning rate: 0.003000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 3
The scaling down factor of out-of-chunk gradients: 0.100000
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/reddit
The number of GCN layers: 3
The number of hidden units: 256
The number of training epoches: 1000
The number of startup epoches: 0
Learning rate: 0.003000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /anvil/projects/x-cis220117/saved_weights_pipe
The random seed: 3
The scaling down factor of out-of-chunk gradients: 0.100000
Initialized node 1 on machine g010.anvil.rcac.purdue.edu
Initialized node 0 on machine g009.anvil.rcac.purdue.edu
Initialized node 2 on machine g011.anvil.rcac.purdue.edu
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.797 seconds.
Building the CSC structure...
        It takes 1.846 seconds.
Building the CSC structure...
        It takes 1.839 seconds.
Building the CSC structure...
        It takes 1.713 seconds.
        It takes 1.718 seconds.
        It takes 1.718 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.401 seconds.
        It takes 0.396 seconds.
Building the Label Vector...
        It takes 0.391 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.068 seconds.
        It takes 0.064 seconds.
        It takes 0.072 seconds.
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
train nodes 153431, valid nodes 23831, test nodes 55703
Number of GPUs: 3
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
WARNING: the current version only applies to linear GNN models!
*** Node 0, starting model training...
Number of operators: 15
0 232965 0 6
0 232965 6 11
0 232965 11 15
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the partition [0, 6) x [0, 232965)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_WEIGHT, output tensors: 1
    Op 2: type OPERATOR_MATMUL, output tensors: 2
    Op 3: type OPERATOR_AGGREGATION, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_AGGREGATION, output tensors: 6
    Op 7: type OPERATOR_WEIGHT, output tensors: 7
    Op 8: type OPERATOR_MATMUL, output tensors: 8
    Op 9: type OPERATOR_RELU, output tensors: 9
    Op 10: type OPERATOR_DROPOUT, output tensors: 10
    Op 11: type OPERATOR_AGGREGATION, output tensors: 11
    Op 12: type OPERATOR_WEIGHT, output tensors: 12
    Op 13: type OPERATOR_MATMUL, output tensors: 13
    Op 14: type OPERATOR_SOFTMAX, output tensors: 14
train nodes 153431, valid nodes 23831, test nodes 55703
Number of GPUs: 3
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
WARNING: the current version only applies to linear GNN models!
*** Node 2, starting model training...
Number of operators: 15
0 232965 0 6
0 232965 6 11
0 232965 11 15
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the partition [11, 15) x [0, 232965)
*** Node 2, constructing the helper classes...
(Forwarding) Node 2 (fragment 0) depends on nodes: 1 (Tensor: 10)
(Backwarding) Node 2 (fragment 0) depends on nodes:
(I-link dependencies): node 2 should send activation to nodes:
(I-link dependencies): node 2 should receive activation from nodes: 1 (tensor: 10)
(I-link dependencies): node 2 should send gradient to nodes:
(I-link dependencies): node 2 should receive gradient from nodes:
Boundaries: 0 0 0 232965 232965 232965
Fragments: [0, 232965)
Chunks (number of global chunks: 12): 0-[0, 19414) 1-[19414, 38828) 2-[38828, 58242) 3-[58242, 77656) 4-[77656, 97070) 5-[97070, 116484) 6-[116484, 135898) 7-[135898, 155312) 8-[155312, 174726) ... 11-[213554, 232965)
(Forwarding) Node 0 (fragment 0) depends on nodes:
(Backwarding) Node 0 (fragment 0) depends on nodes: 1 (Tensor: 5)
(I-link dependencies): node 0 should send activation to nodes: 1 (tensor: 5)
(I-link dependencies): node 0 should receive activation from nodes:
(I-link dependencies): node 0 should send gradient to nodes:
(I-link dependencies): node 0 should receive gradient from nodes:
train nodes 153431, valid nodes 23831, test nodes 55703
Number of GPUs: 3
GPU 0, layer [0, 1)
GPU 1, layer [1, 2)
GPU 2, layer [2, 3)
WARNING: the current version only applies to linear GNN models!
*** Node 1, starting model training...
Number of operators: 15
0 232965 0 6
0 232965 6 11
0 232965 11 15
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the partition [6, 11) x [0, 232965)
*** Node 1, constructing the helper classes...
(Forwarding) Node 1 (fragment 0) depends on nodes: 0 (Tensor: 5)
(Backwarding) Node 1 (fragment 0) depends on nodes: 2 (Tensor: 10)
(I-link dependencies): node 1 should send activation to nodes: 2 (tensor: 10)
(I-link dependencies): node 1 should receive activation from nodes: 0 (tensor: 5)
(I-link dependencies): node 1 should send gradient to nodes:
(I-link dependencies): node 1 should receive gradient from nodes:
232965, 114848857, 114848857
Number of vertices per chunk: 19414
232965, 114848857, 114848857
Number of vertices per chunk: 19414
232965, 114848857, 114848857
Number of vertices per chunk: 19414
csr in-out ready !*** Node 0, setting up some other necessary information...
csr in-out ready !*** Node 2, setting up some other necessary information...
csr in-out ready !*** Node 1, setting up some other necessary information...
*** Node 0, starting the helper threads...
*** Node 2, starting the helper threads...
*** Node 1, starting the helper threads...
+++++++++ Node 1 initializing the weights for op[6, 11)...
+++++++++ Node 1, mapping weight op 7
+++++++++ Node 0 initializing the weights for op[0, 6)...
+++++++++ Node 0, mapping weight op 1
+++++++++ Node 2 initializing the weights for op[11, 15)...
+++++++++ Node 2, mapping weight op 12
RANDOMLY DISPATCH THE CHUNKS...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.003000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.003000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.003000000
    Epoch 9:	Loss 1.36831	TrainAcc 0.7533	ValidAcc 0.7739	TestAcc 0.7679
    Epoch 19:	Loss 0.77119	TrainAcc 0.8556	ValidAcc 0.8674	TestAcc 0.8635
    Epoch 29:	Loss 0.47342	TrainAcc 0.9068	ValidAcc 0.9144	TestAcc 0.9151
    Epoch 39:	Loss 0.41796	TrainAcc 0.9157	ValidAcc 0.9233	TestAcc 0.9218
    Epoch 49:	Loss 0.36465	TrainAcc 0.9245	ValidAcc 0.9296	TestAcc 0.9298
    Epoch 59:	Loss 0.34855	TrainAcc 0.9272	ValidAcc 0.9321	TestAcc 0.9319
    Epoch 69:	Loss 0.33219	TrainAcc 0.9298	ValidAcc 0.9341	TestAcc 0.9349
    Epoch 79:	Loss 0.32193	TrainAcc 0.9322	ValidAcc 0.9365	TestAcc 0.9366
    Epoch 89:	Loss 0.31300	TrainAcc 0.9337	ValidAcc 0.9369	TestAcc 0.9381
    Epoch 99:	Loss 0.30764	TrainAcc 0.9343	ValidAcc 0.9372	TestAcc 0.9380
    Epoch 109:	Loss 0.30113	TrainAcc 0.9351	ValidAcc 0.9388	TestAcc 0.9392
    Epoch 119:	Loss 0.29699	TrainAcc 0.9362	ValidAcc 0.9392	TestAcc 0.9395
    Epoch 129:	Loss 0.29272	TrainAcc 0.9368	ValidAcc 0.9392	TestAcc 0.9404
    Epoch 139:	Loss 0.28896	TrainAcc 0.9370	ValidAcc 0.9394	TestAcc 0.9407
    Epoch 149:	Loss 0.28452	TrainAcc 0.9380	ValidAcc 0.9405	TestAcc 0.9418
    Epoch 159:	Loss 0.28249	TrainAcc 0.9382	ValidAcc 0.9401	TestAcc 0.9420
    Epoch 169:	Loss 0.27957	TrainAcc 0.9387	ValidAcc 0.9405	TestAcc 0.9421
    Epoch 179:	Loss 0.27603	TrainAcc 0.9393	ValidAcc 0.9410	TestAcc 0.9425
    Epoch 189:	Loss 0.27213	TrainAcc 0.9397	ValidAcc 0.9408	TestAcc 0.9427
    Epoch 199:	Loss 0.27105	TrainAcc 0.9400	ValidAcc 0.9417	TestAcc 0.9424
    Epoch 209:	Loss 0.26862	TrainAcc 0.9401	ValidAcc 0.9413	TestAcc 0.9428
    Epoch 219:	Loss 0.26678	TrainAcc 0.9413	ValidAcc 0.9422	TestAcc 0.9436
    Epoch 229:	Loss 0.26474	TrainAcc 0.9413	ValidAcc 0.9423	TestAcc 0.9439
    Epoch 239:	Loss 0.26275	TrainAcc 0.9417	ValidAcc 0.9428	TestAcc 0.9438
    Epoch 249:	Loss 0.26029	TrainAcc 0.9419	ValidAcc 0.9431	TestAcc 0.9441
    Epoch 259:	Loss 0.25902	TrainAcc 0.9422	ValidAcc 0.9421	TestAcc 0.9440
    Epoch 269:	Loss 0.25791	TrainAcc 0.9422	ValidAcc 0.9433	TestAcc 0.9439
    Epoch 279:	Loss 0.25663	TrainAcc 0.9427	ValidAcc 0.9431	TestAcc 0.9442
    Epoch 289:	Loss 0.25403	TrainAcc 0.9431	ValidAcc 0.9424	TestAcc 0.9439
    Epoch 299:	Loss 0.25315	TrainAcc 0.9429	ValidAcc 0.9428	TestAcc 0.9439
    Epoch 309:	Loss 0.25097	TrainAcc 0.9433	ValidAcc 0.9423	TestAcc 0.9449
    Epoch 319:	Loss 0.25055	TrainAcc 0.9437	ValidAcc 0.9441	TestAcc 0.9445
    Epoch 329:	Loss 0.24890	TrainAcc 0.9437	ValidAcc 0.9436	TestAcc 0.9446
    Epoch 339:	Loss 0.24820	TrainAcc 0.9441	ValidAcc 0.9435	TestAcc 0.9447
    Epoch 349:	Loss 0.24589	TrainAcc 0.9441	ValidAcc 0.9439	TestAcc 0.9447
    Epoch 359:	Loss 0.24520	TrainAcc 0.9446	ValidAcc 0.9435	TestAcc 0.9447
    Epoch 369:	Loss 0.24427	TrainAcc 0.9444	ValidAcc 0.9442	TestAcc 0.9458
    Epoch 379:	Loss 0.24343	TrainAcc 0.9446	ValidAcc 0.9441	TestAcc 0.9451
    Epoch 389:	Loss 0.24278	TrainAcc 0.9452	ValidAcc 0.9441	TestAcc 0.9460
    Epoch 399:	Loss 0.24035	TrainAcc 0.9454	ValidAcc 0.9442	TestAcc 0.9455
    Epoch 409:	Loss 0.23938	TrainAcc 0.9451	ValidAcc 0.9436	TestAcc 0.9455
    Epoch 419:	Loss 0.23807	TrainAcc 0.9457	ValidAcc 0.9445	TestAcc 0.9457
    Epoch 429:	Loss 0.23831	TrainAcc 0.9455	ValidAcc 0.9444	TestAcc 0.9452
    Epoch 439:	Loss 0.23678	TrainAcc 0.9459	ValidAcc 0.9438	TestAcc 0.9461
    Epoch 449:	Loss 0.23609	TrainAcc 0.9462	ValidAcc 0.9455	TestAcc 0.9462
    Epoch 459:	Loss 0.23596	TrainAcc 0.9461	ValidAcc 0.9441	TestAcc 0.9456
    Epoch 469:	Loss 0.23478	TrainAcc 0.9464	ValidAcc 0.9445	TestAcc 0.9459
    Epoch 479:	Loss 0.23496	TrainAcc 0.9458	ValidAcc 0.9441	TestAcc 0.9459
    Epoch 489:	Loss 0.23321	TrainAcc 0.9465	ValidAcc 0.9441	TestAcc 0.9456
    Epoch 499:	Loss 0.23283	TrainAcc 0.9463	ValidAcc 0.9439	TestAcc 0.9460
    Epoch 509:	Loss 0.23154	TrainAcc 0.9463	ValidAcc 0.9448	TestAcc 0.9458
    Epoch 519:	Loss 0.23099	TrainAcc 0.9469	ValidAcc 0.9451	TestAcc 0.9456
    Epoch 529:	Loss 0.23007	TrainAcc 0.9471	ValidAcc 0.9451	TestAcc 0.9456
    Epoch 539:	Loss 0.22914	TrainAcc 0.9470	ValidAcc 0.9450	TestAcc 0.9463
    Epoch 549:	Loss 0.22828	TrainAcc 0.9471	ValidAcc 0.9448	TestAcc 0.9460
    Epoch 559:	Loss 0.22819	TrainAcc 0.9473	ValidAcc 0.9444	TestAcc 0.9468
    Epoch 569:	Loss 0.22820	TrainAcc 0.9472	ValidAcc 0.9444	TestAcc 0.9462
    Epoch 579:	Loss 0.22802	TrainAcc 0.9472	ValidAcc 0.9451	TestAcc 0.9467
    Epoch 589:	Loss 0.22567	TrainAcc 0.9473	ValidAcc 0.9451	TestAcc 0.9462
    Epoch 599:	Loss 0.22482	TrainAcc 0.9478	ValidAcc 0.9449	TestAcc 0.9457
    Epoch 609:	Loss 0.22435	TrainAcc 0.9478	ValidAcc 0.9454	TestAcc 0.9469
    Epoch 619:	Loss 0.22381	TrainAcc 0.9480	ValidAcc 0.9451	TestAcc 0.9461
    Epoch 629:	Loss 0.22390	TrainAcc 0.9479	ValidAcc 0.9446	TestAcc 0.9467
    Epoch 639:	Loss 0.22351	TrainAcc 0.9479	ValidAcc 0.9447	TestAcc 0.9458
    Epoch 649:	Loss 0.22189	TrainAcc 0.9486	ValidAcc 0.9457	TestAcc 0.9466
    Epoch 659:	Loss 0.22176	TrainAcc 0.9484	ValidAcc 0.9460	TestAcc 0.9466
    Epoch 669:	Loss 0.22159	TrainAcc 0.9486	ValidAcc 0.9451	TestAcc 0.9466
    Epoch 679:	Loss 0.21997	TrainAcc 0.9486	ValidAcc 0.9444	TestAcc 0.9469
    Epoch 689:	Loss 0.22025	TrainAcc 0.9488	ValidAcc 0.9462	TestAcc 0.9470
    Epoch 699:	Loss 0.21986	TrainAcc 0.9491	ValidAcc 0.9455	TestAcc 0.9474
    Epoch 709:	Loss 0.22015	TrainAcc 0.9483	ValidAcc 0.9456	TestAcc 0.9467
    Epoch 719:	Loss 0.21874	TrainAcc 0.9489	ValidAcc 0.9464	TestAcc 0.9469
    Epoch 729:	Loss 0.21778	TrainAcc 0.9492	ValidAcc 0.9458	TestAcc 0.9467
    Epoch 739:	Loss 0.21736	TrainAcc 0.9492	ValidAcc 0.9455	TestAcc 0.9460
    Epoch 749:	Loss 0.21783	TrainAcc 0.9492	ValidAcc 0.9449	TestAcc 0.9463
    Epoch 759:	Loss 0.21678	TrainAcc 0.9497	ValidAcc 0.9464	TestAcc 0.9462
    Epoch 769:	Loss 0.21600	TrainAcc 0.9497	ValidAcc 0.9461	TestAcc 0.9463
    Epoch 779:	Loss 0.21559	TrainAcc 0.9497	ValidAcc 0.9461	TestAcc 0.9465
    Epoch 789:	Loss 0.21544	TrainAcc 0.9495	ValidAcc 0.9468	TestAcc 0.9471
    Epoch 799:	Loss 0.21463	TrainAcc 0.9499	ValidAcc 0.9445	TestAcc 0.9469
    Epoch 809:	Loss 0.21405	TrainAcc 0.9500	ValidAcc 0.9462	TestAcc 0.9469
    Epoch 819:	Loss 0.21477	TrainAcc 0.9497	ValidAcc 0.9457	TestAcc 0.9459
    Epoch 829:	Loss 0.21450	TrainAcc 0.9498	ValidAcc 0.9468	TestAcc 0.9470
    Epoch 839:	Loss 0.21366	TrainAcc 0.9502	ValidAcc 0.9462	TestAcc 0.9469
    Epoch 849:	Loss 0.21294	TrainAcc 0.9500	ValidAcc 0.9466	TestAcc 0.9470
    Epoch 859:	Loss 0.21243	TrainAcc 0.9500	ValidAcc 0.9462	TestAcc 0.9467
    Epoch 869:	Loss 0.21296	TrainAcc 0.9499	ValidAcc 0.9454	TestAcc 0.9463
    Epoch 879:	Loss 0.21173	TrainAcc 0.9500	ValidAcc 0.9457	TestAcc 0.9467
    Epoch 889:	Loss 0.21136	TrainAcc 0.9505	ValidAcc 0.9466	TestAcc 0.9470
    Epoch 899:	Loss 0.21141	TrainAcc 0.9502	ValidAcc 0.9458	TestAcc 0.9471
    Epoch 909:	Loss 0.21070	TrainAcc 0.9507	ValidAcc 0.9465	TestAcc 0.9477
    Epoch 919:	Loss 0.21021	TrainAcc 0.9506	ValidAcc 0.9458	TestAcc 0.9471
    Epoch 929:	Loss 0.21051	TrainAcc 0.9505	ValidAcc 0.9454	TestAcc 0.9473
    Epoch 939:	Loss 0.20956	TrainAcc 0.9507	ValidAcc 0.9466	TestAcc 0.9465
    Epoch 949:	Loss 0.20800	TrainAcc 0.9506	ValidAcc 0.9466	TestAcc 0.9471
    Epoch 959:	Loss 0.20840	TrainAcc 0.9506	ValidAcc 0.9468	TestAcc 0.9473
    Epoch 969:	Loss 0.20784	TrainAcc 0.9509	ValidAcc 0.9470	TestAcc 0.9470
    Epoch 979:	Loss 0.20843	TrainAcc 0.9506	ValidAcc 0.9464	TestAcc 0.9467
    Epoch 989:	Loss 0.20687	TrainAcc 0.9512	ValidAcc 0.9463	TestAcc 0.9473
Node 0, Layer-level comm throughput (act): -nan GBps
Node 1, Layer-level comm throughput (act): 9.966 GBps
Node 2, Layer-level comm throughput (act): 11.099 GBps
Node 2, Layer-level comm throughput (grad): -nan GBps
Node 1, Layer-level comm throughput (grad): 11.103 GBps
Node 0, Layer-level comm throughput (grad): 9.843 GBps
    Epoch 999:	Loss 0.20680	TrainAcc 0.9515	ValidAcc 0.9469	TestAcc 0.9473
Node 0, compression time: 1.119s, compression size: 222.173GB, throughput: 198.465GBps
Node 0, decompression time: 3.538s, compression size: 222.173GB, throughput: 62.803GBps
Node 0, pure compute time: 72.724 s, total compute time: 77.381 s
Node 0, wait_for_task_time: 22.861 s, wait_for_other_gpus_time: 0.005 s
------------------------node id 0,  per-epoch time: 0.102879 s---------------
Node 1, compression time: 2.702s, compression size: 444.345GB, throughput: 164.421GBps
Node 1, decompression time: 13.385s, compression size: 444.345GB, throughput: 33.197GBps
Node 1, pure compute time: 65.838 s, total compute time: 81.925 s
Node 1, wait_for_task_time: 13.261 s, wait_for_other_gpus_time: 0.005 s
------------------------node id 1,  per-epoch time: 0.102879 s---------------
Node 2, compression time: 1.681s, compression size: 222.173GB, throughput: 132.132GBps
Node 2, decompression time: 3.652s, compression size: 222.173GB, throughput: 60.828GBps
Node 2, pure compute time: 57.694 s, total compute time: 63.028 s
Node 2, wait_for_task_time: 14.843 s, wait_for_other_gpus_time: 0.007 s
------------------------node id 2,  per-epoch time: 0.102879 s---------------
************ Profiling Results ************
	Bubble: 14.745972 (s) (14.29 percentage)
	Compute: 76.789388 (s) (74.43 percentage)
	GradSync: 0.722086 (s) (0.70 percentage)
	GraphComm: 0.022983 (s) (0.02 percentage)
	Imbalance: 8.252931 (s) (8.00 percentage)
	LayerComm: 2.634991 (s) (2.55 percentage)
	Layer-level communication (cluster-wide, per epoch): 0.284 GB
Highest valid_acc: 0.9470
Target test_acc: 0.9470
Epoch to reach the target acc: 970
[MPI Rank 0] Success 
[MPI Rank 2] Success 
[MPI Rank 1] Success 
The graph dataset locates at /anvil/projects/x-cis220117/gnn_datasets/reordered/reddit
The number of GCN layers: 3
The number of hidden units: 256
The number of training epoches: 0
Learning rate: 0.000000
Initialized node g009.anvil.rcac.purdue.edu
Building the CSR structure...
        It takes 1.805 seconds.
Building the CSC structure...
        It takes 1.762 seconds.
Building the Feature Vector...
        It takes 0.304 seconds.
Building the Label Vector...
        It takes 0.028 seconds.
Number of classes: 41
Number of feature dimensions: 602
Dropout: 0.000 
train nodes 153431, valid nodes 23831, test nodes 55703
*** Allocating resources for all tensors...
    OP_TYPE: OPERATOR_INPUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_RELU
    OP_TYPE: OPERATOR_DROPOUT
    OP_TYPE: OPERATOR_WEIGHT
    OP_TYPE: OPERATOR_MATMUL
    OP_TYPE: OPERATOR_AGGREGATION
    OP_TYPE: OPERATOR_SOFTMAX
*** Done allocating resource.
*** Preparing the input tensor...
*** Done preparing the input tensor.
*** Preparing the STD tensor...
    Number of labels: 41
    Number of vertices: 232965
*** Done preparing the STD tensor.
Version 0	Loss 1.0861	TrainAcc 0.7856	ValidAcc 0.8010	TestAcc 0.7997
Version 1	Loss 0.6312	TrainAcc 0.8690	ValidAcc 0.8796	TestAcc 0.8784
Version 2	Loss 0.4360	TrainAcc 0.9114	ValidAcc 0.9176	TestAcc 0.9192
Version 3	Loss 0.3778	TrainAcc 0.9202	ValidAcc 0.9278	TestAcc 0.9266
Version 4	Loss 0.3437	TrainAcc 0.9284	ValidAcc 0.9334	TestAcc 0.9332
Version 5	Loss 0.3246	TrainAcc 0.9319	ValidAcc 0.9365	TestAcc 0.9372
Version 6	Loss 0.3145	TrainAcc 0.9333	ValidAcc 0.9368	TestAcc 0.9382
Version 7	Loss 0.3049	TrainAcc 0.9358	ValidAcc 0.9380	TestAcc 0.9400
Version 8	Loss 0.2985	TrainAcc 0.9370	ValidAcc 0.9393	TestAcc 0.9412
Version 9	Loss 0.2922	TrainAcc 0.9380	ValidAcc 0.9401	TestAcc 0.9419
Version 10	Loss 0.2874	TrainAcc 0.9382	ValidAcc 0.9402	TestAcc 0.9422
Version 11	Loss 0.2824	TrainAcc 0.9394	ValidAcc 0.9408	TestAcc 0.9427
Version 12	Loss 0.2783	TrainAcc 0.9399	ValidAcc 0.9411	TestAcc 0.9436
Version 13	Loss 0.2746	TrainAcc 0.9406	ValidAcc 0.9415	TestAcc 0.9439
Version 14	Loss 0.2711	TrainAcc 0.9410	ValidAcc 0.9413	TestAcc 0.9441
Version 15	Loss 0.2682	TrainAcc 0.9417	ValidAcc 0.9423	TestAcc 0.9447
Version 16	Loss 0.2651	TrainAcc 0.9418	ValidAcc 0.9421	TestAcc 0.9448
Version 17	Loss 0.2624	TrainAcc 0.9423	ValidAcc 0.9423	TestAcc 0.9449
Version 18	Loss 0.2598	TrainAcc 0.9428	ValidAcc 0.9426	TestAcc 0.9454
Version 19	Loss 0.2573	TrainAcc 0.9438	ValidAcc 0.9438	TestAcc 0.9459
Version 20	Loss 0.2552	TrainAcc 0.9438	ValidAcc 0.9434	TestAcc 0.9460
Version 21	Loss 0.2529	TrainAcc 0.9442	ValidAcc 0.9437	TestAcc 0.9459
Version 22	Loss 0.2512	TrainAcc 0.9447	ValidAcc 0.9439	TestAcc 0.9462
Version 23	Loss 0.2494	TrainAcc 0.9449	ValidAcc 0.9445	TestAcc 0.9465
Version 24	Loss 0.2478	TrainAcc 0.9451	ValidAcc 0.9446	TestAcc 0.9464
Version 25	Loss 0.2461	TrainAcc 0.9451	ValidAcc 0.9444	TestAcc 0.9464
Version 26	Loss 0.2442	TrainAcc 0.9457	ValidAcc 0.9450	TestAcc 0.9467
Version 27	Loss 0.2428	TrainAcc 0.9461	ValidAcc 0.9454	TestAcc 0.9471
Version 28	Loss 0.2413	TrainAcc 0.9459	ValidAcc 0.9453	TestAcc 0.9469
Version 29	Loss 0.2399	TrainAcc 0.9463	ValidAcc 0.9457	TestAcc 0.9469
Version 30	Loss 0.2385	TrainAcc 0.9467	ValidAcc 0.9458	TestAcc 0.9469
Version 31	Loss 0.2376	TrainAcc 0.9467	ValidAcc 0.9456	TestAcc 0.9471
Version 32	Loss 0.2359	TrainAcc 0.9472	ValidAcc 0.9461	TestAcc 0.9474
Version 33	Loss 0.2349	TrainAcc 0.9471	ValidAcc 0.9454	TestAcc 0.9474
Version 34	Loss 0.2338	TrainAcc 0.9478	ValidAcc 0.9457	TestAcc 0.9477
Version 35	Loss 0.2329	TrainAcc 0.9475	ValidAcc 0.9460	TestAcc 0.9473
Version 36	Loss 0.2320	TrainAcc 0.9477	ValidAcc 0.9462	TestAcc 0.9477
Version 37	Loss 0.2308	TrainAcc 0.9480	ValidAcc 0.9458	TestAcc 0.9476
Version 38	Loss 0.2300	TrainAcc 0.9483	ValidAcc 0.9463	TestAcc 0.9478
Version 39	Loss 0.2289	TrainAcc 0.9485	ValidAcc 0.9462	TestAcc 0.9480
Version 40	Loss 0.2277	TrainAcc 0.9488	ValidAcc 0.9463	TestAcc 0.9482
Version 41	Loss 0.2269	TrainAcc 0.9489	ValidAcc 0.9468	TestAcc 0.9484
Version 42	Loss 0.2260	TrainAcc 0.9491	ValidAcc 0.9464	TestAcc 0.9482
Version 43	Loss 0.2255	TrainAcc 0.9490	ValidAcc 0.9464	TestAcc 0.9480
Version 44	Loss 0.2250	TrainAcc 0.9489	ValidAcc 0.9466	TestAcc 0.9479
Version 45	Loss 0.2238	TrainAcc 0.9495	ValidAcc 0.9467	TestAcc 0.9485
Version 46	Loss 0.2229	TrainAcc 0.9496	ValidAcc 0.9465	TestAcc 0.9482
Version 47	Loss 0.2222	TrainAcc 0.9497	ValidAcc 0.9469	TestAcc 0.9482
Version 48	Loss 0.2215	TrainAcc 0.9500	ValidAcc 0.9477	TestAcc 0.9482
Version 49	Loss 0.2206	TrainAcc 0.9499	ValidAcc 0.9464	TestAcc 0.9480
Version 50	Loss 0.2198	TrainAcc 0.9501	ValidAcc 0.9468	TestAcc 0.9485
Version 51	Loss 0.2195	TrainAcc 0.9502	ValidAcc 0.9473	TestAcc 0.9484
Version 52	Loss 0.2188	TrainAcc 0.9500	ValidAcc 0.9463	TestAcc 0.9483
Version 53	Loss 0.2179	TrainAcc 0.9506	ValidAcc 0.9472	TestAcc 0.9487
Version 54	Loss 0.2172	TrainAcc 0.9506	ValidAcc 0.9477	TestAcc 0.9487
Version 55	Loss 0.2166	TrainAcc 0.9509	ValidAcc 0.9474	TestAcc 0.9487
Version 56	Loss 0.2158	TrainAcc 0.9507	ValidAcc 0.9470	TestAcc 0.9487
Version 57	Loss 0.2151	TrainAcc 0.9510	ValidAcc 0.9473	TestAcc 0.9486
Version 58	Loss 0.2156	TrainAcc 0.9507	ValidAcc 0.9481	TestAcc 0.9484
Version 59	Loss 0.2141	TrainAcc 0.9513	ValidAcc 0.9479	TestAcc 0.9487
Version 60	Loss 0.2132	TrainAcc 0.9515	ValidAcc 0.9474	TestAcc 0.9487
Version 61	Loss 0.2129	TrainAcc 0.9515	ValidAcc 0.9475	TestAcc 0.9490
Version 62	Loss 0.2129	TrainAcc 0.9513	ValidAcc 0.9478	TestAcc 0.9487
Version 63	Loss 0.2121	TrainAcc 0.9515	ValidAcc 0.9476	TestAcc 0.9485
Version 64	Loss 0.2119	TrainAcc 0.9511	ValidAcc 0.9470	TestAcc 0.9482
Version 65	Loss 0.2106	TrainAcc 0.9519	ValidAcc 0.9476	TestAcc 0.9486
Version 66	Loss 0.2105	TrainAcc 0.9518	ValidAcc 0.9475	TestAcc 0.9487
Version 67	Loss 0.2099	TrainAcc 0.9520	ValidAcc 0.9478	TestAcc 0.9487
Version 68	Loss 0.2099	TrainAcc 0.9519	ValidAcc 0.9479	TestAcc 0.9487
Version 69	Loss 0.2087	TrainAcc 0.9521	ValidAcc 0.9479	TestAcc 0.9492
Version 70	Loss 0.2087	TrainAcc 0.9520	ValidAcc 0.9475	TestAcc 0.9491
Version 71	Loss 0.2077	TrainAcc 0.9523	ValidAcc 0.9478	TestAcc 0.9490
Version 72	Loss 0.2070	TrainAcc 0.9525	ValidAcc 0.9475	TestAcc 0.9487
Version 73	Loss 0.2070	TrainAcc 0.9525	ValidAcc 0.9478	TestAcc 0.9490
Version 74	Loss 0.2066	TrainAcc 0.9527	ValidAcc 0.9486	TestAcc 0.9492
Version 75	Loss 0.2060	TrainAcc 0.9526	ValidAcc 0.9481	TestAcc 0.9488
Version 76	Loss 0.2054	TrainAcc 0.9526	ValidAcc 0.9479	TestAcc 0.9489
Version 77	Loss 0.2054	TrainAcc 0.9526	ValidAcc 0.9480	TestAcc 0.9491
Version 78	Loss 0.2047	TrainAcc 0.9529	ValidAcc 0.9483	TestAcc 0.9494
Version 79	Loss 0.2050	TrainAcc 0.9525	ValidAcc 0.9483	TestAcc 0.9491
Version 80	Loss 0.2048	TrainAcc 0.9528	ValidAcc 0.9483	TestAcc 0.9492
Version 81	Loss 0.2038	TrainAcc 0.9529	ValidAcc 0.9481	TestAcc 0.9491
Version 82	Loss 0.2031	TrainAcc 0.9532	ValidAcc 0.9478	TestAcc 0.9494
Version 83	Loss 0.2028	TrainAcc 0.9530	ValidAcc 0.9477	TestAcc 0.9491
Version 84	Loss 0.2027	TrainAcc 0.9531	ValidAcc 0.9487	TestAcc 0.9495
Version 85	Loss 0.2020	TrainAcc 0.9536	ValidAcc 0.9485	TestAcc 0.9489
Version 86	Loss 0.2025	TrainAcc 0.9526	ValidAcc 0.9477	TestAcc 0.9484
Version 87	Loss 0.2014	TrainAcc 0.9532	ValidAcc 0.9483	TestAcc 0.9491
Version 88	Loss 0.2011	TrainAcc 0.9534	ValidAcc 0.9478	TestAcc 0.9492
Version 89	Loss 0.2013	TrainAcc 0.9532	ValidAcc 0.9483	TestAcc 0.9487
Version 90	Loss 0.1999	TrainAcc 0.9540	ValidAcc 0.9485	TestAcc 0.9492
Version 91	Loss 0.1996	TrainAcc 0.9539	ValidAcc 0.9484	TestAcc 0.9493
Version 92	Loss 0.1997	TrainAcc 0.9540	ValidAcc 0.9482	TestAcc 0.9492
Version 93	Loss 0.1987	TrainAcc 0.9538	ValidAcc 0.9485	TestAcc 0.9493
Version 94	Loss 0.1987	TrainAcc 0.9541	ValidAcc 0.9488	TestAcc 0.9497
Version 95	Loss 0.1984	TrainAcc 0.9539	ValidAcc 0.9486	TestAcc 0.9492
Version 96	Loss 0.1980	TrainAcc 0.9539	ValidAcc 0.9488	TestAcc 0.9499
Version 97	Loss 0.1971	TrainAcc 0.9543	ValidAcc 0.9483	TestAcc 0.9492
Version 98	Loss 0.1970	TrainAcc 0.9542	ValidAcc 0.9483	TestAcc 0.9492
Version 99	Loss 0.1972	TrainAcc 0.9540	ValidAcc 0.9486	TestAcc 0.9497
Version 96 achieved the highest validation accuracy 0.9488 (test accuracy: 0.9499)
