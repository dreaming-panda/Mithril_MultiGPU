gnerv2
Mon Jul  3 11:54:41 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   37C    P8    17W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   37C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   38C    P8    22W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   37C    P8    14W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 16%] Built target context
[ 36%] Built target core
[ 77%] Built target cudahelp
[ 88%] Built target OSDI2023_MULTI_NODES_gcn
[ 88%] Built target OSDI2023_MULTI_NODES_graphsage
[ 88%] Built target OSDI2023_MULTI_NODES_gcnii
[100%] Built target estimate_comm_volume
Initialized node 4 on machine gnerv1
Initialized node 7 on machine gnerv1
Initialized node 5 on machine gnerv1
Initialized node 6 on machine gnerv1
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 2 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 1.798 seconds.
Building the CSC structure...
        It takes 1.969 seconds.
Building the CSC structure...
        It takes 2.226 seconds.
Building the CSC structure...
        It takes 2.315 seconds.
Building the CSC structure...
        It takes 2.429 seconds.
Building the CSC structure...
        It takes 2.557 seconds.
Building the CSC structure...
        It takes 2.591 seconds.
Building the CSC structure...
        It takes 2.599 seconds.
Building the CSC structure...
        It takes 1.807 seconds.
        It takes 1.888 seconds.
        It takes 2.326 seconds.
Building the Feature Vector...
        It takes 2.374 seconds.
        It takes 2.307 seconds.
        It takes 0.275 seconds.
Building the Label Vector...
        It takes 2.301 seconds.
        It takes 2.284 seconds.
        It takes 0.041 seconds.
        It takes 2.321 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.281 seconds.
Building the Label Vector...
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
GPU 2, layer [4, 6)
GPU 3, layer [6, 8)
GPU 4, layer [8, 10)
GPU 5, layer [10, 12)
GPU 6, layer [12, 14)
GPU 7, layer [14, 16)
*** Node 3, starting model training...
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 3 owns the model-level partition [31, 41)
*** Node 3, constructing the helper classes...
        It takes 0.039 seconds.
        It takes 0.314 seconds.
Building the Label Vector...
        It takes 0.295 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
        It takes 0.042 seconds.
        It takes 0.285 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
232965, 114848857, 114848857
Number of vertices per chunk: 7281
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.241 seconds.
Building the Label Vector...
        It takes 0.030 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/reddit/32_parts
The number of GCN layers: 16
The number of hidden units: 128
The number of training epoches: 50
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 5
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 8
        It takes 0.282 seconds.
Building the Label Vector...
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
GPU 2, layer [4, 6)
GPU 3, layer [6, 8)
GPU 4, layer [8, 10)
GPU 5, layer [10, 12)
GPU 6, layer [12, 14)
GPU 7, layer [14, 16)
*** Node 5, starting model training...
Node 5, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 5, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 5 owns the model-level partition [51, 61)
*** Node 5, constructing the helper classes...
        It takes 0.030 seconds.
        It takes 0.279 seconds.
Building the Label Vector...
        It takes 0.037 seconds.
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
GPU 2, layer [4, 6)
GPU 3, layer [6, 8)
GPU 4, layer [8, 10)
GPU 5, layer [10, 12)
GPU 6, layer [12, 14)
GPU 7, layer [14, 16)
*** Node 2, starting model training...
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [21, 31)
*** Node 2, constructing the helper classes...
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
GPU 2, layer [4, 6)
GPU 3, layer [6, 8)
GPU 4, layer [8, 10)
GPU 5, layer [10, 12)
GPU 6, layer [12, 14)
GPU 7, layer [14, 16)
*** Node 1, starting model training...
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [11, 21)
*** Node 1, constructing the helper classes...
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
GPU 2, layer [4, 6)
GPU 3, layer [6, 8)
GPU 4, layer [8, 10)
GPU 5, layer [10, 12)
GPU 6, layer [12, 14)
GPU 7, layer [14, 16)
*** Node 7, starting model training...
Node 7, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [71, 80)
*** Node 7, constructing the helper classes...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
GPU 2, layer [4, 6)
GPU 3, layer [6, 8)
GPU 4, layer [8, 10)
GPU 5, layer [10, 12)
GPU 6, layer [12, 14)
GPU 7, layer [14, 16)
*** Node 0, starting model training...
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 11)
*** Node 0, constructing the helper classes...
Operators:
    Op 0: type OPERATOR_INPUT, output tensors: 0
    Op 1: type OPERATOR_WEIGHT, output tensors: 1
    Op 2: type OPERATOR_MATMUL, output tensors: 2
    Op 3: type OPERATOR_AGGREGATION, output tensors: 3
    Op 4: type OPERATOR_RELU, output tensors: 4
    Op 5: type OPERATOR_DROPOUT, output tensors: 5
    Op 6: type OPERATOR_AGGREGATION, output tensors: 6
    Op 7: type OPERATOR_WEIGHT, output tensors: 7
    Op 8: type OPERATOR_MATMUL, output tensors: 8
    Op 9: type OPERATOR_RELU, output tensors: 9
    Op 10: type OPERATOR_DROPOUT, output tensors: 10
    Op 11: type OPERATOR_AGGREGATION, output tensors: 11
    Op 12: type OPERATOR_WEIGHT, output tensors: 12
    Op 13: type OPERATOR_MATMUL, output tensors: 13
    Op 14: type OPERATOR_RELU, output tensors: 14
    Op 15: type OPERATOR_DROPOUT, output tensors: 15
    Op 16: type OPERATOR_AGGREGATION, output tensors: 16
    Op 17: type OPERATOR_WEIGHT, output tensors: 17
    Op 18: type OPERATOR_MATMUL, output tensors: 18
    Op 19: type OPERATOR_RELU, output tensors: 19
    Op 20: type OPERATOR_DROPOUT, output tensors: 20
    Op 21: type OPERATOR_AGGREGATION, output tensors: 21
    Op 22: type OPERATOR_WEIGHT, output tensors: 22
    Op 23: type OPERATOR_MATMUL, output tensors: 23
    Op 24: type OPERATOR_RELU, output tensors: 24
    Op 25: type OPERATOR_DROPOUT, output tensors: 25
    Op 26: type OPERATOR_AGGREGATION, output tensors: 26
    Op 27: type OPERATOR_WEIGHT, output tensors: 27
    Op 28: type OPERATOR_MATMUL, output tensors: 28
    Op 29: type OPERATOR_RELU, output tensors: 29
    Op 30: type OPERATOR_DROPOUT, output tensors: 30
    Op 31: type OPERATOR_AGGREGATION, output tensors: 31
    Op 32: type OPERATOR_WEIGHT, output tensors: 32
    Op 33: type OPERATOR_MATMUL, output tensors: 33
    Op 34: type OPERATOR_RELU, output tensors: 34
    Op 35: type OPERATOR_DROPOUT, output tensors: 35
    Op 36: type OPERATOR_AGGREGATION, output tensors: 36
    Op 37: type OPERATOR_WEIGHT, output tensors: 37
    Op 38: type OPERATOR_MATMUL, output tensors: 38
    Op 39: type OPERATOR_RELU, output tensors: 39
    Op 40: type OPERATOR_DROPOUT, output tensors: 40
    Op 41: type OPERATOR_AGGREGATION, output tensors: 41
    Op 42: type OPERATOR_WEIGHT, output tensors: 42
    Op 43: type OPERATOR_MATMUL, output tensors: 43
    Op 44: type OPERATOR_RELU, output tensors: 44
    Op 45: type OPERATOR_DROPOUT, output tensors: 45
    Op 46: type OPERATOR_AGGREGATION, output tensors: 46
    Op 47: type OPERATOR_WEIGHT, output tensors: 47
    Op 48: type OPERATOR_MATMUL, output tensors: 48
    Op 49: type OPERATOR_RELU, output tensors: 49
    Op 50: type OPERATOR_DROPOUT, output tensors: 50
    Op 51: type OPERATOR_AGGREGATION, output tensors: 51
    Op 52: type OPERATOR_WEIGHT, output tensors: 52
    Op 53: type OPERATOR_MATMUL, output tensors: 53
    Op 54: type OPERATOR_RELU, output tensors: 54
    Op 55: type OPERATOR_DROPOUT, output tensors: 55
    Op 56: type OPERATOR_AGGREGATION, output tensors: 56
    Op 57: type OPERATOR_WEIGHT, output tensors: 57
    Op 58: type OPERATOR_MATMUL, output tensors: 58
    Op 59: type OPERATOR_RELU, output tensors: 59
    Op 60: type OPERATOR_DROPOUT, output tensors: 60
    Op 61: type OPERATOR_AGGREGATION, output tensors: 61
    Op 62: type OPERATOR_WEIGHT, output tensors: 62
    Op 63: type OPERATOR_MATMUL, output tensors: 63
    Op 64: type OPERATOR_RELU, output tensors: 64
    Op 65: type OPERATOR_DROPOUT, output tensors: 65
    Op 66: type OPERATOR_AGGREGATION, output tensors: 66
    Op 67: type OPERATOR_WEIGHT, output tensors: 67
    Op 68: type OPERATOR_MATMUL, output tensors: 68
    Op 69: type OPERATOR_RELU, output tensors: 69
    Op 70: type OPERATOR_DROPOUT, output tensors: 70
    Op 71: type OPERATOR_AGGREGATION, output tensors: 71
    Op 72: type OPERATOR_WEIGHT, output tensors: 72
    Op 73: type OPERATOR_MATMUL, output tensors: 73
    Op 74: type OPERATOR_RELU, output tensors: 74
    Op 75: type OPERATOR_DROPOUT, output tensors: 75
    Op 76: type OPERATOR_AGGREGATION, output tensors: 76
    Op 77: type OPERATOR_WEIGHT, output tensors: 77
    Op 78: type OPERATOR_MATMUL, output tensors: 78
    Op 79: type OPERATOR_SOFTMAX, output tensors: 79
Chunks (number of global chunks: 32): 0-[0, 7497) 1-[7497, 14564) 2-[14564, 21631) 3-[21631, 28706) 4-[28706, 35773) 5-[35773, 43271) 6-[43271, 50769) 7-[50769, 57836) 8-[57836, 65334) ... 31-[225467, 232965)
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
GPU 2, layer [4, 6)
GPU 3, layer [6, 8)
GPU 4, layer [8, 10)
GPU 5, layer [10, 12)
GPU 6, layer [12, 14)
GPU 7, layer [14, 16)
*** Node 4, starting model training...
Node 4, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 4, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 4 owns the model-level partition [41, 51)
*** Node 4, constructing the helper classes...
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
GPU 2, layer [4, 6)
GPU 3, layer [6, 8)
GPU 4, layer [8, 10)
GPU 5, layer [10, 12)
GPU 6, layer [12, 14)
GPU 7, layer [14, 16)
*** Node 6, starting model training...
Node 6, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 6, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 6 owns the model-level partition [61, 71)
*** Node 6, constructing the helper classes...
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
232965, 114848857, 114848857
Number of vertices per chunk: 7281
csr in-out ready !csr in-out ready !csr in-out ready !csr in-out ready !csr in-out ready !csr in-out ready !csr in-out ready !csr in-out ready !*** Node 0, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 11)...
+++++++++ Node 0, mapping weight op 1
+++++++++ Node 1 initializing the weights for op[11, 21)...
+++++++++ Node 1, mapping weight op 12
+++++++++ Node 2 initializing the weights for op[21, 31)...
+++++++++ Node 2, mapping weight op 22
+++++++++ Node 3 initializing the weights for op[31, 41)...
+++++++++ Node 3, mapping weight op 32
+++++++++ Node 1, mapping weight op 17
+++++++++ Node 2, mapping weight op 27
+++++++++ Node 3, mapping weight op 37
+++++++++ Node 4 initializing the weights for op[41, 51)...
+++++++++ Node 4, mapping weight op 42
+++++++++ Node 5 initializing the weights for op[51, 61)...
+++++++++ Node 5, mapping weight op 52
+++++++++ Node 5, mapping weight op 57
+++++++++ Node 6 initializing the weights for op[61, 71)...
+++++++++ Node 6, mapping weight op 62
+++++++++ Node 6, mapping weight op 67
+++++++++ Node 7 initializing the weights for op[71, 80)...
+++++++++ Node 7, mapping weight op 72
+++++++++ Node 7, mapping weight op 77
+++++++++ Node 4, mapping weight op 47
+++++++++ Node 0, mapping weight op 7
Node 0, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 7, starting task scheduling...
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 10:	Loss 3.4653	TrainAcc 0.0690	ValidAcc 0.0584	BestValid 0.0584
	Epoch 20:	Loss 3.4935	TrainAcc 0.0690	ValidAcc 0.0584	BestValid 0.0584
	Epoch 30:	Loss 3.4332	TrainAcc 0.0690	ValidAcc 0.0584	BestValid 0.0584
	Epoch 40:	Loss 3.3792	TrainAcc 0.0690	ValidAcc 0.0584	BestValid 0.0584
	Epoch 50:	Loss 3.3592	TrainAcc 0.0690	ValidAcc 0.0584	BestValid 0.0584
Node 0, GPU memory consumption: 5.204 GB
Node 2, GPU memory consumption: 4.533 GB
Node 1, GPU memory consumption: 4.533 GB
Node 7, GPU memory consumption: 4.247 GB
Node 3, GPU memory consumption: 4.486 GB
Node 4, GPU memory consumption: 4.488 GB
Node 6, GPU memory consumption: 4.533 GB
Node 5, GPU memory consumption: 4.533 GB
Node 0, Layer-Level Communication Throughput: 56.715 Gbps, Time 21.872 ms
Node 4, Layer-Level Communication Throughput: 62.722 Gbps, Time 34.991 ms
Node 1, Layer-Level Communication Throughput: 59.930 Gbps, Time 36.621 ms
Node 5, Layer-Level Communication Throughput: 59.971 Gbps, Time 36.596 ms
Node 2, Layer-Level Communication Throughput: 56.198 Gbps, Time 39.053 ms
Node 6, Layer-Level Communication Throughput: 65.723 Gbps, Time 33.393 ms
Node 3, Layer-Level Communication Throughput: 66.038 Gbps, Time 33.234 ms
Node 7, Layer-Level Communication Throughput: 44.120 Gbps, Time 21.628 ms
Node 0, Graph-Level Communication Throughput: 0.000 Gbps, Time: 0.134 ms
Node 1, Graph-Level Communication Throughput: 0.000 Gbps, Time: 0.136 ms
Node 2, Graph-Level Communication Throughput: 0.000 Gbps, Time: 0.131 ms
Node 3, Graph-Level Communication Throughput: 0.000 Gbps, Time: 0.130 ms
Node 4, Graph-Level Communication Throughput: 0.000 Gbps, Time: 0.133 ms
Node 5, Graph-Level Communication Throughput: 0.000 Gbps, Time: 0.138 ms
Node 6, Graph-Level Communication Throughput: 0.000 Gbps, Time: 0.136 ms
Node 7, Graph-Level Communication Throughput: 0.000 Gbps, Time: 0.137 ms
------------------------node id 0,  per-epoch time: 0.371506 s---------------
------------------------node id 1,  per-epoch time: 0.371505 s---------------
------------------------node id 2,  per-epoch time: 0.371505 s---------------
------------------------node id 3,  per-epoch time: 0.371506 s---------------
------------------------node id 4,  per-epoch time: 0.371505 s---------------
------------------------node id 5,  per-epoch time: 0.371505 s---------------
------------------------node id 6,  per-epoch time: 0.371505 s---------------
------------------------node id 7,  per-epoch time: 0.371505 s---------------
************ Profiling Results ************
	Bubble: 181.248570 (ms) (47.58 percentage)
	Compute: 154.558905 (ms) (40.58 percentage)
	GraphCommComputeOverhead: 0.262610 (ms) (0.07 percentage)
	GraphCommNetwork: 0.137905 (ms) (0.04 percentage)
	LayerCommNetwork: 32.173647 (ms) (8.45 percentage)
	Optimization: 7.348549 (ms) (1.93 percentage)
	Other: 5.182256 (ms) (1.36 percentage)
	Layer-level communication (cluster-wide, per-epoch): 1.788 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.009 GB
	Total communication (cluster-wide, per-epoch): 1.798 GB
Highest valid_acc: 0.0584
Target test_acc: 0.0574
Epoch to reach the target acc: 10
[MPI Rank 0] Success 
[MPI Rank 4] Success 
[MPI Rank 1] Success 
[MPI Rank 5] Success 
[MPI Rank 2] Success 
[MPI Rank 6] Success 
[MPI Rank 3] Success 
[MPI Rank 7] Success 
