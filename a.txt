gnerv1
Tue Aug  8 18:39:33 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   29C    P8    25W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   29C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   29C    P8    19W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   28C    P8    22W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 25%] Built target context
[ 36%] Built target core
[ 77%] Built target cudahelp
[ 94%] Built target estimate_comm_volume
[ 94%] Built target OSDI2023_MULTI_NODES_graphsage
[ 94%] Built target OSDI2023_MULTI_NODES_gcnii
[100%] Built target OSDI2023_MULTI_NODES_gcn
Running experiments...
gnerv2
gnerv2
gnerv2
gnerv2
gnerv3
gnerv3
gnerv3
gnerv3
Initialized node 6 on machine gnerv3
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 7 on machine gnerv3
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 0 on machine gnerv2
Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
Building the CSC structure...
        It takes 0.019 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
Building the CSC structure...
        It takes 0.021 seconds.
Building the CSC structure...
        It takes 0.026 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
        It takes 0.016 seconds.
        It takes 0.022 seconds.
        It takes 0.021 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
        It takes 0.025 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.029 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/flickr/8_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.109 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.110 seconds.
Building the Label Vector...
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.109 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.109 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 32)
GPU 0, layer [0, 32)
89250, 989006, 989006
Number of vertices per chunk: 11157
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
csr in-out ready !Start Cost Model Initialization...
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 32)
Chunks (number of global chunks: 8): 0-[0, 11288) 1-[11288, 22247) 2-[22247, 33374) 3-[33374, 44530) 4-[44530, 55727) 5-[55727, 66814) 6-[66814, 78011) 7-[78011, 89250)
GPU 0, layer [0, 32)
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
GPU 0, layer [0, 32)
csr in-out ready !Start Cost Model Initialization...
89250, 989006, 989006
Number of vertices per chunk: 11157
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 60.098 Gbps (per GPU), 480.781 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.809 Gbps (per GPU), 478.469 Gbps (aggregated)
The layer-level communication performance: 59.813 Gbps (per GPU), 478.505 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.560 Gbps (per GPU), 476.477 Gbps (aggregated)
The layer-level communication performance: 59.519 Gbps (per GPU), 476.150 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.305 Gbps (per GPU), 474.436 Gbps (aggregated)
The layer-level communication performance: 59.256 Gbps (per GPU), 474.052 Gbps (aggregated)
The layer-level communication performance: 59.225 Gbps (per GPU), 473.798 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.812 Gbps (per GPU), 1246.499 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.827 Gbps (per GPU), 1246.612 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.812 Gbps (per GPU), 1246.496 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.766 Gbps (per GPU), 1246.126 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.815 Gbps (per GPU), 1246.519 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.836 Gbps (per GPU), 1246.684 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.766 Gbps (per GPU), 1246.126 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.815 Gbps (per GPU), 1246.519 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 79.808 Gbps (per GPU), 638.463 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 79.810 Gbps (per GPU), 638.479 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 79.808 Gbps (per GPU), 638.463 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 79.812 Gbps (per GPU), 638.496 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 79.808 Gbps (per GPU), 638.463 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 79.789 Gbps (per GPU), 638.313 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 79.807 Gbps (per GPU), 638.455 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 79.772 Gbps (per GPU), 638.180 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 30.930 Gbps (per GPU), 247.438 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.929 Gbps (per GPU), 247.431 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.924 Gbps (per GPU), 247.390 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.930 Gbps (per GPU), 247.442 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.923 Gbps (per GPU), 247.387 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.928 Gbps (per GPU), 247.424 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.924 Gbps (per GPU), 247.394 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 30.926 Gbps (per GPU), 247.410 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.77ms  0.57ms  0.38ms  2.05 11.29K  0.12M
 chk_1  0.73ms  0.65ms  0.34ms  2.12 10.96K  0.11M
 chk_2  0.77ms  0.57ms  0.38ms  2.04 11.13K  0.12M
 chk_3  0.72ms  0.52ms  0.33ms  2.19 11.16K  0.11M
 chk_4  0.76ms  0.55ms  0.36ms  2.09 11.20K  0.11M
 chk_5  0.72ms  0.54ms  0.35ms  2.05 11.09K  0.10M
 chk_6  0.72ms  0.53ms  0.34ms  2.10 11.20K  0.11M
 chk_7  0.75ms  0.57ms  0.38ms  2.00 11.24K  0.12M
   Avg  0.74  0.56  0.36
   Max  0.77  0.65  0.38
   Min  0.72  0.52  0.33
 Ratio  1.08  1.25  1.15
   Var  0.00  0.00  0.00
Profiling takes 0.220 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 19.475 ms
Partition 0 [0, 4) has cost: 19.475 ms
Partition 1 [4, 8) has cost: 18.047 ms
Partition 2 [8, 12) has cost: 18.047 ms
Partition 3 [12, 16) has cost: 18.047 ms
Partition 4 [16, 20) has cost: 18.047 ms
Partition 5 [20, 24) has cost: 18.047 ms
Partition 6 [24, 28) has cost: 18.047 ms
Partition 7 [28, 32) has cost: 16.394 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 20.372 ms
GPU 0, Compute+Comm Time: 11.074 ms, Bubble Time: 9.298 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 10.468 ms, Bubble Time: 9.379 ms, Imbalance Overhead: 0.524 ms
GPU 2, Compute+Comm Time: 10.468 ms, Bubble Time: 9.394 ms, Imbalance Overhead: 0.509 ms
GPU 3, Compute+Comm Time: 10.468 ms, Bubble Time: 9.463 ms, Imbalance Overhead: 0.440 ms
GPU 4, Compute+Comm Time: 10.468 ms, Bubble Time: 9.474 ms, Imbalance Overhead: 0.429 ms
GPU 5, Compute+Comm Time: 10.468 ms, Bubble Time: 9.530 ms, Imbalance Overhead: 0.373 ms
GPU 6, Compute+Comm Time: 10.468 ms, Bubble Time: 9.566 ms, Imbalance Overhead: 0.337 ms
GPU 7, Compute+Comm Time: 9.959 ms, Bubble Time: 9.661 ms, Imbalance Overhead: 0.752 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 34.693 ms
GPU 0, Compute+Comm Time: 15.939 ms, Bubble Time: 17.266 ms, Imbalance Overhead: 1.488 ms
GPU 1, Compute+Comm Time: 17.083 ms, Bubble Time: 16.771 ms, Imbalance Overhead: 0.839 ms
GPU 2, Compute+Comm Time: 17.083 ms, Bubble Time: 16.406 ms, Imbalance Overhead: 1.204 ms
GPU 3, Compute+Comm Time: 17.083 ms, Bubble Time: 16.040 ms, Imbalance Overhead: 1.570 ms
GPU 4, Compute+Comm Time: 17.083 ms, Bubble Time: 15.675 ms, Imbalance Overhead: 1.935 ms
GPU 5, Compute+Comm Time: 17.083 ms, Bubble Time: 15.309 ms, Imbalance Overhead: 2.301 ms
GPU 6, Compute+Comm Time: 17.083 ms, Bubble Time: 14.942 ms, Imbalance Overhead: 2.668 ms
GPU 7, Compute+Comm Time: 17.905 ms, Bubble Time: 14.928 ms, Imbalance Overhead: 1.860 ms
The estimated cost of the whole pipeline: 57.818 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 37.522 ms
Partition 0 [0, 8) has cost: 37.522 ms
Partition 1 [8, 16) has cost: 36.094 ms
Partition 2 [16, 24) has cost: 36.094 ms
Partition 3 [24, 32) has cost: 34.441 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 25.334 ms
GPU 0, Compute+Comm Time: 14.587 ms, Bubble Time: 10.717 ms, Imbalance Overhead: 0.031 ms
GPU 1, Compute+Comm Time: 14.283 ms, Bubble Time: 10.806 ms, Imbalance Overhead: 0.245 ms
GPU 2, Compute+Comm Time: 14.283 ms, Bubble Time: 10.818 ms, Imbalance Overhead: 0.233 ms
GPU 3, Compute+Comm Time: 14.030 ms, Bubble Time: 10.948 ms, Imbalance Overhead: 0.356 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 39.003 ms
GPU 0, Compute+Comm Time: 20.847 ms, Bubble Time: 17.165 ms, Imbalance Overhead: 0.991 ms
GPU 1, Compute+Comm Time: 21.471 ms, Bubble Time: 16.306 ms, Imbalance Overhead: 1.226 ms
GPU 2, Compute+Comm Time: 21.471 ms, Bubble Time: 15.574 ms, Imbalance Overhead: 1.958 ms
GPU 3, Compute+Comm Time: 21.833 ms, Bubble Time: 15.922 ms, Imbalance Overhead: 1.249 ms
    The estimated cost with 2 DP ways is 67.554 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 73.615 ms
Partition 0 [0, 16) has cost: 73.615 ms
Partition 1 [16, 32) has cost: 70.535 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 48.573 ms
GPU 0, Compute+Comm Time: 32.500 ms, Bubble Time: 16.074 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 32.220 ms, Bubble Time: 16.287 ms, Imbalance Overhead: 0.067 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 61.012 ms
GPU 0, Compute+Comm Time: 39.898 ms, Bubble Time: 19.533 ms, Imbalance Overhead: 1.580 ms
GPU 1, Compute+Comm Time: 40.391 ms, Bubble Time: 20.621 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 115.064 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 144.151 ms
Partition 0 [0, 32) has cost: 144.151 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 136.744 ms
GPU 0, Compute+Comm Time: 136.744 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 145.944 ms
GPU 0, Compute+Comm Time: 145.944 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 296.822 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 160)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 44530, Num Local Vertices: 11197
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 160)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 11288
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 160)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 55727, Num Local Vertices: 11087
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 160)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 11288, Num Local Vertices: 10959
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 160)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 66814, Num Local Vertices: 11197
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 160)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 22247, Num Local Vertices: 11127
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 160)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 78011, Num Local Vertices: 11239
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 160)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 33374, Num Local Vertices: 11156
*** Node 2, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 160)...
+++++++++ Node 0 initializing the weights for op[0, 160)...
+++++++++ Node 4 initializing the weights for op[0, 160)...
+++++++++ Node 3 initializing the weights for op[0, 160)...
+++++++++ Node 6 initializing the weights for op[0, 160)...
+++++++++ Node 2 initializing the weights for op[0, 160)...
+++++++++ Node 7 initializing the weights for op[0, 160)...
+++++++++ Node 1 initializing the weights for op[0, 160)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 223317
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...



*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 1.9458
	Epoch 50:	Loss 1.7303
	Epoch 75:	Loss 1.6377
	Epoch 100:	Loss 1.6279
Node 0, Pre/Post-Pipelining: 3.484 / 5.258 ms, Bubble: 0.213 ms, Compute: 161.569 ms, Comm: 0.007 ms, Imbalance: 0.014 ms
Node 1, Pre/Post-Pipelining: 3.484 / 5.273 ms, Bubble: 0.223 ms, Compute: 161.537 ms, Comm: 0.008 ms, Imbalance: 0.018 ms
Node 4, Pre/Post-Pipelining: 3.625 / 5.288 ms, Bubble: 0.219 ms, Compute: 161.391 ms, Comm: 0.008 ms, Imbalance: 0.016 ms
Node 2, Pre/Post-Pipelining: 3.487 / 5.277 ms, Bubble: 0.035 ms, Compute: 161.719 ms, Comm: 0.007 ms, Imbalance: 0.016 ms
Node 5, Pre/Post-Pipelining: 3.622 / 5.278 ms, Bubble: 0.255 ms, Compute: 161.366 ms, Comm: 0.008 ms, Imbalance: 0.017 ms
Node 3, Pre/Post-Pipelining: 3.482 / 5.248 ms, Bubble: 0.263 ms, Compute: 161.523 ms, Comm: 0.007 ms, Imbalance: 0.017 ms
Node 6, Pre/Post-Pipelining: 3.606 / 5.289 ms, Bubble: 0.159 ms, Compute: 161.465 ms, Comm: 0.008 ms, Imbalance: 0.018 ms
Node 7, Pre/Post-Pipelining: 3.624 / 5.293 ms, Bubble: 0.165 ms, Compute: 161.437 ms, Comm: 0.008 ms, Imbalance: 0.018 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 3.484 ms
Cluster-Wide Average, Post-Pipelining Overhead: 5.258 ms
Cluster-Wide Average, Bubble: 0.213 ms
Cluster-Wide Average, Compute: 161.569 ms
Cluster-Wide Average, Communication: 0.007 ms
Cluster-Wide Average, Imbalance: 0.014 ms
Node 5, GPU memory consumption: 6.024 GB
Node 0, GPU memory consumption: 6.428 GB
Node 4, GPU memory consumption: 6.001 GB
Node 3, GPU memory consumption: 6.001 GB
Node 7, GPU memory consumption: 6.001 GB
Node 1, GPU memory consumption: 6.024 GB
Node 6, GPU memory consumption: 6.024 GB
Node 2, GPU memory consumption: 6.024 GB
Node 0, Graph-Level Communication Throughput: 48.279 Gbps, Time: 130.466 ms
Node 4, Graph-Level Communication Throughput: 49.196 Gbps, Time: 129.692 ms
Node 1, Graph-Level Communication Throughput: 39.859 Gbps, Time: 131.321 ms
Node 5, Graph-Level Communication Throughput: 47.982 Gbps, Time: 130.323 ms
Node 2, Graph-Level Communication Throughput: 47.343 Gbps, Time: 129.734 ms
Node 6, Graph-Level Communication Throughput: 37.613 Gbps, Time: 131.510 ms
Node 3, Graph-Level Communication Throughput: 31.055 Gbps, Time: 132.965 ms
Node 7, Graph-Level Communication Throughput: 48.993 Gbps, Time: 129.632 ms
------------------------node id 4,  per-epoch time: 0.170592 s---------------
------------------------node id 0,  per-epoch time: 0.170591 s---------------
------------------------node id 5,  per-epoch time: 0.170592 s---------------
------------------------node id 1,  per-epoch time: 0.170591 s---------------
------------------------node id 6,  per-epoch time: 0.170592 s---------------
------------------------node id 2,  per-epoch time: 0.170591 s---------------
------------------------node id 7,  per-epoch time: 0.170592 s---------------
------------------------node id 3,  per-epoch time: 0.170591 s---------------
************ Profiling Results ************
	Bubble: 9.367570 (ms) (5.44 percentage)
	Compute: 16.875937 (ms) (9.81 percentage)
	GraphCommComputeOverhead: 8.264713 (ms) (4.80 percentage)
	GraphCommNetwork: 130.707114 (ms) (75.97 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 6.838301 (ms) (3.97 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 5.324 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.018 GB
	Total communication (cluster-wide, per-epoch): 5.343 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0607
Epoch to reach the target acc: 0
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
