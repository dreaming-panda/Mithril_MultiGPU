gnerv1
Mon Aug  7 01:35:10 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   28C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   28C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   27C    P8    19W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   27C    P8    22W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 19%] Built target context
[ 36%] Built target core
[ 77%] Built target cudahelp
[ 91%] Built target estimate_comm_volume
[ 91%] Built target OSDI2023_MULTI_NODES_gcn
[100%] Built target OSDI2023_MULTI_NODES_gcnii
[100%] Built target OSDI2023_MULTI_NODES_graphsage
Running experiments...
gnerv2
gnerv2
gnerv2
gnerv2
gnerv3
gnerv3
gnerv3
gnerv3
Initialized node 0 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
Building the CSC structure...
        It takes 0.014 seconds.
Building the CSC structure...
        It takes 0.015 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
        It takes 0.008 seconds.
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.008 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.011 seconds.
        It takes 0.010 seconds.
        It takes 0.009 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.014 seconds.
        It takes 0.013 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.023 seconds.
Building the Label Vector...
        It takes 0.001 seconds.
        It takes 0.024 seconds.
        It takes 0.032 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.027 seconds.
Building the Label Vector...
        It takes 0.001 seconds.
        It takes 0.001 seconds.
        It takes 0.001 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/amazon_computers/8_parts
The number of GCNII layers: 32
The number of hidden units: 1024
The number of training epoches: 100
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 10
Number of feature dimensions: 767
Number of vertices: 13752
Number of GPUs: 8
        It takes 0.027 seconds.
Building the Label Vector...
        It takes 0.034 seconds.
Building the Label Vector...
        It takes 0.001 seconds.
        It takes 0.025 seconds.
Building the Label Vector...
        It takes 0.025 seconds.
        It takes 0.001 seconds.
Building the Label Vector...
        It takes 0.001 seconds.
        It takes 0.001 seconds.
GPU 0, layer [0, 33)
GPU 0, layer [0, 33)
13752, 505474, 505474
Number of vertices per chunk: 1719
13752, 505474, 505474
Number of vertices per chunk: 1719
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
csr in-out ready !Start Cost Model Initialization...
13752, 505474, 505474
Number of vertices per chunk: 1719
GPU 0, layer [0, 33)
GPU 0, layer [0, 33)
csr in-out ready !Start Cost Model Initialization...
13752, 505474, 505474
Number of vertices per chunk: 1719
13752, 505474, 505474
Number of vertices per chunk: 1719
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 33)
train nodes 200, valid nodes 500, test nodes 1000
GPU 0, layer [0, 33)
Chunks (number of global chunks: 8): 0-[0, 1810) 1-[1810, 3476) 2-[3476, 4974) 3-[4974, 6884) 4-[6884, 8712) 5-[8712, 10469) 6-[10469, 12120) 7-[12120, 13752)
GPU 0, layer [0, 33)
13752, 505474, 505474
Number of vertices per chunk: 1719
13752, 505474, 505474
Number of vertices per chunk: 1719
13752, 505474, 505474
Number of vertices per chunk: 1719
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.768 Gbps (per GPU), 478.144 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.479 Gbps (per GPU), 475.832 Gbps (aggregated)
The layer-level communication performance: 59.469 Gbps (per GPU), 475.749 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.190 Gbps (per GPU), 473.521 Gbps (aggregated)
The layer-level communication performance: 59.227 Gbps (per GPU), 473.820 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.992 Gbps (per GPU), 471.935 Gbps (aggregated)
The layer-level communication performance: 58.946 Gbps (per GPU), 471.569 Gbps (aggregated)
The layer-level communication performance: 58.911 Gbps (per GPU), 471.290 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.396 Gbps (per GPU), 1243.171 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.396 Gbps (per GPU), 1243.171 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.376 Gbps (per GPU), 1243.009 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.391 Gbps (per GPU), 1243.125 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.371 Gbps (per GPU), 1242.968 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.396 Gbps (per GPU), 1243.171 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.382 Gbps (per GPU), 1243.056 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.394 Gbps (per GPU), 1243.148 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 100.502 Gbps (per GPU), 804.020 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.507 Gbps (per GPU), 804.058 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.503 Gbps (per GPU), 804.026 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.507 Gbps (per GPU), 804.052 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.506 Gbps (per GPU), 804.046 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.494 Gbps (per GPU), 803.956 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.503 Gbps (per GPU), 804.026 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 100.504 Gbps (per GPU), 804.033 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 35.340 Gbps (per GPU), 282.717 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.340 Gbps (per GPU), 282.717 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.333 Gbps (per GPU), 282.661 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.335 Gbps (per GPU), 282.680 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.336 Gbps (per GPU), 282.689 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.335 Gbps (per GPU), 282.681 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.328 Gbps (per GPU), 282.626 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 35.327 Gbps (per GPU), 282.615 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.12ms  1.95ms  2.15ms  1.93  1.81K  0.06M
 chk_1  1.04ms  1.89ms  2.08ms  1.99  1.67K  0.07M
 chk_2  0.94ms  1.79ms  1.97ms  2.09  1.50K  0.07M
 chk_3  1.09ms  1.90ms  2.11ms  1.93  1.91K  0.05M
 chk_4  1.12ms  1.95ms  2.15ms  1.92  1.83K  0.05M
 chk_5  1.07ms  1.91ms  2.11ms  1.97  1.76K  0.06M
 chk_6  1.04ms  1.88ms  2.08ms  2.00  1.65K  0.07M
 chk_7  1.03ms  1.87ms  2.06ms  1.99  1.63K  0.07M
   Avg  1.06  1.89  2.09
   Max  1.12  1.95  2.15
   Min  0.94  1.79  1.97
 Ratio  1.19  1.09  1.09
   Var  0.00  0.00  0.00
Profiling takes 0.538 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 69.077 ms
Partition 0 [0, 5) has cost: 69.077 ms
Partition 1 [5, 9) has cost: 60.611 ms
Partition 2 [9, 13) has cost: 60.611 ms
Partition 3 [13, 17) has cost: 60.611 ms
Partition 4 [17, 21) has cost: 60.611 ms
Partition 5 [21, 25) has cost: 60.611 ms
Partition 6 [25, 29) has cost: 60.611 ms
Partition 7 [29, 33) has cost: 62.171 ms
The optimal partitioning:
[0, 5)
[5, 9)
[9, 13)
[13, 17)
[17, 21)
[21, 25)
[25, 29)
[29, 33)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 51.109 ms
GPU 0, Compute+Comm Time: 27.981 ms, Bubble Time: 23.065 ms, Imbalance Overhead: 0.063 ms
GPU 1, Compute+Comm Time: 25.654 ms, Bubble Time: 23.363 ms, Imbalance Overhead: 2.092 ms
GPU 2, Compute+Comm Time: 25.654 ms, Bubble Time: 23.520 ms, Imbalance Overhead: 1.935 ms
GPU 3, Compute+Comm Time: 25.654 ms, Bubble Time: 23.512 ms, Imbalance Overhead: 1.943 ms
GPU 4, Compute+Comm Time: 25.654 ms, Bubble Time: 23.663 ms, Imbalance Overhead: 1.792 ms
GPU 5, Compute+Comm Time: 25.654 ms, Bubble Time: 23.999 ms, Imbalance Overhead: 1.457 ms
GPU 6, Compute+Comm Time: 25.654 ms, Bubble Time: 24.343 ms, Imbalance Overhead: 1.112 ms
GPU 7, Compute+Comm Time: 25.991 ms, Bubble Time: 24.608 ms, Imbalance Overhead: 0.509 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 101.503 ms
GPU 0, Compute+Comm Time: 51.259 ms, Bubble Time: 49.307 ms, Imbalance Overhead: 0.938 ms
GPU 1, Compute+Comm Time: 50.037 ms, Bubble Time: 48.672 ms, Imbalance Overhead: 2.795 ms
GPU 2, Compute+Comm Time: 50.037 ms, Bubble Time: 47.881 ms, Imbalance Overhead: 3.586 ms
GPU 3, Compute+Comm Time: 50.037 ms, Bubble Time: 47.052 ms, Imbalance Overhead: 4.415 ms
GPU 4, Compute+Comm Time: 50.037 ms, Bubble Time: 46.426 ms, Imbalance Overhead: 5.040 ms
GPU 5, Compute+Comm Time: 50.037 ms, Bubble Time: 46.569 ms, Imbalance Overhead: 4.898 ms
GPU 6, Compute+Comm Time: 50.037 ms, Bubble Time: 46.121 ms, Imbalance Overhead: 5.345 ms
GPU 7, Compute+Comm Time: 56.175 ms, Bubble Time: 45.328 ms, Imbalance Overhead: 0.000 ms
The estimated cost of the whole pipeline: 160.243 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 129.688 ms
Partition 0 [0, 9) has cost: 129.688 ms
Partition 1 [9, 17) has cost: 121.222 ms
Partition 2 [17, 25) has cost: 121.222 ms
Partition 3 [25, 33) has cost: 122.782 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 57.509 ms
GPU 0, Compute+Comm Time: 33.029 ms, Bubble Time: 24.219 ms, Imbalance Overhead: 0.261 ms
GPU 1, Compute+Comm Time: 31.847 ms, Bubble Time: 24.663 ms, Imbalance Overhead: 0.998 ms
GPU 2, Compute+Comm Time: 31.847 ms, Bubble Time: 24.851 ms, Imbalance Overhead: 0.811 ms
GPU 3, Compute+Comm Time: 32.020 ms, Bubble Time: 24.944 ms, Imbalance Overhead: 0.545 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 103.257 ms
GPU 0, Compute+Comm Time: 57.244 ms, Bubble Time: 44.713 ms, Imbalance Overhead: 1.301 ms
GPU 1, Compute+Comm Time: 56.617 ms, Bubble Time: 44.747 ms, Imbalance Overhead: 1.893 ms
GPU 2, Compute+Comm Time: 56.617 ms, Bubble Time: 44.284 ms, Imbalance Overhead: 2.356 ms
GPU 3, Compute+Comm Time: 59.767 ms, Bubble Time: 43.343 ms, Imbalance Overhead: 0.147 ms
    The estimated cost with 2 DP ways is 168.805 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 250.910 ms
Partition 0 [0, 17) has cost: 250.910 ms
Partition 1 [17, 33) has cost: 244.004 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 83.880 ms
GPU 0, Compute+Comm Time: 55.891 ms, Bubble Time: 27.989 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 55.372 ms, Bubble Time: 27.649 ms, Imbalance Overhead: 0.858 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 122.666 ms
GPU 0, Compute+Comm Time: 80.754 ms, Bubble Time: 40.806 ms, Imbalance Overhead: 1.106 ms
GPU 1, Compute+Comm Time: 82.060 ms, Bubble Time: 40.606 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 216.873 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 494.915 ms
Partition 0 [0, 33) has cost: 494.915 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 217.363 ms
GPU 0, Compute+Comm Time: 217.363 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 243.284 ms
GPU 0, Compute+Comm Time: 243.284 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 483.680 ms

*** Node 4, starting model training...
Num Stages: 1 / 1
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 1810
*** Node 5, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 233)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 8712, Num Local Vertices: 1757
*** Node 1, starting model training...
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 1810, Num Local Vertices: 1666
*** Node 6, starting model training...
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 233)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 10469, Num Local Vertices: 1651
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 3476, Num Local Vertices: 1498
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 233)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 12120, Num Local Vertices: 1632
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 4974, Num Local Vertices: 1910
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 233)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 6884, Num Local Vertices: 1828
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
+++++++++ Node 5 initializing the weights for op[0, 233)...
+++++++++ Node 4 initializing the weights for op[0, 233)...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 6 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 7 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 36329
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 0.8206
	Epoch 50:	Loss 0.3640
	Epoch 75:	Loss 0.3659
Node 5, Pre/Post-Pipelining: 5.890 / 135.092 ms, Bubble: 0.334 ms, Compute: 303.748 ms, Comm: 0.009 ms, Imbalance: 0.015 ms
Node 3, Pre/Post-Pipelining: 5.882 / 134.954 ms, Bubble: 0.446 ms, Compute: 303.781 ms, Comm: 0.009 ms, Imbalance: 0.013 ms
Node 4, Pre/Post-Pipelining: 5.884 / 135.132 ms, Bubble: 0.261 ms, Compute: 303.782 ms, Comm: 0.010 ms, Imbalance: 0.016 ms
	Epoch 100:	Loss 0.1927
Node 0, Pre/Post-Pipelining: 5.886 / 135.149 ms, Bubble: 0.185 ms, Compute: 303.841 ms, Comm: 0.009 ms, Imbalance: 0.016 ms
Node 6, Pre/Post-Pipelining: 5.890 / 135.142 ms, Bubble: 0.230 ms, Compute: 303.799 ms, Comm: 0.009 ms, Imbalance: 0.014 ms
Node 1, Pre/Post-Pipelining: 5.887 / 135.118 ms, Bubble: 0.259 ms, Compute: 303.798 ms, Comm: 0.010 ms, Imbalance: 0.016 ms
Node 7, Pre/Post-Pipelining: 5.886 / 135.174 ms, Bubble: 0.240 ms, Compute: 303.765 ms, Comm: 0.010 ms, Imbalance: 0.014 ms
Node 2, Pre/Post-Pipelining: 5.887 / 135.306 ms, Bubble: 0.041 ms, Compute: 303.833 ms, Comm: 0.009 ms, Imbalance: 0.014 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 5.886 ms
Cluster-Wide Average, Post-Pipelining Overhead: 135.149 ms
Cluster-Wide Average, Bubble: 0.185 ms
Cluster-Wide Average, Compute: 303.841 ms
Cluster-Wide Average, Communication: 0.009 ms
Cluster-Wide Average, Imbalance: 0.016 ms
Node 4, GPU memory consumption: 9.997 GB
Node 0, GPU memory consumption: 10.571 GB
Node 5, GPU memory consumption: 10.018 GB
Node 1, GPU memory consumption: 10.018 GB
Node 6, GPU memory consumption: 10.018 GB
Node 2, GPU memory consumption: 9.887 GB
Node 7, GPU memory consumption: 9.995 GB
Node 3, GPU memory consumption: 9.997 GB
Node 4, Graph-Level Communication Throughput: 30.223 Gbps, Time: 226.554 ms
Node 0, Graph-Level Communication Throughput: 43.333 Gbps, Time: 224.073 ms
Node 5, Graph-Level Communication Throughput: 34.220 Gbps, Time: 226.259 ms
Node 1, Graph-Level Communication Throughput: 44.767 Gbps, Time: 226.920 ms
Node 6, Graph-Level Communication Throughput: 43.852 Gbps, Time: 227.738 ms
Node 2, Graph-Level Communication Throughput: 44.688 Gbps, Time: 228.638 ms
Node 7, Graph-Level Communication Throughput: 48.952 Gbps, Time: 226.627 ms
Node 3, Graph-Level Communication Throughput: 46.292 Gbps, Time: 225.337 ms
------------------------node id 4,  per-epoch time: 0.445232 s---------------
------------------------node id 0,  per-epoch time: 0.445234 s---------------
------------------------node id 5,  per-epoch time: 0.445231 s---------------
------------------------node id 1,  per-epoch time: 0.445234 s---------------
------------------------node id 6,  per-epoch time: 0.445232 s---------------
------------------------node id 2,  per-epoch time: 0.445234 s---------------
------------------------node id 7,  per-epoch time: 0.445232 s---------------
------------------------node id 3,  per-epoch time: 0.445235 s---------------
************ Profiling Results ************
	Bubble: 142.938493 (ms) (31.92 percentage)
	Compute: 55.502334 (ms) (12.39 percentage)
	GraphCommComputeOverhead: 11.688804 (ms) (2.61 percentage)
	GraphCommNetwork: 226.517693 (ms) (50.58 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 11.189383 (ms) (2.50 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.000 GB
	Graph-level communication (cluster-wide, per-epoch): 8.869 GB
	Weight-sync communication (cluster-wide, per-epoch): 1.024 GB
	Total communication (cluster-wide, per-epoch): 9.893 GB
	Aggregated layer-level communication throughput: 0.000 Gbps
Highest valid_acc: 0.0000
Target test_acc: 0.0260
Epoch to reach the target acc: 0
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
