Wed Sep 20 18:32:15 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   32C    P8    16W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   33C    P8    24W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   34C    P8    20W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   33C    P8    14W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 20%] Built target context
[ 20%] Built target core
[ 70%] Built target cudahelp
[ 85%] Built target OSDI2023_MULTI_NODES_resgcn
[ 85%] Built target OSDI2023_MULTI_NODES_gcnii
[ 85%] Built target OSDI2023_MULTI_NODES_gcn
[ 90%] Built target OSDI2023_MULTI_NODES_resgcn_plus
[ 90%] Built target OSDI2023_MULTI_NODES_graphsage
[100%] Built target estimate_comm_volume
Running experiments...
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 5 on machine gnerv3
DONE MPI INIT
Initialized node 6 on machine gnerv3
DONE MPI INIT
Initialized node 7 on machine gnerv3

Initialized node 4 on machine gnerv3
DONE MPI INIT
Initialized node 0 on machine gnerv2
DONE MPI INIT
Initialized node 1 on machine gnerv2
DONE MPI INIT
Initialized node 2 on machine gnerv2
DONE MPI INIT
Initialized node 3 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.010 seconds.
Building the CSC structure...
        It takes 0.017 seconds.
Building the CSC structure...
        It takes 0.012 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
Building the CSC structure...
        It takes 0.011 seconds.
        It takes 0.013 seconds.
        It takes 0.014 seconds.
Building the Feature Vector...
        It takes 0.012 seconds.
        It takes 0.011 seconds.
        It takes 0.013 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.014 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.016 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.480 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/physics/8_parts
The number of GCNII layers: 8
The number of hidden units: 100
The number of training epoches: 200
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 5
Number of feature dimensions: 8415
Number of vertices: 34493
Number of GPUs: 8
        It takes 0.486 seconds.
Building the Label Vector...
        It takes 0.491 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.007 seconds.
        It takes 0.499 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.498 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.505 seconds.
Building the Label Vector...
        It takes 0.003 seconds.
        It takes 0.517 seconds.
Building the Label Vector...
        It takes 0.516 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 100, valid nodes 500, test nodes 1000
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 8): 0-[0, 4311) 1-[4311, 8623) 2-[8623, 12935) 3-[12935, 17247) 4-[17247, 21558) 5-[21558, 25870) 6-[25870, 30181) 7-[30181, 34493)
34493, 530417, 530417
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 4312
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
csr in-out ready !Start Cost Model Initialization...
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
GPU 0, layer [0, 9)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
34493, 530417, 530417
Number of vertices per chunk: 4312
34493, 530417, 530417
Number of vertices per chunk: 4312
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 55.625 Gbps (per GPU), 445.001 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.380 Gbps (per GPU), 443.040 Gbps (aggregated)
The layer-level communication performance: 55.364 Gbps (per GPU), 442.909 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 55.149 Gbps (per GPU), 441.193 Gbps (aggregated)
The layer-level communication performance: 55.121 Gbps (per GPU), 440.970 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 54.919 Gbps (per GPU), 439.354 Gbps (aggregated)
The layer-level communication performance: 54.877 Gbps (per GPU), 439.019 Gbps (aggregated)
The layer-level communication performance: 54.849 Gbps (per GPU), 438.792 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.659 Gbps (per GPU), 1269.270 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.653 Gbps (per GPU), 1269.223 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.671 Gbps (per GPU), 1269.369 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.647 Gbps (per GPU), 1269.174 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.485 Gbps (per GPU), 1267.880 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.656 Gbps (per GPU), 1269.246 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.650 Gbps (per GPU), 1269.198 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.091 Gbps (per GPU), 1264.726 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 98.448 Gbps (per GPU), 787.583 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.448 Gbps (per GPU), 787.583 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.450 Gbps (per GPU), 787.601 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.447 Gbps (per GPU), 787.576 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.448 Gbps (per GPU), 787.583 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.446 Gbps (per GPU), 787.570 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.449 Gbps (per GPU), 787.589 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 98.399 Gbps (per GPU), 787.189 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 32.476 Gbps (per GPU), 259.810 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.475 Gbps (per GPU), 259.798 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.474 Gbps (per GPU), 259.789 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.474 Gbps (per GPU), 259.796 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.470 Gbps (per GPU), 259.756 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.470 Gbps (per GPU), 259.759 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.470 Gbps (per GPU), 259.761 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 32.466 Gbps (per GPU), 259.727 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  4.16ms  0.43ms  0.55ms  9.58  4.31K  0.10M
 chk_1  4.18ms  0.42ms  0.55ms  9.87  4.31K  0.10M
 chk_2  4.19ms  0.41ms  0.54ms 10.18  4.31K  0.06M
 chk_3  4.19ms  0.41ms  0.54ms 10.13  4.31K  0.06M
 chk_4  4.20ms  0.40ms  0.52ms 10.58  4.31K  0.04M
 chk_5  4.20ms  0.39ms  0.52ms 10.70  4.31K  0.04M
 chk_6  4.19ms  0.40ms  0.52ms 10.59  4.31K  0.04M
 chk_7  4.20ms  0.40ms  0.52ms 10.58  4.31K  0.06M
   Avg  4.19  0.41  0.53
   Max  4.20  0.43  0.55
   Min  4.16  0.39  0.52
 Ratio  1.01  1.11  1.07
   Var  0.00  0.00  0.00
Profiling takes 0.550 s
*** Node 0, starting model training...
*** Node 2, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 65)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 4311
*** Node 3, starting model training...
*** Node 4, starting model training...
*** Node 5, starting model training...
*** Node 1, starting model training...
*** Node 6, starting model training...
*** Node 7, starting model training...
Num Stages: 1 / 1
Node 5, Pipeline Input Tensor: NULL
Node 5, Pipeline Output Tensor: NULL
*** Node 5 owns the model-level partition [0, 65)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 21558, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 4, Pipeline Input Tensor: NULL
Node 4, Pipeline Output Tensor: NULL
*** Node 4 owns the model-level partition [0, 65)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 17247, Num Local Vertices: 4311
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 65)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 8623, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 65)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 12935, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 65)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 4311, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 7, Pipeline Input Tensor: NULL
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [0, 65)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 30181, Num Local Vertices: 4312
Num Stages: 1 / 1
Node 6, Pipeline Input Tensor: NULL
Node 6, Pipeline Output Tensor: NULL
*** Node 6 owns the model-level partition [0, 65)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 25870, Num Local Vertices: 4311
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 7, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
+++++++++ Node 7 initializing the weights for op[0, 65)...
+++++++++ Node 3 initializing the weights for op[0, 65)...
+++++++++ Node 5 initializing the weights for op[0, 65)...
+++++++++ Node 6 initializing the weights for op[0, 65)...
+++++++++ Node 2 initializing the weights for op[0, 65)...
+++++++++ Node 4 initializing the weights for op[0, 65)...
+++++++++ Node 1 initializing the weights for op[0, 65)...
+++++++++ Node 0 initializing the weights for op[0, 65)...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 34236
Node 0, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 4, starting task scheduling...
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 5, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 6, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 7, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 1.6033	TrainAcc 0.8300	ValidAcc 0.7900	TestAcc 0.7870	BestValid 0.7900
	Epoch 10:	Loss 1.0501	TrainAcc 0.9400	ValidAcc 0.9240	TestAcc 0.9150	BestValid 0.9240
	Epoch 20:	Loss 0.3721	TrainAcc 0.9600	ValidAcc 0.9280	TestAcc 0.9340	BestValid 0.9280
	Epoch 30:	Loss 0.1748	TrainAcc 0.9900	ValidAcc 0.9420	TestAcc 0.9420	BestValid 0.9420
	Epoch 40:	Loss 0.1224	TrainAcc 0.9900	ValidAcc 0.9480	TestAcc 0.9460	BestValid 0.9480
	Epoch 50:	Loss 0.0983	TrainAcc 0.9900	ValidAcc 0.9480	TestAcc 0.9380	BestValid 0.9480
	Epoch 60:	Loss 0.0450	TrainAcc 1.0000	ValidAcc 0.9480	TestAcc 0.9390	BestValid 0.9480
	Epoch 70:	Loss 0.0390	TrainAcc 1.0000	ValidAcc 0.9460	TestAcc 0.9390	BestValid 0.9480
	Epoch 80:	Loss 0.0284	TrainAcc 1.0000	ValidAcc 0.9380	TestAcc 0.9390	BestValid 0.9480
	Epoch 90:	Loss 0.0442	TrainAcc 1.0000	ValidAcc 0.9420	TestAcc 0.9360	BestValid 0.9480
	Epoch 100:	Loss 0.0176	TrainAcc 1.0000	ValidAcc 0.9400	TestAcc 0.9370	BestValid 0.9480
	Epoch 110:	Loss 0.0160	TrainAcc 1.0000	ValidAcc 0.9400	TestAcc 0.9320	BestValid 0.9480
	Epoch 120:	Loss 0.0104	TrainAcc 1.0000	ValidAcc 0.9440	TestAcc 0.9350	BestValid 0.9480
	Epoch 130:	Loss 0.0104	TrainAcc 1.0000	ValidAcc 0.9400	TestAcc 0.9360	BestValid 0.9480
	Epoch 140:	Loss 0.0075	TrainAcc 1.0000	ValidAcc 0.9420	TestAcc 0.9350	BestValid 0.9480
	Epoch 150:	Loss 0.0097	TrainAcc 1.0000	ValidAcc 0.9360	TestAcc 0.9330	BestValid 0.9480
	Epoch 160:	Loss 0.0086	TrainAcc 1.0000	ValidAcc 0.9320	TestAcc 0.9370	BestValid 0.9480
	Epoch 170:	Loss 0.0055	TrainAcc 1.0000	ValidAcc 0.9420	TestAcc 0.9350	BestValid 0.9480
	Epoch 180:	Loss 0.0086	TrainAcc 1.0000	ValidAcc 0.9440	TestAcc 0.9320	BestValid 0.9480
	Epoch 190:	Loss 0.0058	TrainAcc 1.0000	ValidAcc 0.9340	TestAcc 0.9300	BestValid 0.9480
	Epoch 200:	Loss 0.0036	TrainAcc 1.0000	ValidAcc 0.9340	TestAcc 0.9340	BestValid 0.9480
****** Epoch Time (Excluding Evaluation Cost): 0.020 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.107 ms (Max: 0.164, Min: 0.057, Sum: 0.858)
Cluster-Wide Average, Compute: 8.309 ms (Max: 8.519, Min: 8.121, Sum: 66.475)
Cluster-Wide Average, Communication-Layer: 0.009 ms (Max: 0.010, Min: 0.008, Sum: 0.072)
Cluster-Wide Average, Bubble-Imbalance: 0.015 ms (Max: 0.017, Min: 0.013, Sum: 0.123)
Cluster-Wide Average, Communication-Graph: 9.194 ms (Max: 9.378, Min: 9.007, Sum: 73.553)
Cluster-Wide Average, Optimization: 1.936 ms (Max: 1.948, Min: 1.906, Sum: 15.492)
Cluster-Wide Average, Others: 0.161 ms (Max: 0.174, Min: 0.148, Sum: 1.292)
****** Breakdown Sum: 19.733 ms ******
Cluster-Wide Average, GPU Memory Consumption: 3.021 GB (Max: 3.342, Min: 2.962, Sum: 24.167)
Cluster-Wide Average, Graph-Level Communication Throughput: 26.471 Gbps (Max: 41.626, Min: 8.781, Sum: 211.771)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 0.204 GB
Weight-sync communication (cluster-wide, per-epoch): 0.048 GB
Total communication (cluster-wide, per-epoch): 0.252 GB
****** Accuracy Results ******
Highest valid_acc: 0.9480
Target test_acc: 0.9460
Epoch to reach the target acc: 39
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 4] Success 
[MPI Rank 2] Success 
[MPI Rank 5] Success 
[MPI Rank 3] Success 
[MPI Rank 6] Success 
[MPI Rank 7] Success 
