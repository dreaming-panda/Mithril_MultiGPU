Fri Sep 15 12:04:38 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   33C    P8    16W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   31C    P8    16W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   31C    P8    17W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   33C    P8    17W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
gnerv4
gnerv4
gnerv4
gnerv4
[ 21%] Built target context
[ 21%] Built target core
[ 73%] Built target cudahelp
Scanning dependencies of target OSDI2023_MULTI_NODES_resgcn
[ 86%] Built target OSDI2023_MULTI_NODES_gcn
[ 86%] Built target OSDI2023_MULTI_NODES_gcnii
[ 86%] Built target OSDI2023_MULTI_NODES_graphsage
[ 86%] Built target estimate_comm_volume
[ 97%] Building CXX object applications/async_multi_gpus/CMakeFiles/OSDI2023_MULTI_NODES_resgcn.dir/resgcn.cc.o
[100%] Linking CXX executable resgcn
[100%] Built target OSDI2023_MULTI_NODES_resgcn
Running experiments...
Initializing the runtime environment
DONE MPI INIT
Initialized node 0 on machine gnerv4
Building the CSR structure...
        It takes 0.000 seconds.
Building the CSC structure...
        It takes 0.000 seconds.
Building the Feature Vector...
        It takes 0.007 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/cora/1_parts
The number of GCNII layers: 16
The number of hidden units: 128
The number of training epoches: 1500
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 7
Number of feature dimensions: 1433
Number of vertices: 2708
Number of GPUs: 1
train nodes 140, valid nodes 500, test nodes 1000
GPU 0, layer [0, 16)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 1): 0-[0, 2708)
2708, 13264, 13264
Number of vertices per chunk: 2708
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 1525201.451 Gbps (per GPU), 1525201.451 Gbps (aggregated)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.13ms  0.45ms  0.56ms  2.50  2.71K  0.01M
   Avg  1.13  0.45  0.56
   Max  1.13  0.45  0.56
   Min  1.13  0.45  0.56
 Ratio  1.00  1.00  1.00
   Var  0.00  0.00  0.00
Profiling takes 0.090 s
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 151)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 2708
*** Node 0, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 151)...
Node 0, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 13.0603	TrainAcc 0.1786	ValidAcc 0.1160	TestAcc 0.1190	BestValid 0.1160
	Epoch 50:	Loss 1.9217	TrainAcc 0.2929	ValidAcc 0.2760	TestAcc 0.2850	BestValid 0.2760
	Epoch 100:	Loss 1.8144	TrainAcc 0.3786	ValidAcc 0.2040	TestAcc 0.2020	BestValid 0.2760
	Epoch 150:	Loss 0.7781	TrainAcc 0.8000	ValidAcc 0.5340	TestAcc 0.5240	BestValid 0.5340
	Epoch 200:	Loss 0.3200	TrainAcc 0.9500	ValidAcc 0.6740	TestAcc 0.6690	BestValid 0.6740
	Epoch 250:	Loss 0.0944	TrainAcc 0.9929	ValidAcc 0.7220	TestAcc 0.7010	BestValid 0.7220
	Epoch 300:	Loss 0.1119	TrainAcc 1.0000	ValidAcc 0.7060	TestAcc 0.7110	BestValid 0.7220
	Epoch 350:	Loss 0.0179	TrainAcc 1.0000	ValidAcc 0.7240	TestAcc 0.7280	BestValid 0.7240
	Epoch 400:	Loss 0.0238	TrainAcc 1.0000	ValidAcc 0.6920	TestAcc 0.6910	BestValid 0.7240
	Epoch 450:	Loss 0.0259	TrainAcc 1.0000	ValidAcc 0.7160	TestAcc 0.7290	BestValid 0.7240
	Epoch 500:	Loss 0.0507	TrainAcc 1.0000	ValidAcc 0.7140	TestAcc 0.7190	BestValid 0.7240
	Epoch 550:	Loss 0.0056	TrainAcc 1.0000	ValidAcc 0.7140	TestAcc 0.7240	BestValid 0.7240
	Epoch 600:	Loss 0.0110	TrainAcc 1.0000	ValidAcc 0.7080	TestAcc 0.7150	BestValid 0.7240
	Epoch 650:	Loss 0.0027	TrainAcc 1.0000	ValidAcc 0.7080	TestAcc 0.7240	BestValid 0.7240
	Epoch 700:	Loss 0.0037	TrainAcc 1.0000	ValidAcc 0.7180	TestAcc 0.7250	BestValid 0.7240
	Epoch 750:	Loss 0.0025	TrainAcc 1.0000	ValidAcc 0.7180	TestAcc 0.7320	BestValid 0.7240
	Epoch 800:	Loss 0.0068	TrainAcc 1.0000	ValidAcc 0.7020	TestAcc 0.7210	BestValid 0.7240
	Epoch 850:	Loss 0.0016	TrainAcc 1.0000	ValidAcc 0.7060	TestAcc 0.7260	BestValid 0.7240
	Epoch 900:	Loss 0.0019	TrainAcc 1.0000	ValidAcc 0.7140	TestAcc 0.7290	BestValid 0.7240
	Epoch 950:	Loss 0.0038	TrainAcc 1.0000	ValidAcc 0.6900	TestAcc 0.7200	BestValid 0.7240
	Epoch 1000:	Loss 0.0050	TrainAcc 1.0000	ValidAcc 0.6900	TestAcc 0.7080	BestValid 0.7240
	Epoch 1050:	Loss 0.0013	TrainAcc 1.0000	ValidAcc 0.7160	TestAcc 0.7260	BestValid 0.7240
	Epoch 1100:	Loss 0.0021	TrainAcc 1.0000	ValidAcc 0.7160	TestAcc 0.7250	BestValid 0.7240
	Epoch 1150:	Loss 0.0079	TrainAcc 1.0000	ValidAcc 0.7300	TestAcc 0.7310	BestValid 0.7300
	Epoch 1200:	Loss 0.0140	TrainAcc 1.0000	ValidAcc 0.7360	TestAcc 0.7480	BestValid 0.7360
	Epoch 1250:	Loss 0.0018	TrainAcc 1.0000	ValidAcc 0.7280	TestAcc 0.7390	BestValid 0.7360
	Epoch 1300:	Loss 0.0016	TrainAcc 1.0000	ValidAcc 0.7220	TestAcc 0.7360	BestValid 0.7360
	Epoch 1350:	Loss 0.0194	TrainAcc 1.0000	ValidAcc 0.7100	TestAcc 0.7210	BestValid 0.7360
	Epoch 1400:	Loss 0.0025	TrainAcc 1.0000	ValidAcc 0.7160	TestAcc 0.7200	BestValid 0.7360
	Epoch 1450:	Loss 0.0148	TrainAcc 1.0000	ValidAcc 0.7060	TestAcc 0.7290	BestValid 0.7360
	Epoch 1500:	Loss 0.0020	TrainAcc 1.0000	ValidAcc 0.7340	TestAcc 0.7460	BestValid 0.7360
****** Epoch Time (Excluding Evaluation Cost): 0.011 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.028 ms (Max: 0.028, Min: 0.028, Sum: 0.028)
Cluster-Wide Average, Compute: 9.361 ms (Max: 9.361, Min: 9.361, Sum: 9.361)
Cluster-Wide Average, Communication-Layer: 0.010 ms (Max: 0.010, Min: 0.010, Sum: 0.010)
Cluster-Wide Average, Bubble-Imbalance: 0.016 ms (Max: 0.016, Min: 0.016, Sum: 0.016)
Cluster-Wide Average, Communication-Graph: 0.074 ms (Max: 0.074, Min: 0.074, Sum: 0.074)
Cluster-Wide Average, Optimization: 1.212 ms (Max: 1.212, Min: 1.212, Sum: 1.212)
Cluster-Wide Average, Others: 0.474 ms (Max: 0.474, Min: 0.474, Sum: 0.474)
****** Breakdown Sum: 11.176 ms ******
Cluster-Wide Average, GPU Memory Consumption: 1.655 GB (Max: 1.655, Min: 1.655, Sum: 1.655)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.000 GB
****** Accuracy Results ******
Highest valid_acc: 0.7360
Target test_acc: 0.7480
Epoch to reach the target acc: 1199
[MPI Rank 0] Success 
