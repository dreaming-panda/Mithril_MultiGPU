Tue Sep 19 16:55:49 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   30C    P8    17W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   29C    P8    18W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   29C    P8    12W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   29C    P8    17W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 25%] Built target context
[ 25%] Built target core
[ 70%] Built target cudahelp
[ 87%] Built target OSDI2023_MULTI_NODES_resgcn
[ 87%] Built target OSDI2023_MULTI_NODES_graphsage
[ 87%] Built target OSDI2023_MULTI_NODES_gcn
[ 87%] Built target estimate_comm_volume
[ 87%] Built target OSDI2023_MULTI_NODES_gcnii
[ 87%] Built target OSDI2023_MULTI_NODES_resgcn_plus
Running experiments...
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
Initialized node 0 on machine gnerv1
DONE MPI INIT
Initialized node 1 on machine gnerv1
DONE MPI INIT
Initialized node 2 on machine gnerv1
DONE MPI INIT
Initialized node 3 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.115 seconds.
Building the CSC structure...
        It takes 0.115 seconds.
Building the CSC structure...
        It takes 0.115 seconds.
Building the CSC structure...
        It takes 0.115 seconds.
Building the CSC structure...
        It takes 0.061 seconds.
        It takes 0.061 seconds.
        It takes 0.061 seconds.
        It takes 0.061 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.186 seconds.
        It takes 0.186 seconds.
        It takes 0.186 seconds.
        It takes 0.186 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
        It takes 0.072 seconds.
        It takes 0.072 seconds.
        It takes 0.072 seconds.
        It takes 0.072 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/4_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 1000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 4
GPU 0, layer [0, 32)
169343, 2484941, 2484941
Number of vertices per chunk: 42336
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 32)
Chunks (number of global chunks: 4): 0-[0, 41468) 1-[41468, 84616) 2-[84616, 126410) 3-[126410, 169343)
GPU 0, layer [0, 32)
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
GPU 0, layer [0, 32)
Number of vertices per chunk: 42336
169343, 2484941, 2484941
Number of vertices per chunk: 42336
169343, 2484941, 2484941
Number of vertices per chunk: 42336
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 117.111 Gbps (per GPU), 468.444 Gbps (aggregated)
The layer-level communication performance: 116.964 Gbps (per GPU), 467.857 Gbps (aggregated)
The layer-level communication performance: 116.809 Gbps (per GPU), 467.235 Gbps (aggregated)
The layer-level communication performance: 116.803 Gbps (per GPU), 467.211 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 159.552 Gbps (per GPU), 638.210 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.619 Gbps (per GPU), 638.475 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.558 Gbps (per GPU), 638.232 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 159.549 Gbps (per GPU), 638.196 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 107.789 Gbps (per GPU), 431.158 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 107.789 Gbps (per GPU), 431.154 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 107.789 Gbps (per GPU), 431.158 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 107.792 Gbps (per GPU), 431.169 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  2.12ms  5.30ms  8.06ms  3.81 41.47K  0.66M
 chk_1  2.14ms  5.03ms  8.30ms  3.88 43.15K  0.57M
 chk_2  2.08ms  4.86ms  8.06ms  3.87 41.79K  0.57M
 chk_3  2.11ms  5.22ms  8.24ms  3.89 42.93K  0.51M
   Avg  2.11  5.10  8.16
   Max  2.14  5.30  8.30
   Min  2.08  4.86  8.06
 Ratio  1.03  1.09  1.03
   Var  0.00  0.03  0.01
Profiling takes 0.787 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 421)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 41468
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 421)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 126410, Num Local Vertices: 42933
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 421)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 84616, Num Local Vertices: 41794
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 421)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 41468, Num Local Vertices: 43148
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 421)...
+++++++++ Node 3 initializing the weights for op[0, 421)...
+++++++++ Node 2 initializing the weights for op[0, 421)...
+++++++++ Node 1 initializing the weights for op[0, 421)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 122443
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 4.0772	TrainAcc 0.1791	ValidAcc 0.0763	TestAcc 0.0586	BestValid 0.0763
	Epoch 50:	Loss 2.0818	TrainAcc 0.4927	ValidAcc 0.5180	TestAcc 0.5087	BestValid 0.5180
	Epoch 100:	Loss 1.6309	TrainAcc 0.6060	ValidAcc 0.6224	TestAcc 0.6150	BestValid 0.6224
	Epoch 150:	Loss 1.4205	TrainAcc 0.6513	ValidAcc 0.6577	TestAcc 0.6533	BestValid 0.6577
	Epoch 200:	Loss 1.3261	TrainAcc 0.6743	ValidAcc 0.6779	TestAcc 0.6708	BestValid 0.6779
	Epoch 250:	Loss 1.2741	TrainAcc 0.6880	ValidAcc 0.6899	TestAcc 0.6817	BestValid 0.6899
	Epoch 300:	Loss 1.2353	TrainAcc 0.6943	ValidAcc 0.6968	TestAcc 0.6913	BestValid 0.6968
	Epoch 350:	Loss 1.2098	TrainAcc 0.7020	ValidAcc 0.6993	TestAcc 0.6890	BestValid 0.6993
	Epoch 400:	Loss 1.1862	TrainAcc 0.7062	ValidAcc 0.7009	TestAcc 0.6904	BestValid 0.7009
	Epoch 450:	Loss 1.1743	TrainAcc 0.7089	ValidAcc 0.7056	TestAcc 0.6977	BestValid 0.7056
	Epoch 500:	Loss 1.1577	TrainAcc 0.7110	ValidAcc 0.7053	TestAcc 0.6978	BestValid 0.7056
	Epoch 550:	Loss 1.1454	TrainAcc 0.7137	ValidAcc 0.7062	TestAcc 0.6964	BestValid 0.7062
	Epoch 600:	Loss 1.1302	TrainAcc 0.7161	ValidAcc 0.7084	TestAcc 0.7010	BestValid 0.7084
	Epoch 650:	Loss 1.1204	TrainAcc 0.7179	ValidAcc 0.7094	TestAcc 0.7024	BestValid 0.7094
	Epoch 700:	Loss 1.1148	TrainAcc 0.7196	ValidAcc 0.7115	TestAcc 0.7003	BestValid 0.7115
	Epoch 750:	Loss 1.1056	TrainAcc 0.7210	ValidAcc 0.7112	TestAcc 0.7011	BestValid 0.7115
	Epoch 800:	Loss 1.1007	TrainAcc 0.7224	ValidAcc 0.7116	TestAcc 0.7029	BestValid 0.7116
	Epoch 850:	Loss 1.0935	TrainAcc 0.7243	ValidAcc 0.7132	TestAcc 0.7018	BestValid 0.7132
	Epoch 900:	Loss 1.0879	TrainAcc 0.7243	ValidAcc 0.7146	TestAcc 0.7053	BestValid 0.7146
	Epoch 950:	Loss 1.0793	TrainAcc 0.7258	ValidAcc 0.7156	TestAcc 0.7088	BestValid 0.7156
	Epoch 1000:	Loss 1.0717	TrainAcc 0.7275	ValidAcc 0.7161	TestAcc 0.7032	BestValid 0.7161
****** Epoch Time (Excluding Evaluation Cost): 0.231 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.078 ms (Max: 0.126, Min: 0.039, Sum: 0.313)
Cluster-Wide Average, Compute: 149.497 ms (Max: 151.676, Min: 146.762, Sum: 597.987)
Cluster-Wide Average, Communication-Layer: 0.009 ms (Max: 0.010, Min: 0.007, Sum: 0.036)
Cluster-Wide Average, Bubble-Imbalance: 0.016 ms (Max: 0.017, Min: 0.015, Sum: 0.064)
Cluster-Wide Average, Communication-Graph: 75.086 ms (Max: 77.841, Min: 73.049, Sum: 300.345)
Cluster-Wide Average, Optimization: 4.639 ms (Max: 4.669, Min: 4.595, Sum: 18.556)
Cluster-Wide Average, Others: 1.934 ms (Max: 2.031, Min: 1.798, Sum: 7.738)
****** Breakdown Sum: 231.260 ms ******
Cluster-Wide Average, GPU Memory Consumption: 11.863 GB (Max: 12.090, Min: 11.505, Sum: 47.454)
Cluster-Wide Average, Graph-Level Communication Throughput: 94.590 Gbps (Max: 108.525, Min: 75.470, Sum: 378.361)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 2.919 GB
Weight-sync communication (cluster-wide, per-epoch): 0.015 GB
Total communication (cluster-wide, per-epoch): 2.934 GB
****** Accuracy Results ******
Highest valid_acc: 0.7161
Target test_acc: 0.7032
Epoch to reach the target acc: 999
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
