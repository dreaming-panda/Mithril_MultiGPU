Tue Sep 19 20:42:44 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 36%   53C    P8    20W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 39%   55C    P8    21W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 35%   52C    P8    15W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 33%   51C    P8    18W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
[ 27%] Built target context
[ 32%] Built target core
[ 70%] Built target cudahelp
[ 77%] Built target OSDI2023_MULTI_NODES_graphsage
[ 77%] Built target OSDI2023_MULTI_NODES_gcn
[ 90%] Built target estimate_comm_volume
[ 90%] Built target OSDI2023_MULTI_NODES_resgcn_plus
[ 90%] Built target OSDI2023_MULTI_NODES_gcnii
[ 90%] Built target OSDI2023_MULTI_NODES_resgcn
Running experiments...
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
Starting GNN Training...
Initializing the runtime environment
DONE MPI INIT
DONE MPI INITInitialized node 0 on machine gnerv1
DONE MPI INIT
DONE MPI INIT
Initialized node 3 on machine gnerv1
Initialized node 2 on machine gnerv1

Initialized node 1 on machine gnerv1
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.052 seconds.
Building the CSC structure...
        It takes 0.054 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.058 seconds.
Building the CSC structure...
        It takes 0.049 seconds.
        It takes 0.051 seconds.
        It takes 0.055 seconds.
        It takes 0.056 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.063 seconds.
        It takes 0.059 seconds.
Building the Label Vector...
Building the Label Vector...
        It takes 0.056 seconds.
Building the Label Vector...
        It takes 0.064 seconds.
Building the Label Vector...
        It takes 0.023 seconds.
        It takes 0.026 seconds.
        It takes 0.025 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/ogbn_arxiv/4_parts
The number of GCNII layers: 32
The number of hidden units: 100
The number of training epoches: 1000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
GCN hyper-parameter alpha: 0.100000
GCN hyper-parameter lambda: 0.500000
Number of classes: 40
Number of feature dimensions: 128
Number of vertices: 169343
Number of GPUs: 4
        It takes 0.025 seconds.
GPU 0, layer [0, 33)
169343, 2484941, 2484941
Number of vertices per chunk: 42336
GPU 0, layer [0, 33)
169343, 2484941, 2484941
Number of vertices per chunk: 42336
csr in-out ready !Start Cost Model Initialization...
train nodes 90941, valid nodes 29799, test nodes 48603
GPU 0, layer [0, 33)
Chunks (number of global chunks: 4): 0-[0, 41468) 1-[41468, 84616) 2-[84616, 126410) 3-[126410, 169343)
GPU 0, layer [0, 33)
169343, 2484941, 2484941
Number of vertices per chunk: 42336
csr in-out ready !Start Cost Model Initialization...
169343, 2484941, 2484941
Number of vertices per chunk: 42336
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 117.045 Gbps (per GPU), 468.182 Gbps (aggregated)
The layer-level communication performance: 117.736 Gbps (per GPU), 470.945 Gbps (aggregated)
The layer-level communication performance: 116.758 Gbps (per GPU), 467.032 Gbps (aggregated)
The layer-level communication performance: 117.553 Gbps (per GPU), 470.213 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.893 Gbps (per GPU), 635.573 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.887 Gbps (per GPU), 635.549 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.848 Gbps (per GPU), 635.392 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.845 Gbps (per GPU), 635.380 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 108.492 Gbps (per GPU), 433.968 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 108.493 Gbps (per GPU), 433.972 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 108.492 Gbps (per GPU), 433.968 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 108.488 Gbps (per GPU), 433.954 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.19ms  2.27ms  2.72ms  2.28 41.47K  0.66M
 chk_1  1.22ms  2.28ms  2.76ms  2.26 43.15K  0.57M
 chk_2  1.20ms  2.20ms  2.67ms  2.22 41.79K  0.57M
 chk_3  1.23ms  2.31ms  2.73ms  2.23 42.93K  0.51M
   Avg  1.21  2.27  2.72
   Max  1.23  2.31  2.76
   Min  1.19  2.20  2.67
 Ratio  1.03  1.05  1.03
   Var  0.00  0.00  0.00
Profiling takes 0.375 s
*** Node 0, starting model training...
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 233)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 41468
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 233)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 84616, Num Local Vertices: 41794
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 233)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 41468, Num Local Vertices: 43148
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 233)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 126410, Num Local Vertices: 42933
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 233)...
+++++++++ Node 1 initializing the weights for op[0, 233)...
+++++++++ Node 2 initializing the weights for op[0, 233)...
+++++++++ Node 3 initializing the weights for op[0, 233)...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 122443
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 3.7128	TrainAcc 0.0235	ValidAcc 0.0204	TestAcc 0.0184	BestValid 0.0204
	Epoch 50:	Loss 2.8061	TrainAcc 0.2900	ValidAcc 0.3063	TestAcc 0.2761	BestValid 0.3063
	Epoch 100:	Loss 2.3402	TrainAcc 0.4263	ValidAcc 0.3998	TestAcc 0.3716	BestValid 0.3998
	Epoch 150:	Loss 2.1193	TrainAcc 0.4906	ValidAcc 0.4689	TestAcc 0.4445	BestValid 0.4689
	Epoch 200:	Loss 1.9794	TrainAcc 0.5367	ValidAcc 0.5275	TestAcc 0.5115	BestValid 0.5275
	Epoch 250:	Loss 1.8963	TrainAcc 0.5593	ValidAcc 0.5669	TestAcc 0.5551	BestValid 0.5669
	Epoch 300:	Loss 1.8399	TrainAcc 0.5723	ValidAcc 0.5761	TestAcc 0.5617	BestValid 0.5761
	Epoch 350:	Loss 1.7938	TrainAcc 0.5849	ValidAcc 0.5954	TestAcc 0.5829	BestValid 0.5954
	Epoch 400:	Loss 1.7518	TrainAcc 0.5947	ValidAcc 0.6033	TestAcc 0.5913	BestValid 0.6033
	Epoch 450:	Loss 1.7131	TrainAcc 0.6020	ValidAcc 0.6085	TestAcc 0.5969	BestValid 0.6085
	Epoch 500:	Loss 1.6755	TrainAcc 0.6097	ValidAcc 0.6182	TestAcc 0.6083	BestValid 0.6182
