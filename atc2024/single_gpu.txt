gnerv1
Fri Apr 19 21:21:13 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA RTX A5000               Off | 00000000:01:00.0 Off |                  Off |
| 30%   42C    P0              68W / 230W |      1MiB / 24564MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000               Off | 00000000:25:00.0 Off |                  Off |
| 30%   38C    P0              65W / 230W |      1MiB / 24564MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000               Off | 00000000:81:00.0 Off |                  Off |
| 30%   36C    P0              57W / 230W |      1MiB / 24564MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000               Off | 00000000:C1:00.0 Off |                  Off |
| 30%   34C    P0              56W / 230W |      1MiB / 24564MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
[  9%] Built target context
[ 30%] Built target core
[ 66%] Built target cudahelp
[ 76%] Built target estimate_comm_volume
[ 76%] Built target OSDI2023_MULTI_NODES_gcnii
[ 76%] Built target OSDI2023_MULTI_NODES_gcn
[ 76%] Built target OSDI2023_MULTI_NODES_graphsage
[ 90%] Built target OSDI2023_MULTI_NODES_resgcn
[ 90%] Built target OSDI2023_MULTI_NODES_lr
[ 90%] Built target OSDI2023_MULTI_NODES_resgcn_plus
Running experiments...
Initializing the runtime environment
DONE MPI INIT
Initialized node 0 on machine gnerv1
Building the CSR structure...
        It takes 2.265 seconds.
Building the CSC structure...
        It takes 2.239 seconds.
Building the Feature Vector...
        It takes 0.235 seconds.
Building the Label Vector...
        It takes 0.028 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/reddit/2_parts
The number of GCN layers: 32
The number of hidden units: 100
The number of training epoches: 20
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 41
Number of feature dimensions: 602
Number of vertices: 232965
Number of GPUs: 1
train nodes 153431, valid nodes 23831, test nodes 55703
GPU 0, layer [0, 32)
Chunks (number of global chunks: 2): 0-[0, 116483) 1-[116483, 232965)
232965, 114848857, 114848857
Number of vertices per chunk: 116483
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 642190.066 Gbps (per GPU), 642190.066 Gbps (aggregated)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0 48.63ms 46.12ms 45.12ms  1.08116.48K 68.42M
 chk_1 33.84ms 30.84ms 29.24ms  1.16116.48K 46.19M
   Avg 41.24 38.48 37.18
   Max 48.63 46.12 45.12
   Min 33.84 30.84 29.24
 Ratio  1.44  1.50  1.54
   Var 54.65 58.35 62.98
Profiling takes 2.648 s
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 159)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 232965
*** Node 0, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 159)...
Node 0, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 0.6931
	Epoch 2:	Loss 0.6931
	Epoch 3:	Loss 0.6931
	Epoch 4:	Loss 0.6931
	Epoch 5:	Loss 0.6931
	Epoch 6:	Loss 0.6931
	Epoch 7:	Loss 0.6931
	Epoch 8:	Loss 0.6931
	Epoch 9:	Loss 0.6931
	Epoch 10:	Loss 0.6931
	Epoch 11:	Loss 0.6931
	Epoch 12:	Loss 0.6931
	Epoch 13:	Loss 0.6931
	Epoch 14:	Loss 0.6928
	Epoch 15:	Loss 0.6907
	Epoch 16:	Loss 0.6801
	Epoch 17:	Loss 0.6368
	Epoch 18:	Loss 0.5191
	Epoch 19:	Loss 0.3599
	Epoch 20:	Loss 0.4744
****** Epoch Time (Excluding Evaluation Cost): 1.873 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.028 ms (Max: 0.028, Min: 0.028, Sum: 0.028)
Cluster-Wide Average, Compute: 1870.213 ms (Max: 1870.213, Min: 1870.213, Sum: 1870.213)
Cluster-Wide Average, Communication-Layer: 0.019 ms (Max: 0.019, Min: 0.019, Sum: 0.019)
Cluster-Wide Average, Bubble-Imbalance: 0.048 ms (Max: 0.048, Min: 0.048, Sum: 0.048)
Cluster-Wide Average, Communication-Graph: 0.279 ms (Max: 0.279, Min: 0.279, Sum: 0.279)
Cluster-Wide Average, Optimization: 0.993 ms (Max: 0.993, Min: 0.993, Sum: 0.993)
Cluster-Wide Average, Others: 1.035 ms (Max: 1.035, Min: 1.035, Sum: 1.035)
****** Breakdown Sum: 1872.616 ms ******
Cluster-Wide Average, GPU Memory Consumption: 21.361 GB (Max: 21.361, Min: 21.361, Sum: 21.361)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.000 GB
[MPI Rank 0] Success 
