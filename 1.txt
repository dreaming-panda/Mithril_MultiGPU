gnerv1
Mon Aug  7 16:41:41 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   52C    P2   178W / 230W |   8806MiB / 24564MiB |     53%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   57C    P2   226W / 230W |   8830MiB / 24564MiB |     96%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   53C    P2   187W / 230W |   8834MiB / 24564MiB |     61%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   48C    P2   160W / 230W |   8786MiB / 24564MiB |     50%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A   1716905      C   ...ns/async_multi_gpus/gcnii     8675MiB |
|    1   N/A  N/A   1716906      C   ...ns/async_multi_gpus/gcnii     8699MiB |
|    2   N/A  N/A   1716907      C   ...ns/async_multi_gpus/gcnii     8703MiB |
|    3   N/A  N/A   1716908      C   ...ns/async_multi_gpus/gcnii     8655MiB |
+-----------------------------------------------------------------------------+
[ 27%] Built target context
[ 36%] Built target core
[ 77%] Built target cudahelp
[ 97%] Built target OSDI2023_MULTI_NODES_gcnii
[ 97%] Built target OSDI2023_MULTI_NODES_graphsage
[ 97%] Built target estimate_comm_volume
[ 97%] Built target OSDI2023_MULTI_NODES_gcn
Running experiments...
gnerv3
gnerv3
gnerv3
gnerv3
gnerv2
gnerv2
gnerv2
gnerv2
Initialized node 0 on machine gnerv2
Initialized node 1 on machine gnerv2
Initialized node 2 on machine gnerv2
Initialized node 3 on machine gnerv2
Initialized node 4 on machine gnerv3
Initialized node 5 on machine gnerv3
Initialized node 6 on machine gnerv3
Initialized node 7 on machine gnerv3
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.016 seconds.
Building the CSC structure...
        It takes 0.023 seconds.
Building the CSC structure...
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
        It takes 0.024 seconds.
Building the CSC structure...
        It takes 0.025 seconds.
Building the CSC structure...
        It takes 0.020 seconds.
        It takes 0.027 seconds.
Building the CSC structure...
        It takes 0.018 seconds.
Building the Feature Vector...
        It takes 0.021 seconds.
        It takes 0.020 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.017 seconds.
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.022 seconds.
        It takes 0.021 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.098 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.104 seconds.
Building the Label Vector...
        It takes 0.104 seconds.
        It takes 0.007 seconds.
Building the Label Vector...
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/weighted_shuffled_partitioned_graphs/flickr/32_parts
The number of GCNII layers: 32
The number of hidden units: 200
The number of training epoches: 5000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 7
Number of feature dimensions: 500
Number of vertices: 89250
Number of GPUs: 8
        It takes 0.103 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.008 seconds.
        It takes 0.008 seconds.
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.108 seconds.
Building the Label Vector...
        It takes 0.007 seconds.
        It takes 0.007 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
89250, 989006, 989006
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
train nodes 44625, valid nodes 22312, test nodes 22313
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
Chunks (number of global chunks: 32): 0-[0, 2809) 1-[2809, 5626) 2-[5626, 8426) 3-[8426, 11230) 4-[11230, 14047) 5-[14047, 16800) 6-[16800, 19507) 7-[19507, 22266) 8-[22266, 25059) ... 31-[86469, 89250)
89250, 989006, 989006
Number of vertices per chunk: 2790
89250, 989006, 989006
Number of vertices per chunk: 2790
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
89250, 989006, 989006
89250, 989006, 989006
Number of vertices per chunk: 2790
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
GPU 2, layer [8, 12)
GPU 3, layer [12, 16)
GPU 4, layer [16, 20)
GPU 5, layer [20, 24)
GPU 6, layer [24, 28)
GPU 7, layer [28, 32)
89250, 989006, 989006
Number of vertices per chunk: 2790
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 59.687 Gbps (per GPU), 477.494 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.403 Gbps (per GPU), 475.226 Gbps (aggregated)
The layer-level communication performance: 59.394 Gbps (per GPU), 475.151 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 59.150 Gbps (per GPU), 473.204 Gbps (aggregated)
The layer-level communication performance: 59.115 Gbps (per GPU), 472.920 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The layer-level communication performance: 58.926 Gbps (per GPU), 471.404 Gbps (aggregated)
The layer-level communication performance: 58.882 Gbps (per GPU), 471.053 Gbps (aggregated)
The layer-level communication performance: 58.850 Gbps (per GPU), 470.800 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 156.239 Gbps (per GPU), 1249.910 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.239 Gbps (per GPU), 1249.909 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.224 Gbps (per GPU), 1249.793 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.247 Gbps (per GPU), 1249.978 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.236 Gbps (per GPU), 1249.886 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.148 Gbps (per GPU), 1249.188 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.221 Gbps (per GPU), 1249.769 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 156.157 Gbps (per GPU), 1249.258 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 101.664 Gbps (per GPU), 813.309 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.670 Gbps (per GPU), 813.363 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.668 Gbps (per GPU), 813.342 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.670 Gbps (per GPU), 813.362 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.665 Gbps (per GPU), 813.322 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.667 Gbps (per GPU), 813.336 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.667 Gbps (per GPU), 813.336 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 101.669 Gbps (per GPU), 813.349 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
****** Start profiling the graph-level communication performance with supernodesize = 8 ******
The graph-level communication performance (supernode = 8): 33.227 Gbps (per GPU), 265.819 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.227 Gbps (per GPU), 265.813 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.228 Gbps (per GPU), 265.823 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.228 Gbps (per GPU), 265.826 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.224 Gbps (per GPU), 265.793 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.224 Gbps (per GPU), 265.792 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.220 Gbps (per GPU), 265.763 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 8): 33.225 Gbps (per GPU), 265.802 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.89ms  0.71ms  0.33ms  2.66  2.81K  0.03M
 chk_1  0.91ms  0.72ms  0.34ms  2.69  2.82K  0.03M
 chk_2  0.90ms  0.72ms  0.35ms  2.58  2.80K  0.03M
 chk_3  0.88ms  0.71ms  0.34ms  2.63  2.80K  0.03M
 chk_4  0.91ms  0.72ms  0.34ms  2.67  2.82K  0.03M
 chk_5  0.91ms  0.74ms  0.36ms  2.50  2.75K  0.03M
 chk_6  0.87ms  0.70ms  0.33ms  2.67  2.71K  0.03M
 chk_7  0.89ms  0.72ms  0.34ms  2.60  2.76K  0.03M
 chk_8  0.89ms  0.72ms  0.34ms  2.62  2.79K  0.03M
 chk_9  0.88ms  0.72ms  0.33ms  2.65  2.81K  0.03M
chk_10  0.88ms  0.71ms  0.32ms  2.72  2.81K  0.03M
chk_11  0.91ms  0.73ms  0.36ms  2.54  2.74K  0.03M
chk_12  0.91ms  0.73ms  0.35ms  2.56  2.76K  0.03M
chk_13  0.90ms  0.72ms  0.35ms  2.57  2.75K  0.03M
chk_14  0.89ms  0.71ms  0.33ms  2.67  2.81K  0.03M
chk_15  0.89ms  0.72ms  0.34ms  2.62  2.77K  0.03M
chk_16  0.89ms  0.72ms  0.34ms  2.61  2.78K  0.03M
chk_17  0.91ms  0.72ms  0.34ms  2.67  2.79K  0.03M
chk_18  0.91ms  0.72ms  0.34ms  2.71  2.82K  0.03M
chk_19  0.87ms  0.70ms  0.32ms  2.72  2.81K  0.03M
chk_20  0.90ms  0.72ms  0.36ms  2.53  2.77K  0.03M
chk_21  0.91ms  0.72ms  0.33ms  2.72  2.84K  0.02M
chk_22  0.89ms  0.71ms  0.34ms  2.62  2.78K  0.03M
chk_23  0.89ms  0.72ms  0.34ms  2.64  2.80K  0.03M
chk_24  0.89ms  0.72ms  0.34ms  2.61  2.80K  0.03M
chk_25  0.89ms  0.71ms  0.33ms  2.69  2.81K  0.03M
chk_26  0.88ms  0.71ms  0.33ms  2.69  2.81K  0.03M
chk_27  0.90ms  0.72ms  0.34ms  2.63  2.79K  0.03M
chk_28  0.90ms  0.72ms  0.36ms  2.51  2.77K  0.03M
chk_29  0.90ms  0.72ms  0.34ms  2.60  2.77K  0.03M
chk_30  0.90ms  0.72ms  0.34ms  2.62  2.80K  0.03M
chk_31  0.90ms  0.73ms  0.35ms  2.58  2.78K  0.03M
   Avg  0.89  0.72  0.34
   Max  0.91  0.74  0.36
   Min  0.87  0.70  0.32
 Ratio  1.05  1.06  1.14
   Var  0.00  0.00  0.00
Profiling takes 0.828 s
Evaluating the performance of the pure model-parallel execution plan.
The bottleneck stage in the optimal plan: 97.462 ms
Partition 0 [0, 4) has cost: 97.462 ms
Partition 1 [4, 8) has cost: 91.794 ms
Partition 2 [8, 12) has cost: 91.794 ms
Partition 3 [12, 16) has cost: 91.794 ms
Partition 4 [16, 20) has cost: 91.794 ms
Partition 5 [20, 24) has cost: 91.794 ms
Partition 6 [24, 28) has cost: 91.794 ms
Partition 7 [28, 32) has cost: 79.738 ms
The optimal partitioning:
[0, 4)
[4, 8)
[8, 12)
[12, 16)
[16, 20)
[20, 24)
[24, 28)
[28, 32)
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 48.169 ms
GPU 0, Compute+Comm Time: 40.079 ms, Bubble Time: 8.090 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 37.056 ms, Bubble Time: 8.164 ms, Imbalance Overhead: 2.949 ms
GPU 2, Compute+Comm Time: 37.056 ms, Bubble Time: 8.257 ms, Imbalance Overhead: 2.857 ms
GPU 3, Compute+Comm Time: 37.056 ms, Bubble Time: 8.339 ms, Imbalance Overhead: 2.774 ms
GPU 4, Compute+Comm Time: 37.056 ms, Bubble Time: 8.413 ms, Imbalance Overhead: 2.700 ms
GPU 5, Compute+Comm Time: 37.056 ms, Bubble Time: 8.507 ms, Imbalance Overhead: 2.606 ms
GPU 6, Compute+Comm Time: 37.056 ms, Bubble Time: 8.616 ms, Imbalance Overhead: 2.498 ms
GPU 7, Compute+Comm Time: 33.626 ms, Bubble Time: 8.770 ms, Imbalance Overhead: 5.773 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 92.619 ms
GPU 0, Compute+Comm Time: 65.252 ms, Bubble Time: 16.767 ms, Imbalance Overhead: 10.600 ms
GPU 1, Compute+Comm Time: 73.878 ms, Bubble Time: 16.471 ms, Imbalance Overhead: 2.270 ms
GPU 2, Compute+Comm Time: 73.878 ms, Bubble Time: 16.363 ms, Imbalance Overhead: 2.378 ms
GPU 3, Compute+Comm Time: 73.878 ms, Bubble Time: 16.288 ms, Imbalance Overhead: 2.453 ms
GPU 4, Compute+Comm Time: 73.878 ms, Bubble Time: 16.249 ms, Imbalance Overhead: 2.492 ms
GPU 5, Compute+Comm Time: 73.878 ms, Bubble Time: 16.177 ms, Imbalance Overhead: 2.564 ms
GPU 6, Compute+Comm Time: 73.878 ms, Bubble Time: 16.108 ms, Imbalance Overhead: 2.633 ms
GPU 7, Compute+Comm Time: 76.523 ms, Bubble Time: 16.061 ms, Imbalance Overhead: 0.035 ms
The estimated cost of the whole pipeline: 147.828 ms

Evaluating the hybrid-parallelism execution plan with 2 DP ways.
The bottleneck stage in the optimal plan: 189.256 ms
Partition 0 [0, 8) has cost: 189.256 ms
Partition 1 [8, 16) has cost: 183.588 ms
Partition 2 [16, 24) has cost: 183.588 ms
Partition 3 [24, 32) has cost: 171.532 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 54.372 ms
GPU 0, Compute+Comm Time: 46.077 ms, Bubble Time: 8.284 ms, Imbalance Overhead: 0.011 ms
GPU 1, Compute+Comm Time: 44.557 ms, Bubble Time: 8.353 ms, Imbalance Overhead: 1.463 ms
GPU 2, Compute+Comm Time: 44.557 ms, Bubble Time: 8.440 ms, Imbalance Overhead: 1.376 ms
GPU 3, Compute+Comm Time: 42.848 ms, Bubble Time: 8.641 ms, Imbalance Overhead: 2.883 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 98.084 ms
GPU 0, Compute+Comm Time: 77.229 ms, Bubble Time: 15.555 ms, Imbalance Overhead: 5.299 ms
GPU 1, Compute+Comm Time: 81.525 ms, Bubble Time: 15.210 ms, Imbalance Overhead: 1.348 ms
GPU 2, Compute+Comm Time: 81.525 ms, Bubble Time: 15.138 ms, Imbalance Overhead: 1.420 ms
GPU 3, Compute+Comm Time: 82.853 ms, Bubble Time: 15.093 ms, Imbalance Overhead: 0.138 ms
    The estimated cost with 2 DP ways is 160.079 ms

Evaluating the hybrid-parallelism execution plan with 4 DP ways.
The bottleneck stage in the optimal plan: 372.844 ms
Partition 0 [0, 16) has cost: 372.844 ms
Partition 1 [16, 32) has cost: 355.120 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 81.037 ms
GPU 0, Compute+Comm Time: 72.229 ms, Bubble Time: 8.807 ms, Imbalance Overhead: 0.000 ms
GPU 1, Compute+Comm Time: 70.602 ms, Bubble Time: 8.984 ms, Imbalance Overhead: 1.450 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 122.608 ms
GPU 0, Compute+Comm Time: 106.495 ms, Bubble Time: 13.578 ms, Imbalance Overhead: 2.535 ms
GPU 1, Compute+Comm Time: 109.315 ms, Bubble Time: 13.293 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 4 DP ways is 213.827 ms

Evaluating the hybrid-parallelism execution plan with 8 DP ways.
The bottleneck stage in the optimal plan: 727.963 ms
Partition 0 [0, 32) has cost: 727.963 ms
****** Estimating the Forwarding Pipeline Cost ******
Simulation Results: Total Runtime: 271.641 ms
GPU 0, Compute+Comm Time: 271.641 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
****** Estimating the Backwarding Pipeline Cost *******
Simulation Results: Total Runtime: 308.309 ms
GPU 0, Compute+Comm Time: 308.309 ms, Bubble Time: 0.000 ms, Imbalance Overhead: 0.000 ms
    The estimated cost with 8 DP ways is 608.948 ms

*** Node 0, starting model training...
Num Stages: 8 / 8
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_ADD
*** Node 0 owns the model-level partition [0, 37)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 4, starting model training...
Num Stages: 8 / 8
*** Node 1, starting model training...
Num Stages: 8 / 8
Node 1, Pipeline Input Tensor: OPERATOR_ADD
Node 1, Pipeline Output Tensor: OPERATOR_ADD
*** Node 1 owns the model-level partition [37, 73)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 5, starting model training...
Num Stages: 8 / 8
Node 5, Pipeline Input Tensor: OPERATOR_ADD
Node 5, Pipeline Output Tensor: OPERATOR_ADD
*** Node 5 owns the model-level partition [181, 217)
*** Node 5, constructing the helper classes...
Node 5, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 6, starting model training...
Num Stages: 8 / 8
Node 6, Pipeline Input Tensor: OPERATOR_ADD
Node 6, Pipeline Output Tensor: OPERATOR_ADD
*** Node 6 owns the model-level partition [217, 253)
*** Node 6, constructing the helper classes...
Node 6, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 2, starting model training...
Num Stages: 8 / 8
Node 2, Pipeline Input Tensor: OPERATOR_ADD
Node 2, Pipeline Output Tensor: OPERATOR_ADD
*** Node 2 owns the model-level partition [73, 109)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, starting model training...
Num Stages: 8 / 8
Node 7, Pipeline Input Tensor: OPERATOR_ADD
Node 7, Pipeline Output Tensor: NULL
*** Node 7 owns the model-level partition [253, 287)
*** Node 7, constructing the helper classes...
Node 7, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 3, starting model training...
Num Stages: 8 / 8
Node 3, Pipeline Input Tensor: OPERATOR_ADD
Node 3, Pipeline Output Tensor: OPERATOR_ADD
*** Node 3 owns the model-level partition [109, 145)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 89250
Node 4, Pipeline Input Tensor: OPERATOR_ADD
Node 4, Pipeline Output Tensor: OPERATOR_ADD
*** Node 4 owns the model-level partition [145, 181)
*** Node 4, constructing the helper classes...
Node 4, Local Vertex Begin: 0, Num Local Vertices: 89250
*** Node 7, setting up some other necessary information...
*** Node 5, setting up some other necessary information...
*** Node 6, setting up some other necessary information...
*** Node 4, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 0, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
+++++++++ Node 1 initializing the weights for op[37, 73)...
+++++++++ Node 2 initializing the weights for op[73, 109)...
+++++++++ Node 5 initializing the weights for op[181, 217)...
+++++++++ Node 3 initializing the weights for op[109, 145)...
+++++++++ Node 6 initializing the weights for op[217, 253)...
+++++++++ Node 7 initializing the weights for op[253, 287)...
+++++++++ Node 4 initializing the weights for op[145, 181)...
+++++++++ Node 0 initializing the weights for op[0, 37)...
Node 4, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 5, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 6, discovering the vertices that will be sent across graph boundary...
Node 6, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 7, discovering the vertices that will be sent across graph boundary...
Node 7, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
Node 4, discovering the vertices that will be received across the graph boundary.
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 5, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 4, starting task scheduling...
*** Node 5, starting task scheduling...
*** Node 0, starting task scheduling...
*** Node 6, starting task scheduling...
*** Node 1, starting task scheduling...
*** Node 7, starting task scheduling...
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000



The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 25:	Loss 1.6182	TrainAcc 0.4259	ValidAcc 0.4292	TestAcc 0.4272	BestValid 0.4292
	Epoch 50:	Loss 1.5736	TrainAcc 0.4594	ValidAcc 0.4610	TestAcc 0.4614	BestValid 0.4610
	Epoch 75:	Loss 1.5464	TrainAcc 0.4773	ValidAcc 0.4792	TestAcc 0.4784	BestValid 0.4792
	Epoch 100:	Loss 1.5314	TrainAcc 0.4905	ValidAcc 0.4920	TestAcc 0.4912	BestValid 0.4920
	Epoch 125:	Loss 1.5185	TrainAcc 0.4894	ValidAcc 0.4926	TestAcc 0.4901	BestValid 0.4926
	Epoch 150:	Loss 1.5093	TrainAcc 0.5015	ValidAcc 0.5019	TestAcc 0.5027	BestValid 0.5019
	Epoch 175:	Loss 1.5013	TrainAcc 0.4991	ValidAcc 0.5000	TestAcc 0.4990	BestValid 0.5019
	Epoch 200:	Loss 1.4960	TrainAcc 0.4979	ValidAcc 0.4984	TestAcc 0.4964	BestValid 0.5019
	Epoch 225:	Loss 1.4939	TrainAcc 0.5068	ValidAcc 0.5075	TestAcc 0.5081	BestValid 0.5075
	Epoch 250:	Loss 1.4865	TrainAcc 0.5050	ValidAcc 0.5066	TestAcc 0.5061	BestValid 0.5075
	Epoch 275:	Loss 1.4759	TrainAcc 0.5000	ValidAcc 0.4996	TestAcc 0.4994	BestValid 0.5075
	Epoch 300:	Loss 1.4756	TrainAcc 0.5050	ValidAcc 0.5039	TestAcc 0.5046	BestValid 0.5075
	Epoch 325:	Loss 1.4714	TrainAcc 0.5069	ValidAcc 0.5074	TestAcc 0.5063	BestValid 0.5075
	Epoch 350:	Loss 1.4642	TrainAcc 0.5034	ValidAcc 0.5027	TestAcc 0.5022	BestValid 0.5075
	Epoch 375:	Loss 1.4678	TrainAcc 0.5095	ValidAcc 0.5108	TestAcc 0.5092	BestValid 0.5108
	Epoch 400:	Loss 1.4583	TrainAcc 0.5087	ValidAcc 0.5087	TestAcc 0.5074	BestValid 0.5108
	Epoch 425:	Loss 1.4622	TrainAcc 0.5055	ValidAcc 0.5045	TestAcc 0.5032	BestValid 0.5108
	Epoch 450:	Loss 1.4554	TrainAcc 0.5089	ValidAcc 0.5079	TestAcc 0.5061	BestValid 0.5108
	Epoch 475:	Loss 1.4573	TrainAcc 0.5003	ValidAcc 0.4993	TestAcc 0.4969	BestValid 0.5108
	Epoch 500:	Loss 1.4543	TrainAcc 0.5120	ValidAcc 0.5101	TestAcc 0.5103	BestValid 0.5108
	Epoch 525:	Loss 1.4463	TrainAcc 0.5076	ValidAcc 0.5061	TestAcc 0.5047	BestValid 0.5108
	Epoch 550:	Loss 1.4621	TrainAcc 0.5105	ValidAcc 0.5085	TestAcc 0.5080	BestValid 0.5108
	Epoch 575:	Loss 1.4415	TrainAcc 0.5146	ValidAcc 0.5127	TestAcc 0.5131	BestValid 0.5127
	Epoch 600:	Loss 1.4462	TrainAcc 0.5130	ValidAcc 0.5111	TestAcc 0.5103	BestValid 0.5127
	Epoch 625:	Loss 1.4422	TrainAcc 0.5157	ValidAcc 0.5133	TestAcc 0.5141	BestValid 0.5133
	Epoch 650:	Loss 1.4475	TrainAcc 0.5079	ValidAcc 0.5046	TestAcc 0.5046	BestValid 0.5133
	Epoch 675:	Loss 1.4331	TrainAcc 0.5169	ValidAcc 0.5152	TestAcc 0.5144	BestValid 0.5152
	Epoch 700:	Loss 1.4442	TrainAcc 0.5160	ValidAcc 0.5154	TestAcc 0.5136	BestValid 0.5154
	Epoch 725:	Loss 1.4373	TrainAcc 0.5184	ValidAcc 0.5144	TestAcc 0.5155	BestValid 0.5154
	Epoch 750:	Loss 1.4318	TrainAcc 0.5124	ValidAcc 0.5084	TestAcc 0.5087	BestValid 0.5154
	Epoch 775:	Loss 1.4387	TrainAcc 0.5176	ValidAcc 0.5156	TestAcc 0.5138	BestValid 0.5156
	Epoch 800:	Loss 1.4335	TrainAcc 0.5199	ValidAcc 0.5161	TestAcc 0.5163	BestValid 0.5161
	Epoch 825:	Loss 1.4334	TrainAcc 0.5117	ValidAcc 0.5078	TestAcc 0.5072	BestValid 0.5161
	Epoch 850:	Loss 1.4316	TrainAcc 0.5170	ValidAcc 0.5130	TestAcc 0.5128	BestValid 0.5161
	Epoch 875:	Loss 1.4289	TrainAcc 0.5095	ValidAcc 0.5069	TestAcc 0.5063	BestValid 0.5161
	Epoch 900:	Loss 1.4336	TrainAcc 0.5076	ValidAcc 0.5045	TestAcc 0.5029	BestValid 0.5161
	Epoch 925:	Loss 1.4268	TrainAcc 0.5169	ValidAcc 0.5130	TestAcc 0.5131	BestValid 0.5161
	Epoch 950:	Loss 1.4254	TrainAcc 0.5160	ValidAcc 0.5112	TestAcc 0.5112	BestValid 0.5161
	Epoch 975:	Loss 1.4434	TrainAcc 0.5119	ValidAcc 0.5075	TestAcc 0.5077	BestValid 0.5161
	Epoch 1000:	Loss 1.4143	TrainAcc 0.5206	ValidAcc 0.5152	TestAcc 0.5155	BestValid 0.5161
	Epoch 1025:	Loss 1.4276	TrainAcc 0.5142	ValidAcc 0.5084	TestAcc 0.5094	BestValid 0.5161
	Epoch 1050:	Loss 1.4304	TrainAcc 0.5129	ValidAcc 0.5098	TestAcc 0.5089	BestValid 0.5161
	Epoch 1075:	Loss 1.4254	TrainAcc 0.5231	ValidAcc 0.5177	TestAcc 0.5176	BestValid 0.5177
	Epoch 1100:	Loss 1.4235	TrainAcc 0.5212	ValidAcc 0.5157	TestAcc 0.5166	BestValid 0.5177
	Epoch 1125:	Loss 1.4231	TrainAcc 0.5170	ValidAcc 0.5123	TestAcc 0.5120	BestValid 0.5177
	Epoch 1150:	Loss 1.4241	TrainAcc 0.5226	ValidAcc 0.5177	TestAcc 0.5179	BestValid 0.5177
	Epoch 1175:	Loss 1.4154	TrainAcc 0.5180	ValidAcc 0.5138	TestAcc 0.5129	BestValid 0.5177
	Epoch 1200:	Loss 1.4344	TrainAcc 0.5250	ValidAcc 0.5178	TestAcc 0.5189	BestValid 0.5178
	Epoch 1225:	Loss 1.4224	TrainAcc 0.5235	ValidAcc 0.5173	TestAcc 0.5181	BestValid 0.5178
	Epoch 1250:	Loss 1.4110	TrainAcc 0.5200	ValidAcc 0.5134	TestAcc 0.5145	BestValid 0.5178
	Epoch 1275:	Loss 1.4213	TrainAcc 0.5228	ValidAcc 0.5171	TestAcc 0.5175	BestValid 0.5178
	Epoch 1300:	Loss 1.4202	TrainAcc 0.5120	ValidAcc 0.5056	TestAcc 0.5052	BestValid 0.5178
	Epoch 1325:	Loss 1.4125	TrainAcc 0.5176	ValidAcc 0.5125	TestAcc 0.5128	BestValid 0.5178
	Epoch 1350:	Loss 1.4250	TrainAcc 0.5256	ValidAcc 0.5187	TestAcc 0.5195	BestValid 0.5187
	Epoch 1375:	Loss 1.4152	TrainAcc 0.5225	ValidAcc 0.5175	TestAcc 0.5167	BestValid 0.5187
	Epoch 1400:	Loss 1.4098	TrainAcc 0.5222	ValidAcc 0.5160	TestAcc 0.5167	BestValid 0.5187
	Epoch 1425:	Loss 1.4110	TrainAcc 0.5241	ValidAcc 0.5182	TestAcc 0.5192	BestValid 0.5187
	Epoch 1450:	Loss 1.4095	TrainAcc 0.5152	ValidAcc 0.5087	TestAcc 0.5082	BestValid 0.5187
	Epoch 1475:	Loss 1.4070	TrainAcc 0.5243	ValidAcc 0.5184	TestAcc 0.5187	BestValid 0.5187
	Epoch 1500:	Loss 1.4207	TrainAcc 0.5190	ValidAcc 0.5121	TestAcc 0.5129	BestValid 0.5187
	Epoch 1525:	Loss 1.4107	TrainAcc 0.5193	ValidAcc 0.5132	TestAcc 0.5125	BestValid 0.5187
	Epoch 1550:	Loss 1.4060	TrainAcc 0.5216	ValidAcc 0.5146	TestAcc 0.5154	BestValid 0.5187
	Epoch 1575:	Loss 1.4035	TrainAcc 0.5238	ValidAcc 0.5176	TestAcc 0.5182	BestValid 0.5187
	Epoch 1600:	Loss 1.4004	TrainAcc 0.5151	ValidAcc 0.5088	TestAcc 0.5085	BestValid 0.5187
	Epoch 1625:	Loss 1.4261	TrainAcc 0.5148	ValidAcc 0.5100	TestAcc 0.5084	BestValid 0.5187
	Epoch 1650:	Loss 1.3986	TrainAcc 0.5215	ValidAcc 0.5151	TestAcc 0.5143	BestValid 0.5187
	Epoch 1675:	Loss 1.4232	TrainAcc 0.5194	ValidAcc 0.5124	TestAcc 0.5114	BestValid 0.5187
	Epoch 1700:	Loss 1.4087	TrainAcc 0.5249	ValidAcc 0.5183	TestAcc 0.5186	BestValid 0.5187
	Epoch 1725:	Loss 1.4001	TrainAcc 0.5206	ValidAcc 0.5142	TestAcc 0.5121	BestValid 0.5187
	Epoch 1750:	Loss 1.4140	TrainAcc 0.5279	ValidAcc 0.5209	TestAcc 0.5207	BestValid 0.5209
	Epoch 1775:	Loss 1.3983	TrainAcc 0.5213	ValidAcc 0.5158	TestAcc 0.5146	BestValid 0.5209
	Epoch 1800:	Loss 1.4036	TrainAcc 0.5191	ValidAcc 0.5128	TestAcc 0.5115	BestValid 0.5209
	Epoch 1825:	Loss 1.4180	TrainAcc 0.5145	ValidAcc 0.5078	TestAcc 0.5073	BestValid 0.5209
	Epoch 1850:	Loss 1.4084	TrainAcc 0.5241	ValidAcc 0.5173	TestAcc 0.5183	BestValid 0.5209
	Epoch 1875:	Loss 1.4061	TrainAcc 0.5144	ValidAcc 0.5072	TestAcc 0.5068	BestValid 0.5209
	Epoch 1900:	Loss 1.4129	TrainAcc 0.5231	ValidAcc 0.5164	TestAcc 0.5166	BestValid 0.5209
	Epoch 1925:	Loss 1.3956	TrainAcc 0.5227	ValidAcc 0.5163	TestAcc 0.5157	BestValid 0.5209
	Epoch 1950:	Loss 1.4017	TrainAcc 0.5248	ValidAcc 0.5180	TestAcc 0.5185	BestValid 0.5209
	Epoch 1975:	Loss 1.3995	TrainAcc 0.5277	ValidAcc 0.5189	TestAcc 0.5208	BestValid 0.5209
	Epoch 2000:	Loss 1.4051	TrainAcc 0.5271	ValidAcc 0.5206	TestAcc 0.5207	BestValid 0.5209
	Epoch 2025:	Loss 1.3989	TrainAcc 0.5206	ValidAcc 0.5134	TestAcc 0.5132	BestValid 0.5209
	Epoch 2050:	Loss 1.4016	TrainAcc 0.5262	ValidAcc 0.5187	TestAcc 0.5192	BestValid 0.5209
	Epoch 2075:	Loss 1.3978	TrainAcc 0.5266	ValidAcc 0.5185	TestAcc 0.5196	BestValid 0.5209
	Epoch 2100:	Loss 1.4061	TrainAcc 0.5260	ValidAcc 0.5189	TestAcc 0.5192	BestValid 0.5209
	Epoch 2125:	Loss 1.4036	TrainAcc 0.5167	ValidAcc 0.5118	TestAcc 0.5092	BestValid 0.5209
	Epoch 2150:	Loss 1.3998	TrainAcc 0.5257	ValidAcc 0.5192	TestAcc 0.5197	BestValid 0.5209
	Epoch 2175:	Loss 1.3969	TrainAcc 0.5189	ValidAcc 0.5122	TestAcc 0.5116	BestValid 0.5209
	Epoch 2200:	Loss 1.4068	TrainAcc 0.5239	ValidAcc 0.5168	TestAcc 0.5156	BestValid 0.5209
	Epoch 2225:	Loss 1.4080	TrainAcc 0.5190	ValidAcc 0.5114	TestAcc 0.5116	BestValid 0.5209
	Epoch 2250:	Loss 1.3991	TrainAcc 0.5292	ValidAcc 0.5206	TestAcc 0.5215	BestValid 0.5209
	Epoch 2275:	Loss 1.3951	TrainAcc 0.5242	ValidAcc 0.5174	TestAcc 0.5183	BestValid 0.5209
	Epoch 2300:	Loss 1.3990	TrainAcc 0.5262	ValidAcc 0.5182	TestAcc 0.5188	BestValid 0.5209
	Epoch 2325:	Loss 1.3925	TrainAcc 0.5219	ValidAcc 0.5132	TestAcc 0.5131	BestValid 0.5209
	Epoch 2350:	Loss 1.3905	TrainAcc 0.5267	ValidAcc 0.5192	TestAcc 0.5199	BestValid 0.5209
	Epoch 2375:	Loss 1.3906	TrainAcc 0.5276	ValidAcc 0.5203	TestAcc 0.5206	BestValid 0.5209
	Epoch 2400:	Loss 1.3969	TrainAcc 0.5139	ValidAcc 0.5082	TestAcc 0.5066	BestValid 0.5209
	Epoch 2425:	Loss 1.4047	TrainAcc 0.5282	ValidAcc 0.5208	TestAcc 0.5203	BestValid 0.5209
	Epoch 2450:	Loss 1.4061	TrainAcc 0.5219	ValidAcc 0.5143	TestAcc 0.5131	BestValid 0.5209
	Epoch 2475:	Loss 1.3932	TrainAcc 0.5163	ValidAcc 0.5094	TestAcc 0.5086	BestValid 0.5209
	Epoch 2500:	Loss 1.3984	TrainAcc 0.5244	ValidAcc 0.5169	TestAcc 0.5157	BestValid 0.5209
	Epoch 2525:	Loss 1.3921	TrainAcc 0.5219	ValidAcc 0.5144	TestAcc 0.5137	BestValid 0.5209
	Epoch 2550:	Loss 1.4152	TrainAcc 0.5194	ValidAcc 0.5121	TestAcc 0.5119	BestValid 0.5209
	Epoch 2575:	Loss 1.4063	TrainAcc 0.5102	ValidAcc 0.5029	TestAcc 0.5019	BestValid 0.5209
	Epoch 2600:	Loss 1.3911	TrainAcc 0.5259	ValidAcc 0.5180	TestAcc 0.5193	BestValid 0.5209
	Epoch 2625:	Loss 1.3979	TrainAcc 0.5210	ValidAcc 0.5125	TestAcc 0.5128	BestValid 0.5209
	Epoch 2650:	Loss 1.4067	TrainAcc 0.5255	ValidAcc 0.5174	TestAcc 0.5178	BestValid 0.5209
	Epoch 2675:	Loss 1.3843	TrainAcc 0.5198	ValidAcc 0.5112	TestAcc 0.5113	BestValid 0.5209
	Epoch 2700:	Loss 1.4114	TrainAcc 0.5237	ValidAcc 0.5156	TestAcc 0.5151	BestValid 0.5209
	Epoch 2725:	Loss 1.3887	TrainAcc 0.5306	ValidAcc 0.5214	TestAcc 0.5223	BestValid 0.5214
	Epoch 2750:	Loss 1.3918	TrainAcc 0.5271	ValidAcc 0.5191	TestAcc 0.5199	BestValid 0.5214
	Epoch 2775:	Loss 1.3981	TrainAcc 0.5228	ValidAcc 0.5146	TestAcc 0.5137	BestValid 0.5214
	Epoch 2800:	Loss 1.3839	TrainAcc 0.5265	ValidAcc 0.5174	TestAcc 0.5188	BestValid 0.5214
	Epoch 2825:	Loss 1.3990	TrainAcc 0.5216	ValidAcc 0.5135	TestAcc 0.5138	BestValid 0.5214
	Epoch 2850:	Loss 1.3898	TrainAcc 0.5194	ValidAcc 0.5108	TestAcc 0.5109	BestValid 0.5214
	Epoch 2875:	Loss 1.3906	TrainAcc 0.5254	ValidAcc 0.5170	TestAcc 0.5163	BestValid 0.5214
	Epoch 2900:	Loss 1.4009	TrainAcc 0.5303	ValidAcc 0.5225	TestAcc 0.5226	BestValid 0.5225
	Epoch 2925:	Loss 1.4022	TrainAcc 0.5290	ValidAcc 0.5209	TestAcc 0.5223	BestValid 0.5225
	Epoch 2950:	Loss 1.3849	TrainAcc 0.5301	ValidAcc 0.5211	TestAcc 0.5226	BestValid 0.5225
	Epoch 2975:	Loss 1.3976	TrainAcc 0.5286	ValidAcc 0.5195	TestAcc 0.5202	BestValid 0.5225
	Epoch 3000:	Loss 1.3867	TrainAcc 0.5265	ValidAcc 0.5177	TestAcc 0.5181	BestValid 0.5225
	Epoch 3025:	Loss 1.3963	TrainAcc 0.5275	ValidAcc 0.5173	TestAcc 0.5189	BestValid 0.5225
	Epoch 3050:	Loss 1.3855	TrainAcc 0.5240	ValidAcc 0.5151	TestAcc 0.5153	BestValid 0.5225
	Epoch 3075:	Loss 1.3984	TrainAcc 0.5253	ValidAcc 0.5178	TestAcc 0.5184	BestValid 0.5225
	Epoch 3100:	Loss 1.3878	TrainAcc 0.5200	ValidAcc 0.5111	TestAcc 0.5103	BestValid 0.5225
	Epoch 3125:	Loss 1.3947	TrainAcc 0.5231	ValidAcc 0.5141	TestAcc 0.5134	BestValid 0.5225
	Epoch 3150:	Loss 1.4156	TrainAcc 0.5281	ValidAcc 0.5191	TestAcc 0.5189	BestValid 0.5225
	Epoch 3175:	Loss 1.3914	TrainAcc 0.5173	ValidAcc 0.5105	TestAcc 0.5084	BestValid 0.5225
	Epoch 3200:	Loss 1.3826	TrainAcc 0.5258	ValidAcc 0.5161	TestAcc 0.5160	BestValid 0.5225
	Epoch 3225:	Loss 1.3993	TrainAcc 0.5239	ValidAcc 0.5146	TestAcc 0.5136	BestValid 0.5225
	Epoch 3250:	Loss 1.3970	TrainAcc 0.5227	ValidAcc 0.5138	TestAcc 0.5134	BestValid 0.5225
	Epoch 3275:	Loss 1.3817	TrainAcc 0.5213	ValidAcc 0.5130	TestAcc 0.5116	BestValid 0.5225
	Epoch 3300:	Loss 1.3791	TrainAcc 0.5263	ValidAcc 0.5169	TestAcc 0.5175	BestValid 0.5225
	Epoch 3325:	Loss 1.3930	TrainAcc 0.5236	ValidAcc 0.5145	TestAcc 0.5132	BestValid 0.5225
	Epoch 3350:	Loss 1.3804	TrainAcc 0.5217	ValidAcc 0.5135	TestAcc 0.5123	BestValid 0.5225
	Epoch 3375:	Loss 1.4023	TrainAcc 0.5214	ValidAcc 0.5125	TestAcc 0.5115	BestValid 0.5225
	Epoch 3400:	Loss 1.3837	TrainAcc 0.5096	ValidAcc 0.5042	TestAcc 0.5007	BestValid 0.5225
	Epoch 3425:	Loss 1.3745	TrainAcc 0.5229	ValidAcc 0.5134	TestAcc 0.5133	BestValid 0.5225
	Epoch 3450:	Loss 1.3817	TrainAcc 0.5209	ValidAcc 0.5107	TestAcc 0.5114	BestValid 0.5225
	Epoch 3475:	Loss 1.3922	TrainAcc 0.5165	ValidAcc 0.5095	TestAcc 0.5073	BestValid 0.5225
	Epoch 3500:	Loss 1.3891	TrainAcc 0.5270	ValidAcc 0.5168	TestAcc 0.5185	BestValid 0.5225
	Epoch 3525:	Loss 1.3983	TrainAcc 0.5174	ValidAcc 0.5086	TestAcc 0.5084	BestValid 0.5225
	Epoch 3550:	Loss 1.3942	TrainAcc 0.5283	ValidAcc 0.5186	TestAcc 0.5196	BestValid 0.5225
	Epoch 3575:	Loss 1.3862	TrainAcc 0.5201	ValidAcc 0.5120	TestAcc 0.5116	BestValid 0.5225
	Epoch 3600:	Loss 1.3826	TrainAcc 0.5252	ValidAcc 0.5161	TestAcc 0.5179	BestValid 0.5225
	Epoch 3625:	Loss 1.3875	TrainAcc 0.5239	ValidAcc 0.5138	TestAcc 0.5152	BestValid 0.5225
	Epoch 3650:	Loss 1.3933	TrainAcc 0.5268	ValidAcc 0.5178	TestAcc 0.5199	BestValid 0.5225
	Epoch 3675:	Loss 1.3873	TrainAcc 0.5220	ValidAcc 0.5121	TestAcc 0.5118	BestValid 0.5225
	Epoch 3700:	Loss 1.3846	TrainAcc 0.5172	ValidAcc 0.5090	TestAcc 0.5075	BestValid 0.5225
	Epoch 3725:	Loss 1.3697	TrainAcc 0.5207	ValidAcc 0.5134	TestAcc 0.5120	BestValid 0.5225
	Epoch 3750:	Loss 1.3741	TrainAcc 0.5221	ValidAcc 0.5111	TestAcc 0.5116	BestValid 0.5225
	Epoch 3775:	Loss 1.3757	TrainAcc 0.5214	ValidAcc 0.5122	TestAcc 0.5121	BestValid 0.5225
	Epoch 3800:	Loss 1.3983	TrainAcc 0.5214	ValidAcc 0.5105	TestAcc 0.5102	BestValid 0.5225
	Epoch 3825:	Loss 1.3790	TrainAcc 0.5245	ValidAcc 0.5148	TestAcc 0.5142	BestValid 0.5225
	Epoch 3850:	Loss 1.4037	TrainAcc 0.5083	ValidAcc 0.5015	TestAcc 0.4976	BestValid 0.5225
	Epoch 3875:	Loss 1.3829	TrainAcc 0.5213	ValidAcc 0.5125	TestAcc 0.5128	BestValid 0.5225
	Epoch 3900:	Loss 1.3749	TrainAcc 0.5273	ValidAcc 0.5172	TestAcc 0.5191	BestValid 0.5225
	Epoch 3925:	Loss 1.3772	TrainAcc 0.5163	ValidAcc 0.5081	TestAcc 0.5061	BestValid 0.5225
	Epoch 3950:	Loss 1.3721	TrainAcc 0.5228	ValidAcc 0.5135	TestAcc 0.5138	BestValid 0.5225
	Epoch 3975:	Loss 1.3778	TrainAcc 0.5295	ValidAcc 0.5191	TestAcc 0.5209	BestValid 0.5225
	Epoch 4000:	Loss 1.3780	TrainAcc 0.5269	ValidAcc 0.5177	TestAcc 0.5189	BestValid 0.5225
	Epoch 4025:	Loss 1.3714	TrainAcc 0.5264	ValidAcc 0.5151	TestAcc 0.5162	BestValid 0.5225
	Epoch 4050:	Loss 1.3745	TrainAcc 0.5191	ValidAcc 0.5097	TestAcc 0.5092	BestValid 0.5225
	Epoch 4075:	Loss 1.3750	TrainAcc 0.5172	ValidAcc 0.5070	TestAcc 0.5073	BestValid 0.5225
	Epoch 4100:	Loss 1.3754	TrainAcc 0.5274	ValidAcc 0.5176	TestAcc 0.5197	BestValid 0.5225
	Epoch 4125:	Loss 1.3780	TrainAcc 0.5135	ValidAcc 0.5065	TestAcc 0.5041	BestValid 0.5225
	Epoch 4150:	Loss 1.3841	TrainAcc 0.5226	ValidAcc 0.5130	TestAcc 0.5126	BestValid 0.5225
	Epoch 4175:	Loss 1.3821	TrainAcc 0.5230	ValidAcc 0.5128	TestAcc 0.5118	BestValid 0.5225
	Epoch 4200:	Loss 1.3912	TrainAcc 0.5319	ValidAcc 0.5211	TestAcc 0.5229	BestValid 0.5225
	Epoch 4225:	Loss 1.3758	TrainAcc 0.5286	ValidAcc 0.5172	TestAcc 0.5195	BestValid 0.5225
	Epoch 4250:	Loss 1.3766	TrainAcc 0.5285	ValidAcc 0.5188	TestAcc 0.5203	BestValid 0.5225
	Epoch 4275:	Loss 1.3702	TrainAcc 0.5173	ValidAcc 0.5069	TestAcc 0.5071	BestValid 0.5225
	Epoch 4300:	Loss 1.3717	TrainAcc 0.5262	ValidAcc 0.5141	TestAcc 0.5155	BestValid 0.5225
	Epoch 4325:	Loss 1.3734	TrainAcc 0.5228	ValidAcc 0.5125	TestAcc 0.5115	BestValid 0.5225
	Epoch 4350:	Loss 1.3688	TrainAcc 0.5269	ValidAcc 0.5162	TestAcc 0.5167	BestValid 0.5225
	Epoch 4375:	Loss 1.3723	TrainAcc 0.5081	ValidAcc 0.5003	TestAcc 0.4977	BestValid 0.5225
	Epoch 4400:	Loss 1.3690	TrainAcc 0.5166	ValidAcc 0.5074	TestAcc 0.5064	BestValid 0.5225
	Epoch 4425:	Loss 1.3710	TrainAcc 0.5140	ValidAcc 0.5062	TestAcc 0.5041	BestValid 0.5225
	Epoch 4450:	Loss 1.3849	TrainAcc 0.5284	ValidAcc 0.5164	TestAcc 0.5184	BestValid 0.5225
	Epoch 4475:	Loss 1.3705	TrainAcc 0.5174	ValidAcc 0.5072	TestAcc 0.5069	BestValid 0.5225
	Epoch 4500:	Loss 1.3905	TrainAcc 0.5210	ValidAcc 0.5111	TestAcc 0.5109	BestValid 0.5225
	Epoch 4525:	Loss 1.3838	TrainAcc 0.5255	ValidAcc 0.5147	TestAcc 0.5149	BestValid 0.5225
	Epoch 4550:	Loss 1.4058	TrainAcc 0.4995	ValidAcc 0.4938	TestAcc 0.4898	BestValid 0.5225
	Epoch 4575:	Loss 1.3712	TrainAcc 0.5182	ValidAcc 0.5080	TestAcc 0.5075	BestValid 0.5225
	Epoch 4600:	Loss 1.3679	TrainAcc 0.5248	ValidAcc 0.5129	TestAcc 0.5132	BestValid 0.5225
	Epoch 4625:	Loss 1.3682	TrainAcc 0.5273	ValidAcc 0.5162	TestAcc 0.5166	BestValid 0.5225
	Epoch 4650:	Loss 1.3709	TrainAcc 0.5162	ValidAcc 0.5072	TestAcc 0.5045	BestValid 0.5225
	Epoch 4675:	Loss 1.3731	TrainAcc 0.5213	ValidAcc 0.5120	TestAcc 0.5117	BestValid 0.5225
	Epoch 4700:	Loss 1.3794	TrainAcc 0.5172	ValidAcc 0.5072	TestAcc 0.5057	BestValid 0.5225
	Epoch 4725:	Loss 1.4190	TrainAcc 0.5230	ValidAcc 0.5108	TestAcc 0.5105	BestValid 0.5225
	Epoch 4750:	Loss 1.3763	TrainAcc 0.5293	ValidAcc 0.5192	TestAcc 0.5205	BestValid 0.5225
	Epoch 4775:	Loss 1.3758	TrainAcc 0.5251	ValidAcc 0.5134	TestAcc 0.5140	BestValid 0.5225
	Epoch 4800:	Loss 1.3794	TrainAcc 0.5194	ValidAcc 0.5092	TestAcc 0.5083	BestValid 0.5225
	Epoch 4825:	Loss 1.3606	TrainAcc 0.5208	ValidAcc 0.5101	TestAcc 0.5095	BestValid 0.5225
	Epoch 4850:	Loss 1.3772	TrainAcc 0.5164	ValidAcc 0.5077	TestAcc 0.5060	BestValid 0.5225
	Epoch 4875:	Loss 1.3824	TrainAcc 0.5121	ValidAcc 0.5026	TestAcc 0.5004	BestValid 0.5225
	Epoch 4900:	Loss 1.3703	TrainAcc 0.5290	ValidAcc 0.5188	TestAcc 0.5186	BestValid 0.5225
	Epoch 4925:	Loss 1.4115	TrainAcc 0.5105	ValidAcc 0.5004	TestAcc 0.4990	BestValid 0.5225
	Epoch 4950:	Loss 1.3712	TrainAcc 0.5223	ValidAcc 0.5112	TestAcc 0.5115	BestValid 0.5225
	Epoch 4975:	Loss 1.3694	TrainAcc 0.5223	ValidAcc 0.5114	TestAcc 0.5104	BestValid 0.5225
	Epoch 5000:	Loss 1.3740	TrainAcc 0.5155	ValidAcc 0.5071	TestAcc 0.5054	BestValid 0.5225
Node 4, Pre/Post-Pipelining: 0.152 / 3.298 ms, Bubble: 27.844 ms, Compute: 107.854 ms, Comm: 16.363 ms, Imbalance: 6.807 ms
Node 6, Pre/Post-Pipelining: 0.252 / 3.190 ms, Bubble: 28.625 ms, Compute: 105.543 ms, Comm: 15.391 ms, Imbalance: 9.379 ms
Node 7, Pre/Post-Pipelining: 0.279 / 6.324 ms, Bubble: 26.651 ms, Compute: 96.832 ms, Comm: 11.045 ms, Imbalance: 21.424 ms
Node 1, Pre/Post-Pipelining: 0.190 / 3.251 ms, Bubble: 27.376 ms, Compute: 108.288 ms, Comm: 15.699 ms, Imbalance: 7.383 ms
Node 2, Pre/Post-Pipelining: 0.191 / 3.239 ms, Bubble: 27.633 ms, Compute: 106.281 ms, Comm: 17.209 ms, Imbalance: 7.793 ms
Node 3, Pre/Post-Pipelining: 0.214 / 3.206 ms, Bubble: 27.498 ms, Compute: 107.701 ms, Comm: 16.177 ms, Imbalance: 7.411 ms
Node 0, Pre/Post-Pipelining: 0.129 / 3.330 ms, Bubble: 27.469 ms, Compute: 115.238 ms, Comm: 11.454 ms, Imbalance: 4.462 ms
Node 5, Pre/Post-Pipelining: 0.193 / 3.245 ms, Bubble: 28.101 ms, Compute: 107.738 ms, Comm: 16.534 ms, Imbalance: 6.454 ms
Cluster-Wide Average, Pre-Pipelining Overhead: 0.129 ms
Cluster-Wide Average, Post-Pipelining Overhead: 3.330 ms
Cluster-Wide Average, Bubble: 27.469 ms
Cluster-Wide Average, Compute: 115.238 ms
Cluster-Wide Average, Communication: 11.454 ms
Cluster-Wide Average, Imbalance: 4.462 ms
Node 4, GPU memory consumption: 3.071 GB
Node 6, GPU memory consumption: 3.094 GB
Node 0, GPU memory consumption: 3.913 GB
Node 7, GPU memory consumption: 2.782 GB
Node 2, GPU memory consumption: 3.094 GB
Node 1, GPU memory consumption: 3.094 GB
Node 5, GPU memory consumption: 3.094 GB
Node 3, GPU memory consumption: 3.071 GB
Node 4, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 0, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 5, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 1, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 2, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 3, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 6, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
Node 7, Graph-Level Communication Throughput: -nan Gbps, Time: 0.000 ms
------------------------node id 4,  per-epoch time: 0.545781 s---------------
------------------------node id 0,  per-epoch time: 0.545781 s---------------
------------------------node id 5,  per-epoch time: 0.545782 s---------------
------------------------node id 1,  per-epoch time: 0.545781 s---------------
------------------------node id 6,  per-epoch time: 0.545781 s---------------
------------------------node id 2,  per-epoch time: 0.545781 s---------------
------------------------node id 7,  per-epoch time: 0.545782 s---------------
------------------------node id 3,  per-epoch time: 0.545781 s---------------
************ Profiling Results ************
	Bubble: 439.285901 (ms) (80.53 percentage)
	Compute: 99.644047 (ms) (18.27 percentage)
	GraphCommComputeOverhead: 0.000000 (ms) (0.00 percentage)
	GraphCommNetwork: 0.000000 (ms) (0.00 percentage)
	LayerCommNetwork: 0.000000 (ms) (0.00 percentage)
	Optimization: 0.000000 (ms) (0.00 percentage)
	Other: 6.578422 (ms) (1.21 percentage)
	Layer-level communication (cluster-wide, per-epoch): 0.931 GB
	Graph-level communication (cluster-wide, per-epoch): 0.000 GB
	Weight-sync communication (cluster-wide, per-epoch): 0.010 GB
	Total communication (cluster-wide, per-epoch): 0.941 GB
	Aggregated layer-level communication throughput: 533.691 Gbps
Highest valid_acc: 0.5225
Target test_acc: 0.5226
Epoch to reach the target acc: 2899
[MPI Rank 4] Success 
[MPI Rank 0] Success 
[MPI Rank 5] Success 
[MPI Rank 1] Success 
[MPI Rank 6] Success 
[MPI Rank 2] Success 
[MPI Rank 7] Success 
[MPI Rank 3] Success 
