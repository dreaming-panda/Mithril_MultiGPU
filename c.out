Sat Sep  9 23:48:34 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   40C    P8    16W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   43C    P8    25W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   44C    P8    23W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   43C    P8    15W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
gnerv2
gnerv2
gnerv2
gnerv2
[ 25%] Built target core
[ 25%] Built target context
[ 77%] Built target cudahelp
[ 83%] Built target OSDI2023_MULTI_NODES_gcnii
[ 88%] Built target OSDI2023_MULTI_NODES_graphsage
[ 88%] Built target estimate_comm_volume
[100%] Built target OSDI2023_MULTI_NODES_gcn
Running experiments...
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INIT
DONE MPI INIT
Initialized node 3 on machine gnerv2
Initialized node 0 on machine gnerv2
DONE MPI INIT
Initialized node 1 on machine gnerv2
DONE MPI INIT
Initialized node 2 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.290 seconds.
Building the CSC structure...
        It takes 0.296 seconds.
Building the CSC structure...
        It takes 0.300 seconds.
Building the CSC structure...
        It takes 0.327 seconds.
Building the CSC structure...
        It takes 0.291 seconds.
        It takes 0.291 seconds.
        It takes 0.325 seconds.
        It takes 0.316 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.481 seconds.
Building the Label Vector...
        It takes 0.530 seconds.
Building the Label Vector...
        It takes 0.504 seconds.
Building the Label Vector...
        It takes 0.527 seconds.
Building the Label Vector...
        It takes 0.181 seconds.
        It takes 0.199 seconds.
        It takes 0.190 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/yelp/16_parts
The number of GCNII layers: 8
The number of hidden units: 128
The number of training epoches: 1000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.100
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 100
Number of feature dimensions: 300
Number of vertices: 716847
Number of GPUs: 4
        It takes 0.192 seconds.
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 44803
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 537635, valid nodes 107527, test nodes 71685
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 16): 0-[0, 43495) 1-[43495, 89644) 2-[89644, 135406) 3-[135406, 178980) 4-[178980, 225129) 5-[225129, 271278) 6-[271278, 317427) 7-[317427, 363576) 8-[363576, 407071) ... 15-[673353, 716847)
GPU 0, layer [0, 4)
GPU 1, layer [4, 8)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 44803
716847, 13954819, 13954819
Number of vertices per chunk: 44803
716847, 13954819, 13954819
Number of vertices per chunk: 44803
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 114.429 Gbps (per GPU), 457.714 Gbps (aggregated)
The layer-level communication performance: 115.066 Gbps (per GPU), 460.263 Gbps (aggregated)
The layer-level communication performance: 114.137 Gbps (per GPU), 456.547 Gbps (aggregated)
The layer-level communication performance: 114.907 Gbps (per GPU), 459.626 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.255 Gbps (per GPU), 621.022 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.230 Gbps (per GPU), 620.918 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.255 Gbps (per GPU), 621.022 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.232 Gbps (per GPU), 620.930 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 91.064 Gbps (per GPU), 364.258 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 91.064 Gbps (per GPU), 364.255 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 91.064 Gbps (per GPU), 364.255 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 91.064 Gbps (per GPU), 364.257 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  5.01ms  3.49ms  2.74ms  1.83 43.49K  0.57M
 chk_1  6.48ms  4.85ms  4.06ms  1.60 46.15K  2.10M
 chk_2  5.58ms  3.96ms  3.20ms  1.74 45.76K  1.16M
 chk_3  5.22ms  3.67ms  2.93ms  1.78 43.57K  0.96M
 chk_4  4.88ms  3.23ms  2.45ms  1.99 46.15K  0.50M
 chk_5  5.04ms  3.39ms  2.62ms  1.92 46.15K  0.51M
 chk_6  5.34ms  3.68ms  2.90ms  1.84 46.15K  0.86M
 chk_7  5.43ms  4.21ms  3.01ms  1.80 46.15K  1.00M
 chk_8  5.02ms  3.47ms  3.07ms  1.64 43.49K  0.58M
 chk_9  5.64ms  4.00ms  3.21ms  1.76 46.14K  0.81M
chk_10  5.50ms  3.48ms  2.73ms  2.01 43.95K  0.71M
chk_11  4.95ms  3.41ms  2.68ms  1.85 43.49K  0.61M
chk_12  4.74ms  3.19ms  2.45ms  1.93 43.49K  0.37M
chk_13  5.76ms  4.11ms  3.34ms  1.72 45.71K  1.06M
chk_14  5.30ms  3.75ms  3.01ms  1.76 43.49K  0.81M
chk_15  5.15ms  3.59ms  2.84ms  1.81 43.49K  0.62M
   Avg  5.32  3.72  2.95
   Max  6.48  4.85  4.06
   Min  4.74  3.19  2.45
 Ratio  1.37  1.52  1.66
   Var  0.17  0.17  0.15
Profiling takes 2.287 s
*** Node 0, starting model training...
Num Stages: 2 / 2
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 34)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 363576
*** Node 1, starting model training...
Num Stages: 2 / 2
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [0, 34)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 363576, Num Local Vertices: 353271
*** Node 2, starting model training...
Num Stages: 2 / 2
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [34, 64)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 363576
*** Node 3, starting model training...
Num Stages: 2 / 2
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [34, 64)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 363576, Num Local Vertices: 353271
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 34)...
+++++++++ Node 1 initializing the weights for op[0, 34)...
+++++++++ Node 3 initializing the weights for op[34, 64)...
+++++++++ Node 2 initializing the weights for op[34, 64)...
Node 2, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 583872
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 0.7160	TrainAcc 0.1602	ValidAcc 0.1591	TestAcc 0.1593	BestValid 0.1591
	Epoch 50:	Loss 0.2652	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.1591
	Epoch 100:	Loss 0.2529	TrainAcc 0.1417	ValidAcc 0.1424	TestAcc 0.1421	BestValid 0.1591
	Epoch 150:	Loss 0.2402	TrainAcc 0.2287	ValidAcc 0.2289	TestAcc 0.2294	BestValid 0.2289
	Epoch 200:	Loss 0.2306	TrainAcc 0.2991	ValidAcc 0.2990	TestAcc 0.2995	BestValid 0.2990
	Epoch 250:	Loss 0.2256	TrainAcc 0.3894	ValidAcc 0.3889	TestAcc 0.3903	BestValid 0.3889
	Epoch 300:	Loss 0.2218	TrainAcc 0.4537	ValidAcc 0.4535	TestAcc 0.4549	BestValid 0.4535
	Epoch 350:	Loss 0.2189	TrainAcc 0.4675	ValidAcc 0.4671	TestAcc 0.4685	BestValid 0.4671
	Epoch 400:	Loss 0.2169	TrainAcc 0.4743	ValidAcc 0.4741	TestAcc 0.4753	BestValid 0.4741
	Epoch 450:	Loss 0.2151	TrainAcc 0.4792	ValidAcc 0.4786	TestAcc 0.4801	BestValid 0.4786
	Epoch 500:	Loss 0.2135	TrainAcc 0.4869	ValidAcc 0.4861	TestAcc 0.4880	BestValid 0.4861
	Epoch 550:	Loss 0.2118	TrainAcc 0.4914	ValidAcc 0.4903	TestAcc 0.4922	BestValid 0.4903
	Epoch 600:	Loss 0.2104	TrainAcc 0.4942	ValidAcc 0.4929	TestAcc 0.4950	BestValid 0.4929
	Epoch 650:	Loss 0.2093	TrainAcc 0.4957	ValidAcc 0.4944	TestAcc 0.4961	BestValid 0.4944
	Epoch 700:	Loss 0.2083	TrainAcc 0.4991	ValidAcc 0.4978	TestAcc 0.4994	BestValid 0.4978
	Epoch 750:	Loss 0.2073	TrainAcc 0.5022	ValidAcc 0.5008	TestAcc 0.5023	BestValid 0.5008
	Epoch 800:	Loss 0.2064	TrainAcc 0.5051	ValidAcc 0.5036	TestAcc 0.5053	BestValid 0.5036
	Epoch 850:	Loss 0.2057	TrainAcc 0.5066	ValidAcc 0.5053	TestAcc 0.5067	BestValid 0.5053
	Epoch 900:	Loss 0.2051	TrainAcc 0.5109	ValidAcc 0.5091	TestAcc 0.5110	BestValid 0.5091
	Epoch 950:	Loss 0.2044	TrainAcc 0.5100	ValidAcc 0.5087	TestAcc 0.5099	BestValid 0.5091
	Epoch 1000:	Loss 0.2037	TrainAcc 0.5140	ValidAcc 0.5124	TestAcc 0.5140	BestValid 0.5124
****** Epoch Time (Excluding Evaluation Cost): 0.234 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 23.458 ms (Max: 25.409, Min: 21.532, Sum: 93.830)
Cluster-Wide Average, Compute: 125.609 ms (Max: 137.224, Min: 113.971, Sum: 502.435)
Cluster-Wide Average, Communication-Layer: 25.486 ms (Max: 26.744, Min: 24.210, Sum: 101.943)
Cluster-Wide Average, Bubble-Imbalance: 7.053 ms (Max: 14.431, Min: -0.342, Sum: 28.213)
Cluster-Wide Average, Communication-Graph: 42.974 ms (Max: 45.682, Min: 40.273, Sum: 171.896)
Cluster-Wide Average, Optimization: 2.459 ms (Max: 2.502, Min: 2.418, Sum: 9.836)
Cluster-Wide Average, Others: 7.441 ms (Max: 8.434, Min: 6.475, Sum: 29.766)
****** Breakdown Sum: 234.480 ms ******
Cluster-Wide Average, GPU Memory Consumption: 8.568 GB (Max: 9.467, Min: 7.874, Sum: 34.274)
Cluster-Wide Average, Graph-Level Communication Throughput: 131.623 Gbps (Max: 138.059, Min: 125.658, Sum: 526.494)
Cluster-Wide Average, Layer-Level Communication Throughput: 57.667 Gbps (Max: 59.768, Min: 55.684, Sum: 230.667)
Layer-level communication (cluster-wide, per-epoch): 0.684 GB
Graph-level communication (cluster-wide, per-epoch): 2.227 GB
Weight-sync communication (cluster-wide, per-epoch): 0.002 GB
Total communication (cluster-wide, per-epoch): 2.913 GB
****** Accuracy Results ******
Highest valid_acc: 0.5124
Target test_acc: 0.5140
Epoch to reach the target acc: 999
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
