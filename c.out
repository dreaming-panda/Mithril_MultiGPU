Fri Sep 15 12:00:51 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   39C    P8    16W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   41C    P8    16W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   41C    P8    17W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   46C    P8    17W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
gnerv4
gnerv4
gnerv4
gnerv4
[ 18%] Built target context
[ 34%] Built target core
[ 73%] Built target cudahelp
[ 86%] Built target OSDI2023_MULTI_NODES_graphsage
[ 86%] Built target OSDI2023_MULTI_NODES_gcnii
[ 86%] Built target estimate_comm_volume
[ 86%] Built target OSDI2023_MULTI_NODES_resgcn
[100%] Built target OSDI2023_MULTI_NODES_gcn
Running experiments...
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 2 on machine gnerv4
DONE MPI INIT
Initialized node 3 on machine gnerv4
DONE MPI INIT
Initialized node 1 on machine gnerv4

Initialized node 0 on machine gnerv4
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.009 seconds.
Building the CSC structure...
        It takes 0.000 seconds.
        It takes 0.000 seconds.
        It takes 0.000 seconds.
        It takes 0.000 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.050 seconds.
        It takes 0.050 seconds.
        It takes 0.050 seconds.
        It takes 0.050 seconds.
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
Building the Label Vector...
        It takes 0.001 seconds.
        It takes 0.001 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/cora/4_parts
The number of GCNII layers: 16
The number of hidden units: 128
The number of training epoches: 1500
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 7
Number of feature dimensions: 1433
Number of vertices: 2708
Number of GPUs: 4
        It takes 0.001 seconds.
        It takes 0.001 seconds.
GPU 0, layer [0, 16)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
2708, 13264, 13264
Number of vertices per chunk: 677
csr in-out ready !Start Cost Model Initialization...
GPU 0, layer [0, 16)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
2708, 13264, 13264
Number of vertices per chunk: 677
csr in-out ready !Start Cost Model Initialization...
train nodes 140, valid nodes 500, test nodes 1000
GPU 0, layer [0, 16)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 4): 0-[0, 677) 1-[677, 1354) 2-[1354, 2031) 3-[2031, 2708)
2708, 13264, 13264
Number of vertices per chunk: 677
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
GPU 0, layer [0, 16)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
2708, 13264, 13264
Number of vertices per chunk: 677
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 114.868 Gbps (per GPU), 459.470 Gbps (aggregated)
The layer-level communication performance: 114.767 Gbps (per GPU), 459.068 Gbps (aggregated)
The layer-level communication performance: 115.361 Gbps (per GPU), 461.445 Gbps (aggregated)
The layer-level communication performance: 114.568 Gbps (per GPU), 458.272 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 158.779 Gbps (per GPU), 635.116 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.782 Gbps (per GPU), 635.128 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.785 Gbps (per GPU), 635.140 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 158.782 Gbps (per GPU), 635.128 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 106.424 Gbps (per GPU), 425.696 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 106.419 Gbps (per GPU), 425.676 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 106.428 Gbps (per GPU), 425.713 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 106.425 Gbps (per GPU), 425.699 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  0.80ms  0.53ms  0.62ms  1.52  0.68K  0.00M
 chk_1  0.81ms  0.53ms  0.62ms  1.54  0.68K  0.00M
 chk_2  0.81ms  0.53ms  0.62ms  1.54  0.68K  0.00M
 chk_3  0.80ms  0.53ms  0.70ms  1.52  0.68K  0.00M
   Avg  0.81  0.53  0.64
   Max  0.81  0.53  0.70
   Min  0.80  0.53  0.62
 Ratio  1.01  1.01  1.13
   Var  0.00  0.00  0.00
Profiling takes 0.150 s
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 199)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 677
*** Node 1, starting model training...
*** Node 2, starting model training...
*** Node 3, starting model training...
Num Stages: 1 / 1
Node 2, Pipeline Input Tensor: NULL
Node 2, Pipeline Output Tensor: NULL
*** Node 2 owns the model-level partition [0, 199)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 1354, Num Local Vertices: 677
Num Stages: 1 / 1
Node 3, Pipeline Input Tensor: NULL
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [0, 199)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 2031, Num Local Vertices: 677
Num Stages: 1 / 1
Node 1, Pipeline Input Tensor: NULL
Node 1, Pipeline Output Tensor: NULL
*** Node 1 owns the model-level partition [0, 199)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 677, Num Local Vertices: 677
*** Node 0, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 3 initializing the weights for op[0, 199)...
+++++++++ Node 2 initializing the weights for op[0, 199)...
+++++++++ Node 0 initializing the weights for op[0, 199)...
+++++++++ Node 1 initializing the weights for op[0, 199)...
Node 1, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be sent across graph boundary...
Node 0, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 465
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 6.7626	TrainAcc 0.0786	ValidAcc 0.0620	TestAcc 0.0640	BestValid 0.0620
	Epoch 50:	Loss 0.4798	TrainAcc 0.9786	ValidAcc 0.5160	TestAcc 0.4950	BestValid 0.5160
	Epoch 100:	Loss 0.0942	TrainAcc 0.9929	ValidAcc 0.5360	TestAcc 0.5290	BestValid 0.5360
	Epoch 150:	Loss 0.2464	TrainAcc 1.0000	ValidAcc 0.5540	TestAcc 0.5530	BestValid 0.5540
	Epoch 200:	Loss 0.0630	TrainAcc 1.0000	ValidAcc 0.5700	TestAcc 0.5650	BestValid 0.5700
	Epoch 250:	Loss 0.0324	TrainAcc 1.0000	ValidAcc 0.5720	TestAcc 0.5680	BestValid 0.5720
	Epoch 300:	Loss 0.0135	TrainAcc 1.0000	ValidAcc 0.5560	TestAcc 0.5530	BestValid 0.5720
	Epoch 350:	Loss 0.0041	TrainAcc 1.0000	ValidAcc 0.5720	TestAcc 0.5670	BestValid 0.5720
	Epoch 400:	Loss 0.0206	TrainAcc 1.0000	ValidAcc 0.5680	TestAcc 0.5750	BestValid 0.5720
	Epoch 450:	Loss 0.0496	TrainAcc 1.0000	ValidAcc 0.5600	TestAcc 0.5740	BestValid 0.5720
	Epoch 500:	Loss 0.0029	TrainAcc 1.0000	ValidAcc 0.5620	TestAcc 0.5730	BestValid 0.5720
	Epoch 550:	Loss 0.0206	TrainAcc 1.0000	ValidAcc 0.5740	TestAcc 0.5760	BestValid 0.5740
	Epoch 600:	Loss 0.0414	TrainAcc 1.0000	ValidAcc 0.5760	TestAcc 0.5890	BestValid 0.5760
	Epoch 650:	Loss 0.0203	TrainAcc 1.0000	ValidAcc 0.5780	TestAcc 0.5850	BestValid 0.5780
	Epoch 700:	Loss 0.0026	TrainAcc 1.0000	ValidAcc 0.5860	TestAcc 0.5940	BestValid 0.5860
	Epoch 750:	Loss 0.0007	TrainAcc 1.0000	ValidAcc 0.5780	TestAcc 0.5870	BestValid 0.5860
	Epoch 800:	Loss 0.0062	TrainAcc 1.0000	ValidAcc 0.5880	TestAcc 0.6030	BestValid 0.5880
	Epoch 850:	Loss 0.0012	TrainAcc 1.0000	ValidAcc 0.5880	TestAcc 0.5960	BestValid 0.5880
	Epoch 900:	Loss 0.0005	TrainAcc 1.0000	ValidAcc 0.5880	TestAcc 0.6000	BestValid 0.5880
	Epoch 950:	Loss 0.0180	TrainAcc 1.0000	ValidAcc 0.5840	TestAcc 0.5930	BestValid 0.5880
	Epoch 1000:	Loss 0.0034	TrainAcc 1.0000	ValidAcc 0.5760	TestAcc 0.5770	BestValid 0.5880
	Epoch 1050:	Loss 0.0004	TrainAcc 1.0000	ValidAcc 0.5800	TestAcc 0.5870	BestValid 0.5880
	Epoch 1100:	Loss 0.0002	TrainAcc 1.0000	ValidAcc 0.5900	TestAcc 0.6020	BestValid 0.5900
	Epoch 1150:	Loss 0.0227	TrainAcc 1.0000	ValidAcc 0.5860	TestAcc 0.6040	BestValid 0.5900
	Epoch 1200:	Loss 0.0018	TrainAcc 1.0000	ValidAcc 0.5780	TestAcc 0.5940	BestValid 0.5900
	Epoch 1250:	Loss 0.0015	TrainAcc 1.0000	ValidAcc 0.5720	TestAcc 0.6000	BestValid 0.5900
	Epoch 1300:	Loss 0.0007	TrainAcc 1.0000	ValidAcc 0.5780	TestAcc 0.5920	BestValid 0.5900
	Epoch 1350:	Loss 0.0058	TrainAcc 1.0000	ValidAcc 0.5740	TestAcc 0.5850	BestValid 0.5900
	Epoch 1400:	Loss 0.0010	TrainAcc 1.0000	ValidAcc 0.5800	TestAcc 0.5960	BestValid 0.5900
	Epoch 1450:	Loss 0.0015	TrainAcc 1.0000	ValidAcc 0.5840	TestAcc 0.6040	BestValid 0.5900
	Epoch 1500:	Loss 0.0019	TrainAcc 1.0000	ValidAcc 0.5800	TestAcc 0.6010	BestValid 0.5900
****** Epoch Time (Excluding Evaluation Cost): 0.018 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.061 ms (Max: 0.089, Min: 0.033, Sum: 0.244)
Cluster-Wide Average, Compute: 10.193 ms (Max: 10.382, Min: 10.029, Sum: 40.774)
Cluster-Wide Average, Communication-Layer: 0.009 ms (Max: 0.009, Min: 0.008, Sum: 0.035)
Cluster-Wide Average, Bubble-Imbalance: 0.016 ms (Max: 0.016, Min: 0.015, Sum: 0.064)
Cluster-Wide Average, Communication-Graph: 3.303 ms (Max: 3.521, Min: 3.069, Sum: 13.211)
Cluster-Wide Average, Optimization: 3.928 ms (Max: 3.962, Min: 3.896, Sum: 15.711)
Cluster-Wide Average, Others: 0.692 ms (Max: 0.729, Min: 0.660, Sum: 2.768)
****** Breakdown Sum: 18.201 ms ******
Cluster-Wide Average, GPU Memory Consumption: 1.349 GB (Max: 1.524, Min: 1.264, Sum: 5.395)
Cluster-Wide Average, Graph-Level Communication Throughput: 6.931 Gbps (Max: 8.839, Min: 4.467, Sum: 27.725)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 0.007 GB
Weight-sync communication (cluster-wide, per-epoch): 0.016 GB
Total communication (cluster-wide, per-epoch): 0.023 GB
****** Accuracy Results ******
Highest valid_acc: 0.5900
Target test_acc: 0.6160
Epoch to reach the target acc: 1099
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
