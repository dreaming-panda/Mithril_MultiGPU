Fri Sep 15 12:02:52 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   33C    P8    16W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   34C    P8    16W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   34C    P8    17W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   38C    P8    17W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
gnerv4
gnerv4
gnerv4
gnerv4
[ 10%] Built target context
[ 34%] Built target core
[ 73%] Built target cudahelp
[ 97%] Built target OSDI2023_MULTI_NODES_resgcn
[ 97%] Built target OSDI2023_MULTI_NODES_gcnii
[ 97%] Built target estimate_comm_volume
[ 97%] Built target OSDI2023_MULTI_NODES_graphsage
[ 97%] Built target OSDI2023_MULTI_NODES_gcn
Running experiments...
Initializing the runtime environment
DONE MPI INIT
Initialized node 0 on machine gnerv4
Building the CSR structure...
        It takes 0.001 seconds.
Building the CSC structure...
        It takes 0.000 seconds.
Building the Feature Vector...
        It takes 0.019 seconds.
Building the Label Vector...
        It takes 0.000 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/cora/1_parts
The number of GCNII layers: 16
The number of hidden units: 128
The number of training epoches: 1500
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.500
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 7
Number of feature dimensions: 1433
Number of vertices: 2708
Number of GPUs: 1
train nodes 140, valid nodes 500, test nodes 1000
GPU 0, layer [0, 16)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 1): 0-[0, 2708)
2708, 13264, 13264
Number of vertices per chunk: 2708
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
The layer-level communication performance: 1677721.610 Gbps (per GPU), 1677721.610 Gbps (aggregated)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  1.92ms  1.25ms  1.35ms  1.55  2.71K  0.01M
   Avg  1.92  1.25  1.35
   Max  1.92  1.25  1.35
   Min  1.92  1.25  1.35
 Ratio  1.00  1.00  1.00
   Var  0.00  0.00  0.00
Profiling takes 0.118 s
*** Node 0, starting model training...
Num Stages: 1 / 1
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: NULL
*** Node 0 owns the model-level partition [0, 199)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 2708
*** Node 0, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 199)...
Node 0, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 6.2126	TrainAcc 0.1357	ValidAcc 0.1060	TestAcc 0.1060	BestValid 0.1060
	Epoch 50:	Loss 0.2934	TrainAcc 0.9929	ValidAcc 0.6560	TestAcc 0.6700	BestValid 0.6560
	Epoch 100:	Loss 0.0743	TrainAcc 1.0000	ValidAcc 0.6780	TestAcc 0.7060	BestValid 0.6780
	Epoch 150:	Loss 0.0263	TrainAcc 1.0000	ValidAcc 0.7000	TestAcc 0.7220	BestValid 0.7000
	Epoch 200:	Loss 0.0332	TrainAcc 1.0000	ValidAcc 0.6980	TestAcc 0.7130	BestValid 0.7000
	Epoch 250:	Loss 0.0221	TrainAcc 1.0000	ValidAcc 0.7100	TestAcc 0.7260	BestValid 0.7100
	Epoch 300:	Loss 0.0471	TrainAcc 1.0000	ValidAcc 0.7140	TestAcc 0.7280	BestValid 0.7140
	Epoch 350:	Loss 0.0400	TrainAcc 1.0000	ValidAcc 0.7180	TestAcc 0.7260	BestValid 0.7180
	Epoch 400:	Loss 0.0044	TrainAcc 1.0000	ValidAcc 0.7140	TestAcc 0.7370	BestValid 0.7180
	Epoch 450:	Loss 0.0097	TrainAcc 1.0000	ValidAcc 0.7040	TestAcc 0.7360	BestValid 0.7180
	Epoch 500:	Loss 0.0004	TrainAcc 1.0000	ValidAcc 0.7080	TestAcc 0.7330	BestValid 0.7180
	Epoch 550:	Loss 0.0601	TrainAcc 1.0000	ValidAcc 0.7180	TestAcc 0.7440	BestValid 0.7180
	Epoch 600:	Loss 0.0043	TrainAcc 1.0000	ValidAcc 0.7260	TestAcc 0.7400	BestValid 0.7260
	Epoch 650:	Loss 0.0011	TrainAcc 1.0000	ValidAcc 0.7120	TestAcc 0.7380	BestValid 0.7260
	Epoch 700:	Loss 0.0097	TrainAcc 1.0000	ValidAcc 0.7180	TestAcc 0.7400	BestValid 0.7260
	Epoch 750:	Loss 0.0164	TrainAcc 1.0000	ValidAcc 0.7200	TestAcc 0.7400	BestValid 0.7260
	Epoch 800:	Loss 0.0015	TrainAcc 1.0000	ValidAcc 0.7380	TestAcc 0.7580	BestValid 0.7380
	Epoch 850:	Loss 0.0078	TrainAcc 1.0000	ValidAcc 0.7240	TestAcc 0.7430	BestValid 0.7380
	Epoch 900:	Loss 0.0002	TrainAcc 1.0000	ValidAcc 0.7240	TestAcc 0.7460	BestValid 0.7380
	Epoch 950:	Loss 0.0105	TrainAcc 1.0000	ValidAcc 0.7180	TestAcc 0.7510	BestValid 0.7380
	Epoch 1000:	Loss 0.0007	TrainAcc 1.0000	ValidAcc 0.7300	TestAcc 0.7510	BestValid 0.7380
	Epoch 1050:	Loss 0.0048	TrainAcc 1.0000	ValidAcc 0.7420	TestAcc 0.7530	BestValid 0.7420
	Epoch 1100:	Loss 0.0002	TrainAcc 1.0000	ValidAcc 0.7380	TestAcc 0.7490	BestValid 0.7420
	Epoch 1150:	Loss 0.0003	TrainAcc 1.0000	ValidAcc 0.7380	TestAcc 0.7520	BestValid 0.7420
	Epoch 1200:	Loss 0.0075	TrainAcc 1.0000	ValidAcc 0.7220	TestAcc 0.7520	BestValid 0.7420
	Epoch 1250:	Loss 0.0043	TrainAcc 1.0000	ValidAcc 0.7300	TestAcc 0.7510	BestValid 0.7420
	Epoch 1300:	Loss 0.0000	TrainAcc 1.0000	ValidAcc 0.7200	TestAcc 0.7550	BestValid 0.7420
	Epoch 1350:	Loss 0.0005	TrainAcc 1.0000	ValidAcc 0.7200	TestAcc 0.7540	BestValid 0.7420
	Epoch 1400:	Loss 0.0004	TrainAcc 1.0000	ValidAcc 0.7360	TestAcc 0.7620	BestValid 0.7420
	Epoch 1450:	Loss 0.0004	TrainAcc 1.0000	ValidAcc 0.7320	TestAcc 0.7610	BestValid 0.7420
	Epoch 1500:	Loss 0.0004	TrainAcc 1.0000	ValidAcc 0.7260	TestAcc 0.7540	BestValid 0.7420
****** Epoch Time (Excluding Evaluation Cost): 0.023 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 0.029 ms (Max: 0.029, Min: 0.029, Sum: 0.029)
Cluster-Wide Average, Compute: 20.097 ms (Max: 20.097, Min: 20.097, Sum: 20.097)
Cluster-Wide Average, Communication-Layer: 0.010 ms (Max: 0.010, Min: 0.010, Sum: 0.010)
Cluster-Wide Average, Bubble-Imbalance: 0.016 ms (Max: 0.016, Min: 0.016, Sum: 0.016)
Cluster-Wide Average, Communication-Graph: 0.073 ms (Max: 0.073, Min: 0.073, Sum: 0.073)
Cluster-Wide Average, Optimization: 1.520 ms (Max: 1.520, Min: 1.520, Sum: 1.520)
Cluster-Wide Average, Others: 0.791 ms (Max: 0.791, Min: 0.791, Sum: 0.791)
****** Breakdown Sum: 22.536 ms ******
Cluster-Wide Average, GPU Memory Consumption: 1.717 GB (Max: 1.717, Min: 1.717, Sum: 1.717)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 0.000 Gbps (Max: 0.000, Min: 0.000, Sum: 0.000)
Layer-level communication (cluster-wide, per-epoch): 0.000 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 0.000 GB
****** Accuracy Results ******
Highest valid_acc: 0.7420
Target test_acc: 0.7550
Epoch to reach the target acc: 1049
[MPI Rank 0] Success 
