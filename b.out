Sat Sep  9 23:39:57 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    On   | 00000000:01:00.0 Off |                  Off |
| 30%   41C    P8    16W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    On   | 00000000:25:00.0 Off |                  Off |
| 30%   45C    P8    25W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    On   | 00000000:81:00.0 Off |                  Off |
| 30%   46C    P8    23W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    On   | 00000000:C1:00.0 Off |                  Off |
| 30%   46C    P8    15W / 230W |      1MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
gnerv2
gnerv2
gnerv2
gnerv2
[ 36%] Built target core
[ 36%] Built target context
[ 77%] Built target cudahelp
[ 86%] Built target OSDI2023_MULTI_NODES_gcn
[ 86%] Built target OSDI2023_MULTI_NODES_graphsage
[ 97%] Built target estimate_comm_volume
[ 97%] Built target OSDI2023_MULTI_NODES_gcnii
Running experiments...
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
Initializing the runtime environment
DONE MPI INITDONE MPI INIT
Initialized node 2 on machine gnerv2
DONE MPI INIT
Initialized node 1 on machine gnerv2
DONE MPI INIT
Initialized node 3 on machine gnerv2

Initialized node 0 on machine gnerv2
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
Building the CSR structure...
        It takes 0.288 seconds.
Building the CSC structure...
        It takes 0.289 seconds.
Building the CSC structure...
        It takes 0.296 seconds.
Building the CSC structure...
        It takes 0.317 seconds.
Building the CSC structure...
        It takes 0.292 seconds.
        It takes 0.296 seconds.
        It takes 0.322 seconds.
        It takes 0.312 seconds.
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
Building the Feature Vector...
        It takes 0.497 seconds.
Building the Label Vector...
        It takes 0.517 seconds.
Building the Label Vector...
        It takes 0.496 seconds.
Building the Label Vector...
        It takes 0.518 seconds.
Building the Label Vector...
        It takes 0.183 seconds.
        It takes 0.196 seconds.
        It takes 0.188 seconds.
The graph dataset locates at /shared_hdd_storage/jingjichen/gnn_datasets/partitioned_graphs/yelp/16_parts
The number of GCNII layers: 8
The number of hidden units: 128
The number of training epoches: 1000
Learning rate: 0.001000
The partition strategy: model
The dropout rate: 0.100
The checkpointed weight file: /tmp/saved_weights_pipe
The random seed: 1
Number of classes: 100
Number of feature dimensions: 300
Number of vertices: 716847
Number of GPUs: 4
        It takes 0.204 seconds.
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
GPU 2, layer [4, 6)
GPU 3, layer [6, 8)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 44803
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
GPU 2, layer [4, 6)
GPU 3, layer [6, 8)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
train nodes 537635, valid nodes 107527, test nodes 71685
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
GPU 2, layer [4, 6)
GPU 3, layer [6, 8)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
Chunks (number of global chunks: 16): 0-[0, 43495) 1-[43495, 89644) 2-[89644, 135406) 3-[135406, 178980) 4-[178980, 225129) 5-[225129, 271278) 6-[271278, 317427) 7-[317427, 363576) 8-[363576, 407071) ... 15-[673353, 716847)
716847, 13954819, 13954819
Number of vertices per chunk: 44803
GPU 0, layer [0, 2)
GPU 1, layer [2, 4)
GPU 2, layer [4, 6)
GPU 3, layer [6, 8)
WARNING: the current version only applies to linear GNN models!
WARNING: currently, exact inference during the whole training process will enforce the evaluation frequency to every 25 epoches.
716847, 13954819, 13954819
Number of vertices per chunk: 44803
716847, 13954819, 13954819
Number of vertices per chunk: 44803
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
csr in-out ready !Start Cost Model Initialization...
***** Start profiling the layer-level communication performance *******
csr in-out ready !Start Cost Model Initialization...
The layer-level communication performance: 113.518 Gbps (per GPU), 454.073 Gbps (aggregated)
The layer-level communication performance: 113.387 Gbps (per GPU), 453.547 Gbps (aggregated)
The layer-level communication performance: 113.228 Gbps (per GPU), 452.910 Gbps (aggregated)
The layer-level communication performance: 113.219 Gbps (per GPU), 452.875 Gbps (aggregated)
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
****** Start profiling the graph-level communication performance with supernodesize = 2 ******
The graph-level communication performance (supernode = 2): 155.896 Gbps (per GPU), 623.584 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.844 Gbps (per GPU), 623.375 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.896 Gbps (per GPU), 623.584 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 2): 155.847 Gbps (per GPU), 623.387 Gbps (aggregated, cluster-wide)
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
****** Start profiling the graph-level communication performance with supernodesize = 4 ******
The graph-level communication performance (supernode = 4): 102.514 Gbps (per GPU), 410.057 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.514 Gbps (per GPU), 410.054 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.514 Gbps (per GPU), 410.057 Gbps (aggregated, cluster-wide)
The graph-level communication performance (supernode = 4): 102.514 Gbps (per GPU), 410.057 Gbps (aggregated, cluster-wide)
 LType   LT0   LT1   LT2 Ratio  VSum  ESum
 chk_0  5.02ms  3.49ms  2.74ms  1.83 43.49K  0.57M
 chk_1  6.54ms  4.85ms  4.06ms  1.61 46.15K  2.10M
 chk_2  5.60ms  3.98ms  3.20ms  1.75 45.76K  1.16M
 chk_3  5.23ms  3.66ms  2.93ms  1.79 43.57K  0.96M
 chk_4  4.89ms  3.23ms  2.45ms  1.99 46.15K  0.50M
 chk_5  5.05ms  3.39ms  2.62ms  1.92 46.15K  0.51M
 chk_6  5.34ms  3.68ms  2.90ms  1.84 46.15K  0.86M
 chk_7  5.45ms  3.78ms  3.01ms  1.81 46.15K  1.00M
 chk_8  5.03ms  3.46ms  3.03ms  1.66 43.49K  0.58M
 chk_9  5.65ms  3.99ms  3.21ms  1.76 46.14K  0.81M
chk_10  5.45ms  3.48ms  2.74ms  1.99 43.95K  0.71M
chk_11  4.96ms  3.41ms  2.67ms  1.86 43.49K  0.61M
chk_12  4.75ms  3.19ms  2.45ms  1.93 43.49K  0.37M
chk_13  5.75ms  4.11ms  3.34ms  1.72 45.71K  1.06M
chk_14  5.30ms  3.74ms  3.01ms  1.76 43.49K  0.81M
chk_15  5.15ms  3.59ms  2.85ms  1.81 43.49K  0.62M
   Avg  5.32  3.69  2.95
   Max  6.54  4.85  4.06
   Min  4.75  3.19  2.45
 Ratio  1.38  1.52  1.66
   Var  0.18  0.16  0.15
Profiling takes 2.291 s
*** Node 0, starting model training...
Num Stages: 4 / 4
Node 0, Pipeline Input Tensor: NULL
Node 0, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 0 owns the model-level partition [0, 18)
*** Node 0, constructing the helper classes...
Node 0, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 1, starting model training...
Num Stages: 4 / 4
Node 1, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 1, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 1 owns the model-level partition [18, 34)
*** Node 1, constructing the helper classes...
Node 1, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 2, starting model training...
Num Stages: 4 / 4
Node 2, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 2, Pipeline Output Tensor: OPERATOR_DROPOUT
*** Node 2 owns the model-level partition [34, 50)
*** Node 2, constructing the helper classes...
Node 2, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 3, starting model training...
Num Stages: 4 / 4
Node 3, Pipeline Input Tensor: OPERATOR_DROPOUT
Node 3, Pipeline Output Tensor: NULL
*** Node 3 owns the model-level partition [50, 64)
*** Node 3, constructing the helper classes...
Node 3, Local Vertex Begin: 0, Num Local Vertices: 716847
*** Node 0, setting up some other necessary information...
*** Node 1, setting up some other necessary information...
*** Node 3, setting up some other necessary information...
*** Node 2, setting up some other necessary information...
+++++++++ Node 0 initializing the weights for op[0, 18)...
+++++++++ Node 1 initializing the weights for op[18, 34)...
+++++++++ Node 3 initializing the weights for op[50, 64)...
+++++++++ Node 2 initializing the weights for op[34, 50)...
Node 0, discovering the vertices that will be sent across graph boundary...
The number of mirror vertices: 0
Node 0, discovering the vertices that will be received across the graph boundary.
Node 1, discovering the vertices that will be sent across graph boundary...
Node 1, discovering the vertices that will be received across the graph boundary.
Node 2, discovering the vertices that will be sent across graph boundary...
Node 2, discovering the vertices that will be received across the graph boundary.
Node 3, discovering the vertices that will be sent across graph boundary...
Node 3, discovering the vertices that will be received across the graph boundary.
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
****** Start Scheduling the Tasks in a Pipelined Fashion ******
*** Node 0, starting task scheduling...



The learning rate specified by the user: 0.001000000
*** Node 1, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 2, starting task scheduling...
The learning rate specified by the user: 0.001000000
*** Node 3, starting task scheduling...
The learning rate specified by the user: 0.001000000
	Epoch 1:	Loss 0.7165	TrainAcc 0.1610	ValidAcc 0.1600	TestAcc 0.1601	BestValid 0.1600
	Epoch 50:	Loss 0.2678	TrainAcc 0.1415	ValidAcc 0.1422	TestAcc 0.1419	BestValid 0.1600
	Epoch 100:	Loss 0.2530	TrainAcc 0.1418	ValidAcc 0.1426	TestAcc 0.1422	BestValid 0.1600
	Epoch 150:	Loss 0.2410	TrainAcc 0.2343	ValidAcc 0.2345	TestAcc 0.2349	BestValid 0.2345
	Epoch 200:	Loss 0.2310	TrainAcc 0.2915	ValidAcc 0.2921	TestAcc 0.2919	BestValid 0.2921
	Epoch 250:	Loss 0.2255	TrainAcc 0.3707	ValidAcc 0.3706	TestAcc 0.3718	BestValid 0.3706
	Epoch 300:	Loss 0.2218	TrainAcc 0.4412	ValidAcc 0.4409	TestAcc 0.4421	BestValid 0.4409
	Epoch 350:	Loss 0.2187	TrainAcc 0.4590	ValidAcc 0.4584	TestAcc 0.4600	BestValid 0.4584
	Epoch 400:	Loss 0.2165	TrainAcc 0.4778	ValidAcc 0.4773	TestAcc 0.4788	BestValid 0.4773
	Epoch 450:	Loss 0.2148	TrainAcc 0.4830	ValidAcc 0.4823	TestAcc 0.4838	BestValid 0.4823
	Epoch 500:	Loss 0.2132	TrainAcc 0.4866	ValidAcc 0.4857	TestAcc 0.4875	BestValid 0.4857
	Epoch 550:	Loss 0.2120	TrainAcc 0.4877	ValidAcc 0.4867	TestAcc 0.4885	BestValid 0.4867
	Epoch 600:	Loss 0.2105	TrainAcc 0.4923	ValidAcc 0.4912	TestAcc 0.4930	BestValid 0.4912
	Epoch 650:	Loss 0.2093	TrainAcc 0.4934	ValidAcc 0.4924	TestAcc 0.4936	BestValid 0.4924
	Epoch 700:	Loss 0.2097	TrainAcc 0.4986	ValidAcc 0.4974	TestAcc 0.4991	BestValid 0.4974
	Epoch 750:	Loss 0.2075	TrainAcc 0.5023	ValidAcc 0.5008	TestAcc 0.5029	BestValid 0.5008
	Epoch 800:	Loss 0.2067	TrainAcc 0.5047	ValidAcc 0.5034	TestAcc 0.5053	BestValid 0.5034
	Epoch 850:	Loss 0.2060	TrainAcc 0.5061	ValidAcc 0.5048	TestAcc 0.5064	BestValid 0.5048
	Epoch 900:	Loss 0.2053	TrainAcc 0.5075	ValidAcc 0.5063	TestAcc 0.5079	BestValid 0.5063
	Epoch 950:	Loss 0.2048	TrainAcc 0.5099	ValidAcc 0.5087	TestAcc 0.5103	BestValid 0.5087
	Epoch 1000:	Loss 0.2040	TrainAcc 0.5123	ValidAcc 0.5111	TestAcc 0.5126	BestValid 0.5111
****** Epoch Time (Excluding Evaluation Cost): 0.220 s ******
****** Breakdown Analysis ******
Cluster-Wide Average, Bubble-Pipeline: 32.240 ms (Max: 35.298, Min: 29.079, Sum: 128.959)
Cluster-Wide Average, Compute: 122.089 ms (Max: 147.179, Min: 108.499, Sum: 488.356)
Cluster-Wide Average, Communication-Layer: 45.099 ms (Max: 53.490, Min: 36.926, Sum: 180.395)
Cluster-Wide Average, Bubble-Imbalance: 17.903 ms (Max: 34.514, Min: 3.427, Sum: 71.612)
Cluster-Wide Average, Communication-Graph: 0.110 ms (Max: 0.125, Min: 0.097, Sum: 0.440)
Cluster-Wide Average, Optimization: 0.132 ms (Max: 0.146, Min: 0.116, Sum: 0.530)
Cluster-Wide Average, Others: 4.225 ms (Max: 7.118, Min: 3.257, Sum: 16.898)
****** Breakdown Sum: 221.798 ms ******
Cluster-Wide Average, GPU Memory Consumption: 5.557 GB (Max: 6.786, Min: 4.981, Sum: 22.229)
Cluster-Wide Average, Graph-Level Communication Throughput: -nan Gbps (Max: -nan, Min: -nan, Sum: -nan)
Cluster-Wide Average, Layer-Level Communication Throughput: 94.874 Gbps (Max: 111.564, Min: 78.631, Sum: 379.495)
Layer-level communication (cluster-wide, per-epoch): 2.051 GB
Graph-level communication (cluster-wide, per-epoch): 0.000 GB
Weight-sync communication (cluster-wide, per-epoch): 0.000 GB
Total communication (cluster-wide, per-epoch): 2.051 GB
****** Accuracy Results ******
Highest valid_acc: 0.5111
Target test_acc: 0.5126
Epoch to reach the target acc: 999
[MPI Rank 0] Success 
[MPI Rank 1] Success 
[MPI Rank 2] Success 
[MPI Rank 3] Success 
